@misc{2020,
  title = {{How To Dockerize ShinyApps}},
  year = {2020},
  month = may,
  journal = {STATWORX},
  urldate = {2021-10-04},
  abstract = {After having successfully run a simple R-script inside a Docker container before, we next attempt to repeat this process for entire apps built within the RShiny framework. Join me on the next step toward deploying your work done in R with the help of neat Docker containers!},
  howpublished = {https://www.statworx.com/de/blog/how-to-dockerize-shinyapps/},
  langid = {ngerman},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\F33TYRPA\\how-to-dockerize-shinyapps.html}
}

@article{aguinis2013,
  title = {Best-{{Practice Recommendations}} for {{Defining}}, {{Identifying}}, and {{Handling Outliers}}},
  author = {Aguinis, Herman and Gottfredson, Ryan K. and Joo, Harry},
  year = {2013},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {16},
  number = {2},
  pages = {270--301},
  issn = {1094-4281, 1552-7425},
  doi = {10.1177/1094428112470848},
  urldate = {2022-10-19},
  abstract = {The presence of outliers, which are data points that deviate markedly from others, is one of the most enduring and pervasive methodological challenges in organizational science research. We provide evidence that different ways of defining, identifying, and handling outliers alter substantive research conclusions. Then, we report results of a literature review of 46 methodological sources (i.e., journal articles, book chapters, and books) addressing the topic of outliers, as well as 232 organizational science journal articles mentioning issues about outliers. Our literature review uncovered (a) 14 unique and mutually exclusive outlier definitions, 39 outlier identification techniques, and 20 different ways of handling outliers; (b) inconsistencies in how outliers are defined, identified, and handled in various methodological sources; and (c) confusion and lack of transparency in how outliers are addressed by substantive researchers. We offer guidelines, including decision-making trees, that researchers can follow to define, identify, and handle error, interesting, and influential (i.e., model fit and prediction) outliers. Although our emphasis is on regression, structural equation modeling, and multilevel modeling, our general framework forms the basis for a research agenda regarding outliers in the context of other data-analytic approaches. Our recommendations can be used by authors as well as journal editors and reviewers to improve the consistency and transparency of practices regarding the treatment of outliers in organizational science research.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\NCZWB694\\Aguinis et al. - 2013 - Best-Practice Recommendations for Defining, Identi.pdf}
}

@article{albers2019,
  title = {The Problem with Unadjusted Multiple and Sequential Statistical Testing},
  author = {Albers, Casper},
  year = {2019},
  month = apr,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1921},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09941-0},
  urldate = {2023-03-22},
  abstract = {In research studies, the need for additional samples to obtain sufficient statistical power has often to be balanced with the experimental costs. One approach to this end is to sequentially collect data until you have sufficient measurements, e.g., when the p-value drops below 0.05. I outline that this approach is common, yet that unadjusted sequential sampling leads to severe statistical issues, such as an inflated rate of false positive findings. As a consequence, the results of such studies are untrustworthy. I identify the statistical methods that can be implemented in order to account for sequential sampling.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\74IQQM5B\\Albers - 2019 - The problem with unadjusted multiple and sequentia.pdf}
}

@article{amrhein2019,
  title = {Inferential {{Statistics}} as {{Descriptive Statistics}}: {{There Is No Replication Crisis}} If {{We Don}}'t {{Expect Replication}}},
  shorttitle = {Inferential {{Statistics}} as {{Descriptive Statistics}}},
  author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {262--270},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1543137},
  urldate = {2022-04-20},
  abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a ``replication crisis'' may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HGKDH9J7\\Amrhein et al. - 2019 - Inferential Statistics as Descriptive Statistics .pdf}
}

@article{anderson2017,
  title = {Addressing the ``{{Replication Crisis}}'': {{Using Original Studies}} to {{Design Replication Studies}} with {{Appropriate Statistical Power}}},
  shorttitle = {Addressing the ``{{Replication Crisis}}''},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  year = {2017},
  month = may,
  journal = {Multivariate Behavioral Research},
  volume = {52},
  number = {3},
  pages = {305--324},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2017.1289361},
  urldate = {2021-09-23},
  abstract = {Psychology is undergoing a replication crisis. The discussion surrounding this crisis has centered on mistrust of previous findings. Researchers planning replication studies often use the original study sample effect size as the basis for sample size planning. However, this strategy ignores uncertainty and publication bias in estimated effect sizes, resulting in overly optimistic calculations. A psychologist who intends to obtain power of .80 in the replication study, and performs calculations accordingly, may have an actual power lower than .80. We performed simulations to reveal the magnitude of the difference between actual and intended power based on common sample size planning strategies and assessed the performance of methods that aim to correct for effect size uncertainty and/or bias. Our results imply that even if original studies reflect actual phenomena and were conducted in the absence of questionable research practices, popular approaches to designing replication studies may result in a low success rate, especially if the original study is underpowered. Methods correcting for bias and/or uncertainty generally had higher actual power, but were not a panacea for an underpowered original study. Thus, it becomes imperative that 1) original studies are adequately powered and 2) replication studies are designed with methods that are more likely to yield the intended level of power.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\B8K5RALX\\Anderson und Maxwell - 2017 - Addressing the “Replication Crisis” Using Origina.pdf}
}

@article{armitage1947,
  title = {Some {{Sequential Tests}} of {{Student}}'s {{Hypothesis}}},
  author = {Armitage, P.},
  year = {1947},
  journal = {Supplement to the Journal of the Royal Statistical Society},
  volume = {9},
  number = {2},
  eprint = {2984117},
  eprinttype = {jstor},
  pages = {250},
  issn = {14666162},
  doi = {10.2307/2984117},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\U5YWH6LC\\Armitage - 1947 - Some Sequential Tests of Student's Hypothesis.pdf}
}

@article{aust2022,
  title = {Papaja: {{Prepare}} American Psychological Association Journal Articles with {{R Markdown}}  {\emph{(}}{{{\emph{Version}}}}{\emph{ 0.1.1) [}}{{{\emph{R Package}}}}{\emph{]}}},
  shorttitle = {R {{Package}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2022},
  doi = {https://CRAN.R-project.org/package=papaja},
  keywords = {1 paper,software}
}

@article{bakan1966,
  title = {{{THE TEST OF SIGNIFICANCE IN PSYCHOLOGICAL RESEARCH}}},
  author = {Bakan, David},
  year = {1966},
  pages = {15},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DZRCGLXG\\Bakan - THE TEST OF SIGNIFICANCE IN PSYCHOLOGICAL RESEARCH.pdf}
}

@article{bakken2004,
  title = {Data Obfuscation: Anonymity and Desensitization of Usable Data Sets},
  shorttitle = {Data Obfuscation},
  author = {Bakken, D.E. and Rarameswaran, R. and Blough, D.M. and Franz, A.A. and Palmer, T.J.},
  year = {2004},
  month = nov,
  journal = {IEEE Security and Privacy Magazine},
  volume = {2},
  number = {6},
  pages = {34--41},
  issn = {1540-7993},
  doi = {10.1109/MSP.2004.97},
  urldate = {2021-11-09},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\YDKYFGUT\\Bakken et al. - 2004 - Data obfuscation anonymity and desensitization of.pdf}
}

@article{barnard1949,
  title = {Statistical Inference},
  author = {Barnard, G. A.},
  year = {1949},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {11},
  number = {2},
  pages = {115--149},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1949.tb00028.x},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ADXSQIJF\\[No title found].pdf}
}

@article{barnard1952,
  title = {The {{Frequency Justification}} of {{Certain Sequential Tests}}},
  author = {Barnard, G. A.},
  year = {1952},
  journal = {Biometrika},
  volume = {39},
  number = {1/2},
  eprint = {2332473},
  eprinttype = {jstor},
  pages = {144--150},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332473},
  urldate = {2023-01-04},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EKRVZCWM\\Barnard - 1952 - The Frequency Justification of Certain Sequential .pdf}
}

@article{barnett2004,
  title = {Regression to the Mean: What It Is and How to Deal with It},
  shorttitle = {Regression to the Mean},
  author = {Barnett, A. G},
  year = {2004},
  month = aug,
  journal = {International Journal of Epidemiology},
  volume = {34},
  number = {1},
  pages = {215--220},
  issn = {1464-3685},
  doi = {10.1093/ije/dyh299},
  urldate = {2021-09-23},
  abstract = {Background Regression to the mean (RTM) is a statistical phenomenon that can make natural variation in repeated data look like real change. It happens when unusually large or small measurements tend to be followed by measurements that are closer to the mean. Methods We give some examples of the phenomenon, and discuss methods to overcome it at the design and analysis stages of a study. Results The effect of RTM in a sample becomes more noticeable with increasing measurement error and when follow-up measurements are only examined on a sub-sample selected using a baseline value. Conclusions RTM is a ubiquitous phenomenon in repeated data and should always be considered as a possible cause of an observed change. Its effect can be alleviated through better study design and use of suitable statistical methods.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UZNC9J5V\\Barnett - 2004 - Regression to the mean what it is and how to deal.pdf}
}

@article{bassler2010,
  title = {Stopping Randomized Trials Early for Benefit and Estimation of Treatment Effects: {{Systematic}} Review and Meta-Regression Analysis},
  shorttitle = {Stopping Randomized Trials Early for Benefit and Estimation of Treatment Effects},
  author = {Bassler, Dirk and Briel, Matthias and Montori, Victor M. and Lane, Melanie and Glasziou, Paul and Zhou, Qi and {Heels-Ansdell}, Diane and Walter, Stephen D. and Guyatt, Gordon H. and the {STOPIT-2 Study Group}, and},
  year = {2010},
  month = mar,
  journal = {JAMA},
  volume = {303},
  number = {12},
  pages = {1180--1187},
  issn = {0098-7484},
  doi = {10.1001/jama.2010.310},
  urldate = {2023-02-27},
  abstract = {Theory and simulation suggest that randomized controlled trials (RCTs) stopped early for benefit (truncated RCTs) systematically overestimate treatment effects for the outcome that precipitated early stopping.To compare the treatment effect from truncated RCTs with that from meta-analyses of RCTs addressing the same question but not stopped early (nontruncated RCTs) and to explore factors associated with overestimates of effect.Search of MEDLINE, EMBASE, Current Contents, and full-text journal content databases to identify truncated RCTs up to January 2007; search of MEDLINE, Cochrane Database of Systematic Reviews, and Database of Abstracts of Reviews of Effects to identify systematic reviews from which individual RCTs were extracted up to January 2008.Selected studies were RCTs reported as having stopped early for benefit and matching nontruncated RCTs from systematic reviews. Independent reviewers with medical content expertise, working blinded to trial results, judged the eligibility of the nontruncated RCTs based on their similarity to the truncated RCTs.Reviewers with methodological expertise conducted data extraction independently.The analysis included 91 truncated RCTs asking 63 different questions and 424 matching nontruncated RCTs. The pooled ratio of relative risks in truncated RCTs vs matching nontruncated RCTs was 0.71 (95\% confidence interval, 0.65-0.77). This difference was independent of the presence of a statistical stopping rule and the methodological quality of the studies as assessed by allocation concealment and blinding. Large differences in treatment effect size between truncated and nontruncated RCTs (ratio of relative risks \&lt;0.75) occurred with truncated RCTs having fewer than 500 events. In 39 of the 63 questions (62\%), the pooled effects of the nontruncated RCTs failed to demonstrate significant benefit.Truncated RCTs were associated with greater effect sizes than RCTs not stopped early. This difference was independent of the presence of statistical stopping rules and was greatest in smaller studies.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Q5YAVVIG\\Bassler et al. - 2010 - Stopping Randomized Trials Early for Benefit and E.pdf;C\:\\Users\\Admin\\Zotero\\storage\\TT27LZRD\\185591.html}
}

@article{begley2012,
  title = {Raise Standards for Preclinical Cancer Research},
  author = {Begley, C. Glenn and Ellis, Lee M.},
  year = {2012},
  month = mar,
  journal = {Nature},
  volume = {483},
  number = {7391},
  pages = {531--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/483531a},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\F4VEBU45\\Begley und Ellis - 2012 - Raise standards for preclinical cancer research.pdf}
}

@article{berger2003,
  title = {Could {{Fisher}}, {{Jeffreys}} and {{Neyman Have Agreed}} on {{Testing}}?},
  author = {Berger, James O.},
  year = {2003},
  month = feb,
  journal = {Statistical Science},
  volume = {18},
  number = {1},
  issn = {0883-4237},
  doi = {10.1214/ss/1056397485},
  urldate = {2021-09-23},
  abstract = {Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QIXTQDF2\\Berger - 2003 - Could Fisher, Jeffreys and Neyman Have Agreed on T.pdf}
}

@article{bhattacharjee2015,
  title = {Confidence Interval Estimation Following {{SPRT}} in a Normal Distribution with Equal Mean and Variance},
  author = {Bhattacharjee, Debanjan and Mukhopadhyay, Nitis},
  year = {2015},
  month = oct,
  journal = {Sequential Analysis},
  volume = {34},
  number = {4},
  pages = {504--531},
  issn = {0747-4946, 1532-4176},
  doi = {10.1080/07474946.2015.1099948},
  urldate = {2023-03-20},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\3B7YWD6Q\\Bhattacharjee und Mukhopadhyay - 2015 - Confidence Interval Estimation Following SPRT in a.pdf}
}

@article{bigdely-shamlo2015,
  title = {The {{PREP}} Pipeline: Standardized Preprocessing for Large-Scale {{EEG}} Analysis},
  shorttitle = {The {{PREP}} Pipeline},
  author = {{Bigdely-Shamlo}, Nima and Mullen, Tim and Kothe, Christian and Su, Kyung-Min and Robbins, Kay A.},
  year = {2015},
  journal = {Frontiers in Neuroinformatics},
  volume = {9},
  issn = {1662-5196},
  urldate = {2022-02-21},
  abstract = {The technology to collect brain imaging and physiological measures has become portable and ubiquitous, opening the possibility of large-scale analysis of real-world human imaging. By its nature, such data is large and complex, making automated processing essential. This paper shows how lack of attention to the very early stages of an EEG preprocessing pipeline can reduce the signal-to-noise ratio and introduce unwanted artifacts into the data, particularly for computations done in single precision. We demonstrate that ordinary average referencing improves the signal-to-noise ratio, but that noisy channels can contaminate the results. We also show that identification of noisy channels depends on the reference and examine the complex interaction of filtering, noisy channel identification, and referencing. We introduce a multi-stage robust referencing scheme to deal with the noisy channel-reference interaction. We propose a standardized early-stage EEG processing pipeline (PREP) and discuss the application of the pipeline to more than 600 EEG datasets. The pipeline includes an automatically generated report for each dataset processed. Users can download the PREP pipeline as a freely available MATLAB library from http://eegstudy.org/prepcode.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\H9V4SL9Y\\Bigdely-Shamlo et al. - 2015 - The PREP pipeline standardized preprocessing for .pdf}
}

@article{bird2002,
  title = {Confidence {{Intervals}} for {{Effect Sizes}} in {{Analysis}} of {{Variance}}},
  author = {Bird, Kevin D.},
  year = {2002},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {62},
  number = {2},
  pages = {197--226},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/0013164402062002001},
  urldate = {2022-08-12},
  abstract = {Although confidence interval procedures for analysis of variance (ANOVA) have been available for some time, they are not well known and are often difficult to implement with statistical packages. This article discusses procedures for constructing individual and simultaneous confidence intervals on contrasts on parameters of a number of fixed-effects ANOVA models, including multivariate analysis of variance (MANOVA) models for the analysis of repeated measures data. Examples show how these procedures can be implemented with accessible software. Confidence interval inference on parameters of random-effects models is also discussed.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TS53PW6I\\Bird - 2002 - Confidence Intervals for Effect Sizes in Analysis .pdf}
}

@article{blanca2017,
  title = {Non-Normal Data: {{Is ANOVA}} Still a Valid Option?},
  shorttitle = {Non-Normal Data},
  author = {Blanca, Mar{\'i}a J. and Alarc{\'o}n, Rafael and Arnau, Jaume},
  year = {2017},
  month = nov,
  journal = {Psicothema},
  number = {29.4},
  pages = {552--557},
  doi = {10.7334/psicothema2016.383},
  urldate = {2021-11-16},
  abstract = {Background: The robustness of F-test to non-normality has been studied from the 1930s through to the present day. However, this extensive body of research has yielded contradictory results, there being evidence both for and against its robustness. This study provides a systematic examination of F-test robustness to violations of normality in terms of Type I error, considering a wide variety of distributions commonly found in the health and social sciences. Method: We conducted a Monte Carlo simulation study involving a design with three groups and several known and unknown distributions. The manipulated variables were: Equal and unequal group sample sizes; group sample size and total sample size; coefficient of sample size variation; shape of the distribution and equal or unequal shapes of the group distributions; and pairing of group size with the degree of contamination in the distribution. Results: The results showed that in terms of Type I error the F-test was robust in 100\% of the cases studied, independently of the manipulated conditions.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CZ52CEDJ\\Blanca et al. - 2017 - Non-normal data Is ANOVA still a valid option.pdf}
}

@article{blanca2018,
  title = {Effect of Variance Ratio on {{ANOVA}} Robustness: {{Might}} 1.5 Be the Limit?},
  shorttitle = {Effect of Variance Ratio on {{ANOVA}} Robustness},
  author = {Blanca, Mar{\'i}a J. and Alarc{\'o}n, Rafael and Arnau, Jaume and Bono, Roser and Bendayan, Rebecca},
  year = {2018},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {50},
  number = {3},
  pages = {937--962},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0918-2},
  urldate = {2022-07-22},
  abstract = {Inconsistencies in the research findings on F-test robustness to variance heterogeneity could be related to the lack of a standard criterion to assess robustness or to the different measures used to quantify heterogeneity. In the present paper we use Monte Carlo simulation to systematically examine the Type I error rate of F-test under heterogeneity. One-way, balanced, and unbalanced designs with monotonic patterns of variance were considered. Variance ratio (VR) was used as a measure of heterogeneity (1.5, 1.6, 1.7, 1.8, 2, 3, 5, and 9), the coefficient of sample size variation as a measure of inequality between group sizes (0.16, 0.33, and 0.50), and the correlation between variance and group size as an indicator of the pairing between them (1, .50, 0, -.50, and -1). Overall, the results suggest that in terms of Type I error a VR above 1.5 may be established as a rule of thumb for considering a potential threat to F-test robustness under heterogeneity with unequal sample sizes.},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AXYQW4TR\\Blanca et al. - 2018 - Effect of variance ratio on ANOVA robustness Migh.pdf}
}

@article{blancamena2017,
  title = {Non-Normal Data: {{Is ANOVA}} Still a Valid Option?},
  shorttitle = {Non-Normal Data},
  author = {Blanca Mena, M. Jos{\'e} and Alarc{\'o}n Postigo, Rafael and Arnau Gras, Jaume and Bono Cabr{\'e}, Roser and Bendayan, Rebecca},
  year = {2017},
  publisher = {{Facultad de Psicolog\'ia de la Universidad de Oviedo y el Colegio Oficial de Psic\'ologos del Principado de Asturias}},
  issn = {0214-9915},
  urldate = {2021-11-16},
  abstract = {Background: The robustness of F-test to non-normality has been studied from the 1930s through to the present day. However, this extensive body of research has yielded contradictory results, there being evidence both for and against its robustness. This study provides a systematic examination of F-test robustness to violations of normality in terms of Type I error, considering a wide variety of distributions commonly found in the health and social sciences. Method: We conducted a Monte Carlo simulation study involving a design with three groups and several known and unknown distributions. The manipulated variables were: Equal and unequal group sample sizes; group sample size and total sample size; coeffi cient of sample size variation; shape of the distribution and equal or unequal shapes of the group distributions; and pairing of group size with the degree of contamination in the distribution. Results: The results showed that in terms of Type I error the F-test was robust in 100\% of the cases studied, independently of the manipulated conditions.},
  copyright = {(c) Psicothema, 2017},
  langid = {english},
  annotation = {Accepted: 2018-05-07T11:07:07Z},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XC3PQN73\\Blanca Mena et al. - 2017 - Non-normal data Is ANOVA still a valid option.pdf;C\:\\Users\\Admin\\Zotero\\storage\\LE48MIEY\\122126.html}
}

@article{bland1994,
  title = {Statistics {{Notes}}: {{Some}} Examples of Regression towards the Mean},
  shorttitle = {Statistics {{Notes}}},
  author = {Bland, J M and Altman, D G},
  year = {1994},
  month = sep,
  journal = {BMJ},
  volume = {309},
  number = {6957},
  pages = {780--780},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.309.6957.780},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\YQU5565D\\Bland und Altman - 1994 - Statistics Notes Some examples of regression towa.pdf}
}

@article{bosman,
  title = {Robust {{Bayes}} Factors for {{Bayesian ANOVA}}: Overcoming Adverse Effects of Non-Normality},
  author = {Bosman, Marlyne},
  pages = {45},
  abstract = {The software package Bain (Gu, Mulder, \& Hoijtink, 2017) provides an easy way for researchers to evaluate informative hypotheses with regards to group means. Support in the data for pairs of informative hypotheses is quantified by Bain by computing the approximate adjusted fractional Bayes factor (AAFBF). To compute the AAFBF, Bain only needs group mean estimates, their variances and group sample sizes as input. Unfortunately, the common sample mean and its variance are known to be highly susceptible to non-normality and outliers. Therefore, first of all, this paper investigates with a simulation study to what extent the AAFBF resulting from the sample mean and its variance is affected by non-normality and outliers. Furthermore, Bain provides an unique opportunity to combine Bayes factors with robust statistics. Hence, secondly, this paper investigates with a simulation study to what extent the effect of non-normality and outliers can be ameliorated by replacing the sample mean and the corresponding variance estimates by robust estimates, creating the robust Bayes factor AAFBFROB. Results of the simulation studies showed that the performance of the AAFBF decreases when non-normality or outliers are present and that the AAFBFROB is less affected in most instances. An example study is presented to show how the AAFBFROB can be used and instructions how to compute it are provided. Finally, recommendations for researchers as to when use of the AAFBFROB is preferred over use of the AAFBF are given.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\54IGT3PE\\Bosman - Robust Bayes factors for Bayesian ANOVA overcomin.pdf}
}

@article{botella2006,
  title = {Optimization of Sample Size in Controlled Experiments: {{The CLAST}} Rule},
  shorttitle = {Optimization of Sample Size in Controlled Experiments},
  author = {Botella, Juan and Xim{\'e}nez, Carmen and Revuelta, Javier and Suero, Manuel},
  year = {2006},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {38},
  number = {1},
  pages = {65--76},
  issn = {1554-3528},
  doi = {10.3758/BF03192751},
  urldate = {2023-01-03},
  abstract = {Sequential rules are explored in the context of null hypothesis significance testing. Several studies have demonstrated that the fixed-sample stopping rule, in which the sample size used by researchers is determined in advance, is less practical and less efficient than sequential stopping rules. It is proposed that a sequential stopping rule called CLAST (composite limited adaptive sequential test) is a superior variant of COAST (composite open adaptive sequential test), a sequential rule proposed by Frick (1998). Simulation studies are conducted to test the efficiency of the proposed rule in terms of sample size and power. Two statistical tests are used: the one-tailed t test of mean differences with two matched samples, and the chi-square independence test for twofold contingency tables. The results show that the CLAST rule is more efficient than the COAST rule and reflects more realistically the practice of experimental psychology researchers.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DMN4T2TZ\\Botella et al. - 2006 - Optimization of sample size in controlled experime.pdf}
}

@article{boulesteix2020,
  title = {A Replication Crisis in Methodological Research?},
  author = {Boulesteix, Anne-Laure and Hoffmann, Sabine and Charlton, Alethea and Seibold, Heidi},
  year = {2020},
  journal = {Significance},
  volume = {17},
  number = {5},
  pages = {18--21},
  issn = {1740-9713},
  doi = {10.1111/1740-9713.01444},
  urldate = {2021-09-28},
  abstract = {Statisticians have been keen to critique statistical aspects of the ``replication crisis'' in other scientific disciplines. But new statistical tools are often published and promoted without any thought to replicability. This needs to change, argue Anne-Laure Boulesteix, Sabine Hoffmann, Alethea Charlton and Heidi Seibold},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7ZD44IEJ\\Boulesteix et al. - 2020 - A replication crisis in methodological research.pdf;C\:\\Users\\Admin\\Zotero\\storage\\CXER7TMX\\1740-9713.html}
}

@article{brand2016,
  title = {The {{Precision}} of {{Effect Size Estimation From Published Psychological Research}}: {{Surveying Confidence Intervals}}},
  shorttitle = {The {{Precision}} of {{Effect Size Estimation From Published Psychological Research}}},
  author = {Brand, Andrew and Bradley, Michael T.},
  year = {2016},
  month = feb,
  journal = {Psychological Reports},
  volume = {118},
  number = {1},
  pages = {154--170},
  publisher = {{SAGE Publications Inc}},
  issn = {0033-2941},
  doi = {10.1177/0033294115625265},
  urldate = {2022-08-09},
  abstract = {Confidence interval (CI) widths were calculated for reported Cohen's d standardized effect sizes and examined in two automated surveys of published psychological literature. The first survey reviewed 1,902 articles from Psychological Science. The second survey reviewed a total of 5,169 articles from across the following four APA journals: Journal of Abnormal Psychology, Journal of Applied Psychology, Journal of Experimental Psychology: Human Perception and Performance, and Developmental Psychology. The median CI width for d was greater than 1 in both surveys. Hence, CI widths were, as Cohen (1994) speculated, embarrassingly large. Additional exploratory analyses revealed that CI widths varied across psychological research areas and that CI widths were not discernably decreasing over time. The theoretical implications of these findings are discussed along with ways of reducing the CI widths and thus improving precision of effect size estimation.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2AYTY2A2\\Brand und Bradley - 2016 - The Precision of Effect Size Estimation From Publi.pdf}
}

@article{brandt2014,
  title = {The {{Replication Recipe}}: {{What}} Makes for a Convincing Replication?},
  shorttitle = {The {{Replication Recipe}}},
  author = {Brandt, Mark J. and IJzerman, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and {Giner-Sorolla}, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and {van 't Veer}, Anna},
  year = {2014},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {50},
  pages = {217--224},
  issn = {00221031},
  doi = {10.1016/j.jesp.2013.10.005},
  urldate = {2021-09-23},
  abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\S38DFCQD\\Brandt et al. - 2014 - The Replication Recipe What makes for a convincin.pdf}
}

@article{brown1974,
  title = {Robust {{Tests}} for the {{Equality}} of {{Variances}}},
  author = {Brown, Morton B. and Forsythe, Alan B.},
  year = {1974},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {69},
  number = {346},
  pages = {364--367},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1974.10482955},
  urldate = {2022-08-09},
  abstract = {Alternative formulations of Levene's test statistic for equality of variances are found to be robust under nonnormality. These statistics use more robust estimators of central location in place of the mean. They are compared with the unmodified Levene's statistic, a jackknife procedure, and a {$\chi$}2 test suggested by Layard which are all found to be less robust under nonnormality.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QIE9BTLY\\Brown und Forsythe - 1974 - Robust Tests for the Equality of Variances.pdf}
}

@techreport{bryan2017,
  type = {Preprint},
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.3159v2},
  urldate = {2022-10-18},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5XUB7VBV\\Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf;C\:\\Users\\Admin\\Zotero\\storage\\T4YBTMW9\\Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf;C\:\\Users\\Admin\\Zotero\\storage\\VQUK6RFM\\Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf}
}

@article{button2013,
  title = {Power Failure: {{Why}} Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2023-02-15},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HT3743MN\\Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{campbell2013,
  title = {Similarities and {{Differences}} of {{Bayesian Designs}} and {{Adaptive Designs}} for {{Medical Devices}}: {{A Regulatory View}}},
  shorttitle = {Similarities and {{Differences}} of {{Bayesian Designs}} and {{Adaptive Designs}} for {{Medical Devices}}},
  author = {Campbell, Gregory},
  year = {2013},
  month = nov,
  journal = {Statistics in Biopharmaceutical Research},
  volume = {5},
  number = {4},
  pages = {356--368},
  issn = {1946-6315},
  doi = {10.1080/19466315.2013.846873},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AWMP6XHA\\Campbell - 2013 - Similarities and Differences of Bayesian Designs a.pdf}
}

@article{chen2013,
  title = {Constructing {{Confidence Intervals}} for {{Effect Sizes}} in {{ANOVA Designs}}},
  author = {Chen, Li-Ting and Peng, Chao-Ying Joanne},
  year = {2013},
  month = nov,
  journal = {Journal of Modern Applied Statistical Methods},
  volume = {12},
  number = {2},
  pages = {82--104},
  issn = {1538-9472},
  doi = {10.22237/jmasm/1383278640},
  urldate = {2022-08-12},
  abstract = {A confidence interval for effect sizes provides a range of plausible population effect sizes (ES) that are consistent with data. This article defines an ES as a standardized linear contrast of means. The noncentral method, Bonett's method, and the bias-corrected and accelerated bootstrap method are illustrated for constructing the confidence interval for such an effect size. Results obtained from the three methods are discussed and interpretations of results are offered.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\A38Y7TDD\\Chen und Peng - 2013 - Constructing Confidence Intervals for Effect Sizes.pdf}
}

@article{cochran1947,
  title = {Some {{Consequences When}} the {{Assumptions}} for the {{Analysis}} of {{Variance}} Are Not {{Satisfied}}},
  author = {Cochran, W. G.},
  year = {1947},
  journal = {Biometrics},
  volume = {3},
  number = {1},
  eprint = {3001535},
  eprinttype = {jstor},
  pages = {22--38},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/3001535},
  urldate = {2023-03-02},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2UPRC47J\\Cochran - 1947 - Some Consequences When the Assumptions for the Ana.pdf}
}

@article{cohen1962,
  title = {The Statistical Power of Abnormal-Social Psychological Research: {{A}} Review.},
  shorttitle = {The Statistical Power of Abnormal-Social Psychological Research},
  author = {Cohen, Jacob},
  year = {1962},
  month = sep,
  journal = {The Journal of Abnormal and Social Psychology},
  volume = {65},
  number = {3},
  pages = {145--153},
  issn = {0096-851X},
  doi = {10.1037/h0045186},
  urldate = {2022-04-05},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SAX92DFQ\\Cohen - 1962 - The statistical power of abnormal-social psycholog.pdf}
}

@article{cohen1992,
  title = {A Power Primer},
  shorttitle = {Quantitative Methods in Psychology},
  author = {Cohen, J.},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {112},
  number = {1},
  pages = {1155--1159},
  doi = {10.1037/0033-2909.112.1.155},
  urldate = {2023-04-12},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EXYVZQ4B\\Cohen1992.pdf;C\:\\Users\\Admin\\Zotero\\storage\\6GJMFQS5\\1571417125554624128.html}
}

@article{cohen1994,
  title = {The Earth Is Round (p {$<$} .05).},
  author = {Cohen, Jacob},
  year = {1994},
  month = dec,
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.49.12.997},
  urldate = {2023-04-12},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PELYBREN\\Cohen - 1994 - The earth is round (p  .05)..pdf}
}

@article{cohen2007,
  title = {A Power Primer},
  author = {Cohen, Jacob},
  year = {2007},
  month = sep,
  journal = {Tutorials in Quantitative Methods for Psychology},
  volume = {3},
  number = {2},
  pages = {79--79},
  issn = {1913-4126},
  doi = {10.20982/tqmp.03.2.p079},
  urldate = {2023-04-12},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\WRN2BFDH\\Cohen1992.pdf}
}

@book{cohen2009,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {2009},
  edition = {2. ed., reprint},
  publisher = {{Psychology Press}},
  address = {{New York, NY}},
  isbn = {978-0-8058-0283-2},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\F5YNQANA\\Cohen - 2009 - Statistical power analysis for the behavioral scie.pdf}
}

@article{colling2021,
  title = {Statistical {{Inference}} and the {{Replication Crisis}}},
  author = {Colling, Lincoln J. and Sz{\H u}cs, D{\'e}nes},
  year = {2021},
  month = mar,
  journal = {Review of Philosophy and Psychology},
  volume = {12},
  number = {1},
  pages = {121--147},
  issn = {1878-5166},
  doi = {10.1007/s13164-018-0421-4},
  urldate = {2021-09-27},
  abstract = {The replication crisis has prompted many to call for statistical reform within the psychological sciences. Here we examine issues within Frequentist statistics that may have led to the replication crisis, and we examine the alternative\textemdash Bayesian statistics\textemdash that many have suggested as a replacement. The Frequentist approach and the Bayesian approach offer radically different perspectives on evidence and inference with the Frequentist approach prioritising error control and the Bayesian approach offering a formal method for quantifying the relative strength of evidence for hypotheses. We suggest that rather than mere statistical reform, what is needed is a better understanding of the different modes of statistical inference and a better understanding of how statistical inference relates to scientific inference.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\8DCR5TUU\\Colling und Szűcs - 2021 - Statistical Inference and the Replication Crisis.pdf}
}

@article{colling2021a,
  title = {Statistical {{Inference}} and the {{Replication Crisis}}},
  author = {Colling, Lincoln J. and Sz{\H u}cs, D{\'e}nes},
  year = {2021},
  month = mar,
  journal = {Review of Philosophy and Psychology},
  volume = {12},
  number = {1},
  pages = {121--147},
  issn = {1878-5166},
  doi = {10.1007/s13164-018-0421-4},
  urldate = {2023-04-19},
  abstract = {The replication crisis has prompted many to call for statistical reform within the psychological sciences. Here we examine issues within Frequentist statistics that may have led to the replication crisis, and we examine the alternative\textemdash Bayesian statistics\textemdash that many have suggested as a replacement. The Frequentist approach and the Bayesian approach offer radically different perspectives on evidence and inference with the Frequentist approach prioritising error control and the Bayesian approach offering a formal method for quantifying the relative strength of evidence for hypotheses. We suggest that rather than mere statistical reform, what is needed is a better understanding of the different modes of statistical inference and a better understanding of how statistical inference relates to scientific inference.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\3IUAM2K4\\Colling und Szűcs - 2021 - Statistical Inference and the Replication Crisis.pdf}
}

@article{cooper2018,
  title = {The {{Replication Crisis}} and {{Chemistry Education Research}}},
  author = {Cooper, Melanie M.},
  year = {2018},
  month = jan,
  journal = {Journal of Chemical Education},
  volume = {95},
  number = {1},
  pages = {1--2},
  issn = {0021-9584, 1938-1328},
  doi = {10.1021/acs.jchemed.7b00907},
  urldate = {2021-09-23},
  abstract = {There is a growing acknowledgment that some research findings in fields as diverse as medicine and psychology may not be replicable. There are many reasons for this, including a reliance on small sample sizes, and retroactive choice of data to analyze. It has become clear that the traditional measure of statistical significance (p {$<$} 0.05) is not enough to support research claims. In this editorial I discuss the background and some possible solutions.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\BXERPQCF\\Cooper - 2018 - The Replication Crisis and Chemistry Education Res.pdf}
}

@article{cox1952,
  title = {Sequential Tests for Composite Hypotheses},
  author = {Cox, D R},
  year = {1952},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {48},
  number = {2},
  pages = {290--299},
  doi = {10.1017/S030500410002764X},
  abstract = {A method is given for obtaining sequential tests in the presence of nuisance parameters. It is assumed that a jointly sufficient set of estimators exists for the unknown parameters. A number of special tests are described and their properties discussed.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IGMXPJIM\\Cox - Sequential tests for composite hypotheses.pdf}
}

@article{cumming,
  title = {A {{Primer}} on the {{Understanding}}, {{Use}}, and {{Calculation}} of {{Confidence Intervals}} That Are {{Based}} on {{Central}} and {{Noncentral Distributions}}},
  author = {Cumming, Geoff and Finch, Sue},
  journal = {EDUCATIONAL AND PSYCHOLOGICAL MEASUREMENT},
  pages = {43},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2B7FKJBZ\\Cumming und Finch - A Primer on the Understanding, Use, and Calculatio.pdf}
}

@article{cumming2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613504966},
  urldate = {2021-09-23},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate dataanalytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\B8Q6D39A\\Cumming - 2014 - The New Statistics Why and How.pdf}
}

@article{damirkalpic,
  title = {Student's t-{{Tests}}},
  author = {Damir Kalpic{\textasciiacute}, Barndorff-Nielsen},
  journal = {s t},
  pages = {5},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JFTLAYZ3\\Oe - References and Further Reading.pdf}
}

@article{delacre2019,
  title = {Taking {{Parametric Assumptions Seriously}}: {{Arguments}} for the {{Use}} of {{Welch}}'s {{{\emph{F}}}}-Test Instead of the {{Classical}} {{{\emph{F}}}}-Test in {{One-Way ANOVA}}},
  shorttitle = {Taking {{Parametric Assumptions Seriously}}},
  author = {Delacre, Marie and Leys, Christophe and Mora, Youri L. and Lakens, Dani{\"e}l},
  year = {2019},
  month = aug,
  journal = {International Review of Social Psychology},
  volume = {32},
  number = {1},
  pages = {13},
  publisher = {{Ubiquity Press}},
  issn = {2397-8570},
  doi = {10.5334/irsp.198},
  urldate = {2022-08-09},
  abstract = {Student's t-test and classical F-test ANOVA rely on the assumptions that two or more samples are independent, and that independent and identically distributed residuals are normal and have equal variances between groups. We focus on the assumptions of normality and equality of variances, and argue that these assumptions are often unrealistic in the field of psychology. We underline the current lack of attention to these assumptions through an analysis of researchers' practices. Through Monte Carlo simulations, we illustrate the consequences of performing the classic parametric F-test for ANOVA when the test assumptions are not met on the Type I error rate and statistical power. Under realistic deviations from the assumption of equal variances, the classic F-test can yield severely biased results and lead to invalid statistical inferences. We examine two common alternatives to the F-test, namely the Welch's ANOVA (W-test) and the Brown-Forsythe test (F*-test). Our simulations show that under a range of realistic scenarios, the W-test is a better alternative and we therefore recommend using the W-test by default when comparing means. We provide a detailed example explaining how to perform the W-test in SPSS and R. We summarize our conclusions in practical recommendations that researchers can use to improve their statistical practices.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KU9NQPIB\\Delacre et al. - 2019 - Taking Parametric Assumptions Seriously Arguments.pdf;C\:\\Users\\Admin\\Zotero\\storage\\MZPST6MP\\irsp.198.html}
}

@article{derosario2020,
  title = {Pwr: {{Basic}} Functions for Power Analysis  {\emph{(}}{{{\emph{Version}}}}{\emph{ 1.3.0) [}}{{{\emph{R Package}}}}{\emph{]}}},
  author = {De Rosario, Helios and Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert},
  year = {2020},
  doi = {https://CRAN.R-project.org/package=pwr},
  keywords = {1 paper,software}
}

@article{dienes2011,
  title = {Bayesian {{Versus Orthodox Statistics}}: {{Which Side Are You On}}?},
  shorttitle = {Bayesian {{Versus Orthodox Statistics}}},
  author = {Dienes, Zoltan},
  year = {2011},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {6},
  number = {3},
  pages = {274--290},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691611406920},
  urldate = {2021-09-23},
  abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing\textemdash two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LHSID3L5\\Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Ar.pdf}
}

@article{diepgen,
  title = {{Sequentielles Testen}},
  author = {Diepgen, Raphael},
  publisher = {{Universit\"atsbibliothek Mannheim}},
  doi = {10.25521/HQM11},
  urldate = {2021-09-23},
  langid = {ngerman},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\493CQIRI\\Diepgen - Sequentielles Testen.pdf}
}

@article{donaldson1968,
  title = {Robustness of the {{F-Test}} to {{Errors}} of {{Both Kinds}} and the {{Correlation Between}} the {{Numerator}} and {{Denominator}} of the {{F-Ratio}}},
  author = {Donaldson, Theodore S.},
  year = {1968},
  journal = {Journal of the American Statistical Association},
  volume = {63},
  number = {322},
  eprint = {2284037},
  eprinttype = {jstor},
  pages = {660--676},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2284037},
  urldate = {2022-07-25},
  abstract = {In this study of robustness the insensitivity of the F-test between means to its underlying assumptions (normally distributed populations with equal variances) is investigated. Using two nonnormal distributions (exponential and lognormal), it is found that the test is fairly insensitive for moderate and equal sample size (n = 32) when the variances are equal. Further, for small samples {$<$}latex{$>\$$}(n {$<$} 32)\${$<$}/latex{$>$}, the test is conservative with respect to Type I error. It is also conservative with respect to Type II error for a large range of {$\varphi$} (noncentrality), depending on the size of the sample and {$\alpha$}. When the within cell error variances are heterogenous, the test continues to be conservative for the upper values of {$\varphi$} and slightly biased toward larger Type II errors for smaller values of {$\varphi$} depending on the size of {$\alpha$}. Analysis of the correlation between the numerator and denominator of F under the null hypothesis indicates that the robustness feature is largely due to this correlation. Analytic proofs under the non-null hypothesis were not possible, but some empirical results are presented.},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\536VS54A\\Donaldson - 1968 - Robustness of the F-Test to Errors of Both Kinds a.pdf}
}

@article{eisenhart1947,
  title = {The {{Assumptions Underlying}} the {{Analysis}} of {{Variance}}},
  author = {Eisenhart, Churchill},
  year = {1947},
  journal = {Biometrics},
  volume = {3},
  number = {1},
  eprint = {3001534},
  eprinttype = {jstor},
  pages = {1--21},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/3001534},
  urldate = {2023-03-02},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AU7S893V\\Eisenhart - 1947 - The Assumptions Underlying the Analysis of Varianc.pdf}
}

@article{emerson,
  title = {Parameter Estimation Following Group Sequential Hypothesis Testing},
  author = {Emerson, Scott S and Fleming, Thomas R},
  pages = {18},
  abstract = {Parameter estimation techniques which fail to adjust for the interim analyses of group sequential test designs will introduce bias in much the same way that the repeated use of single sample hypothesis testing causes inflation of the type one statistical error rate. Methods based on the duality of hypothesis testing and interval estimation require definition of an ordering for the outcome space for the test statistic. In this paper, estimation following a group sequential hypothesis test for the mean of a normal distribution with known variance is investigated. A proposed ordering of the sample space based on the maximum likelihood estimate of the mean is found to result in estimates which compare favourably with estimates computed from orderings investigated by Tsiatis, Rosner \& Mehta (1984) and Chang \& O'Brien (1986) for a variety of group sequential designs. The proposed ordering is then adapted for use when the sizes of groups accrued between analyses is random.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\83XHFR9L\\Emerson und Fleming - Parameter estimation following group sequential hy.pdf}
}

@article{emerson1990,
  title = {Parameter Estimation Following Group Sequential Hypothesis Testing},
  author = {EMERSON, SCOTT S. and FLEMING, THOMAS R.},
  year = {1990},
  month = dec,
  journal = {Biometrika},
  volume = {77},
  number = {4},
  pages = {875--892},
  issn = {0006-3444},
  doi = {10.1093/biomet/77.4.875},
  urldate = {2023-02-27},
  abstract = {Parameter estimation techniques which fail to adjust for the interim analyses of group sequential test designs will introduce bias in much the same way that the repeated use of single sample hypothesis testing causes inflation of the type one statistical error rate. Methods based on the duality of hypothesis testing and interval estimation require definition of an ordering for the outcome space for the test statistic. In this paper, estimation following a group sequential hypothesis test for the mean of a normal distribution with known variance is investigated. A proposed ordering of the sample space based on the maximum likelihood estimate of the mean is found to result in estimates which compare favourably with estimates computed from orderings investigated by Tsiatis, Rosner \&amp; Mehta (1984) and Chang \&amp; O'Brien (1986) for a variety of group sequential designs. The proposed ordering is then adapted for use when the sizes of groups accrued between analyses is random.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AL8PES8Z\\EMERSON und FLEMING - 1990 - Parameter estimation following group sequential hy.pdf;C\:\\Users\\Admin\\Zotero\\storage\\TCLTDPPD\\291010.html}
}

@article{emerson2007,
  title = {Bayesian Evaluation of Group Sequential Clinical Trial Designs},
  author = {Emerson, Scott S. and Kittelson, John M. and Gillen, Daniel L.},
  year = {2007},
  month = mar,
  journal = {Statistics in Medicine},
  volume = {26},
  number = {7},
  pages = {1431--1449},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.2640},
  urldate = {2021-09-23},
  abstract = {Clinical trial designs often incorporate a sequential stopping rule to serve as a guide in the early termination of a study. When choosing a particular stopping rule, it is most common to examine frequentist operating characteristics such as type I error, statistical power, and precision of confidence intervals (Statist. Med. 2005, in revision). Increasingly, however, clinical trials are designed and analysed in the Bayesian paradigm. In this paper, we describe how the Bayesian operating characteristics of a particular stopping rule might be evaluated and communicated to the scientific community. In particular, we consider a choice of probability models and a family of prior distributions that allows concise presentation of Bayesian properties for a specified sampling plan. Copyright q 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2DPRRRLT\\Emerson et al. - 2007 - Bayesian evaluation of group sequential clinical t.pdf}
}

@article{emerson2007a,
  title = {Frequentist Evaluation of Group Sequential Clinical Trial Designs},
  author = {Emerson, Scott S. and Kittelson, John M. and Gillen, Daniel L.},
  year = {2007},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {26},
  number = {28},
  pages = {5047--5080},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.2901},
  urldate = {2021-09-23},
  abstract = {Group sequential stopping rules are often used as guidelines in the monitoring of clinical trials in order to address the ethical and efficiency issues inherent in human testing of a new treatment or preventive agent for disease. Such stopping rules have been proposed based on a variety of different criteria, both scientific (e.g. estimates of treatment effect) and statistical (e.g. frequentist type I error, Bayesian posterior probabilities, stochastic curtailment). It is easily shown, however, that a stopping rule based on one of these criteria induces a stopping rule on all other criteria. Thus, the basis used to initially define a stopping rule is relatively unimportant so long as the operating characteristics of the stopping rule are fully investigated. In this paper we describe how the frequentist operating characteristics of a particular stopping rule might be evaluated to ensure that the selected clinical trial design satisfies the constraints imposed by the many different disciplines represented by the clinical trial collaborators. Copyright q 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9SSSE7GH\\Emerson et al. - 2007 - Frequentist evaluation of group sequential clinica.pdf}
}

@article{erdfelder1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  year = {1996},
  month = mar,
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {28},
  number = {1},
  pages = {1--11},
  issn = {1532-5970},
  doi = {10.3758/BF03203630},
  urldate = {2023-04-12},
  abstract = {GPOWER is a completely interactive, menu-driven program for IBM-compatible and Apple Macintosh personal computers. It performs high-precision statistical power analyses for the most common statistical tests in behavioral research, that is,t tests,F tests, and{$\chi$}2 tests. GPOWER computes (1) power values for given sample sizes, effect sizes and{$\alpha$} levels (post hoc power analyses); (2) sample sizes for given effect sizes,{$\alpha$} levels, and power values (a priori power analyses); and (3){$\alpha$} and{$\beta$} values for given sample sizes, effect sizes, and{$\beta$}/{$\alpha$} ratios (compromise power analyses). The program may be used to display graphically the relation between any two of the relevant variables, and it offers the opportunity to compute the effect size measures from basic parameters defining the alternative hypothesis. This article delineates reasons for the development of GPOWER and describes the program's capabilities and handling.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2IN5YR87\\Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf}
}

@article{erdfelder2021,
  title = {On the Efficiency of the Independent Segments Procedure: {{A}} Direct Comparison with Sequential Probability Ratio Tests.},
  shorttitle = {On the Efficiency of the Independent Segments Procedure},
  author = {Erdfelder, Edgar and Schnuerch, Martin},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {501--506},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000404},
  urldate = {2023-03-09},
  abstract = {In this comment, we report a simulation study that assesses error rates and average sample sizes required to reach a statistical decision for two sequential procedures, the sequential probability ratio test (SPRT) originally proposed by Wald (1947) and the independent segments procedure (ISP) recently suggested by Miller and Ulrich (2020). Following Miller and Ulrich (2020), we use sequential one-tailed t tests as examples. In line with the optimal efficiency properties of the SPRT already proven by Wald and Wolfowitz (1948), the SPRT outperformed the ISP in terms of efficiency without compromising error probability control. The efficiency gain in terms of sample size reduction achieved with the SPRT t test relative to the ISP may be as high as 25\%. We thus recommend the SPRT as a default sequential testing procedure especially for detecting small or medium hypothesized effect sizes under H1 whenever a priori knowledge of the maximum sample size is not crucial. If a priori control of the maximum sample size is mandatory, however, the ISP is a very useful addition to the sequential testing literature.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ANAZHWD4\\Erdfelder und Schnuerch - 2021 - On the efficiency of the independent segments proc.pdf}
}

@article{eronen2021,
  title = {The {{Theory Crisis}} in {{Psychology}}: {{How}} to {{Move Forward}}},
  shorttitle = {The {{Theory Crisis}} in {{Psychology}}},
  author = {Eronen, Markus I. and Bringmann, Laura F.},
  year = {2021},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {779--788},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691620970586},
  urldate = {2022-04-20},
  abstract = {Meehl argued in 1978 that theories in psychology come and go, with little cumulative progress. We believe that this assessment still holds, as also evidenced by increasingly common claims that psychology is facing a ``theory crisis'' and that psychologists should invest more in theory building. In this article, we argue that the root cause of the theory crisis is that developing good psychological theories is extremely difficult and that understanding the reasons why it is so difficult is crucial for moving forward in the theory crisis. We discuss three key reasons based on philosophy of science for why developing good psychological theories is so hard: the relative lack of robust phenomena that impose constraints on possible theories, problems of validity of psychological constructs, and obstacles to discovering causal relationships between psychological variables. We conclude with recommendations on how to move past the theory crisis.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\3JW2VLRH\\Eronen und Bringmann - 2021 - The Theory Crisis in Psychology How to Move Forwa.pdf}
}

@article{etz,
  title = {Bayesian {{Inference}} and {{Testing Any Hypothesis You Can Specify}}},
  author = {Etz, Alexander and Haaf, Julia M and Rouder, Jeffrey N and Vandekerckhove, Joachim},
  abstract = {Hypothesis testing is a special form of model selection. Once a pair of competing models is fully defined, their definition immediately leads to a measure of how strongly each model supports the data. The ratio of their support is often called the likelihood ratio or the Bayes factor. Critical in the model-selection endeavor is the specification of the models. In the case of hypothesis testing, it is of the greatest importance that the researcher specify exactly what is meant by a ``null'' hypothesis as well as the alternative to which it is contrasted, and that these are suitable instantiations of theoretical positions. Here, we provide an overview of different instantiations of null and alternative hypotheses that can be useful in practice, but in all cases the inferential procedure is based on the same underlying method of likelihood comparison. An associated app can be found at https://osf.io/mvp53/. This article is the work of the authors and is reformatted from the original, which was published under a CC-By Attribution 4.0 International license and is available at https://psyarxiv.com/wmf3r/.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\86KLR3D6\\Etz et al. - Bayesian Inference and Testing Any Hypothesis You .pdf}
}

@article{etz2018,
  title = {Introduction to the Concept of Likelihood and Its Applications},
  author = {Etz, Alexander},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {60--69},
  doi = {10.1177/2515245917744314},
  abstract = {This Tutorial explains the statistical concept known as likelihood and discusses how it underlies common frequentist and Bayesian statistical methods. The article is suitable for researchers interested in understanding the basis of their statistical tools and is also intended as a resource for teachers to use in their classrooms to introduce the topic to students at a conceptual level.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5KYDDP4W\\Etz - Introduction to the Concept of Likelihood and Its .pdf}
}

@article{etz2018a,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {5--34},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  urldate = {2021-09-23},
  abstract = {We introduce the fundamental tenets of Bayesian inference, which derive from two basic laws of probability theory. We cover the interpretation of probabilities, discrete and continuous versions of Bayes' rule, parameter estimation, and model comparison. Using seven worked examples, we illustrate these principles and set up some of the technical background for the rest of this special issue of Psychonomic Bulletin \& Review. Supplemental material is available via https://osf.io/wskex/.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\C4SJ4TGI\\Etz und Vandekerckhove - 2018 - Introduction to Bayesian Inference for Psychology.pdf}
}

@article{etz2018b,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Etz, Alexander and Gronau, Quentin F. and Dablander, Fabian and Edelsbrunner, Peter A. and Baribault, Beth},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {219--234},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1317-5},
  urldate = {2023-06-15},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences. The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. In addition, we outline an additional 32 articles and books that can be consulted to gain background knowledge about various theoretical specifics and Bayesian approaches to frequently used models. Our goal is to offer researchers a starting point for understanding the core tenets of Bayesian analysis, while requiring a low level of time commitment. After consulting our guide, the reader should understand how and why Bayesian methods work, and feel able to evaluate their use in the behavioral and social sciences.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PY3U8S7H\\Etz et al. - 2018 - How to become a Bayesian in eight easy steps An a.pdf}
}

@article{fan2004,
  title = {Conditional {{Bias}} of {{Point Estimates Following}} a {{Group Sequential Test}}},
  author = {Fan, Xiaoyin (Frank) and DeMets, David L. and Lan, K. K. Gordon},
  year = {2004},
  month = dec,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {14},
  number = {2},
  pages = {505--530},
  publisher = {{Taylor \& Francis}},
  issn = {1054-3406},
  doi = {10.1081/BIP-120037195},
  urldate = {2022-07-11},
  abstract = {Repeated significance testing in a sequential experiment not only increases the overall type I error rate of the false positive conclusion but also causes biases in estimating the unknown parameter. In general, the test statistics in a sequential trial can be properly approximated by a Brownian motion with a drift parameter at interim looks. The unadjusted maximum likelihood estimator can be potentially very biased due to the possible early stopping rule at any interim. In this paper, we investigate the conditional and marginal biases with focus on the conditional one upon the stopping time in estimating the Brownian motion drift parameter. It is found that the conditional bias may be very serious for existing point estimation methods, even if the unconditional bias is satisfactory. New conditional estimators are thus proposed, which can significantly reduce the conditional bias from unconditional estimators. The results of Monte-Carlo studies show that the proposed estimators can provide a much smaller conditional bias and MSE than the naive MLE and a Whitebead's bias reduced estimator.},
  pmid = {15206542},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LYZ85CC2\\Fan et al. - 2004 - Conditional Bias of Point Estimates Following a Gr.pdf;C\:\\Users\\Admin\\Zotero\\storage\\JYUPEIGR\\BIP-120037195.html}
}

@article{feest2019,
  title = {Why {{Replication Is Overrated}}},
  author = {Feest, Uljana},
  year = {2019},
  month = dec,
  journal = {Philosophy of Science},
  volume = {86},
  number = {5},
  pages = {895--905},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/705451},
  urldate = {2022-04-20},
  abstract = {Current debates about the replication crisis in psychology take it for granted that direct replication is valuable, largely focusing on its role in uncovering questionable statistical practices. This article takes a broader look at the notion of replication in psychological experiments. It is argued that all experimentation/replication involves individuation judgments and that research in experimental psychology frequently turns on probing the adequacy of such judgments. In this vein, I highlight the ubiquity of conceptual and material questions in research, arguing that replication has its place but is not as central to psychological research as it is sometimes taken to be.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\826HL6SV\\Feest - 2019 - Why Replication Is Overrated.pdf}
}

@article{fidler,
  title = {Computing {{Correct Confidence Intervals}} for {{Anova Fixed-and Random-Effects Effect Sizes}}},
  author = {Fidler, Fiona and Thompson, Bruce},
  journal = {EDUCATIONAL AND PSYCHOLOGICAL MEASUREMENT},
  pages = {30},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QQ758T6G\\Fidler und Thompson - Computing Correct Confidence Intervals for Anova F.pdf}
}

@article{fiedler2017,
  title = {What {{Constitutes Strong Psychological Science}}? {{The}} ({{Neglected}}) {{Role}} of {{Diagnosticity}} and {{A Priori Theorizing}}},
  shorttitle = {What {{Constitutes Strong Psychological Science}}?},
  author = {Fiedler, Klaus},
  year = {2017},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {1},
  pages = {46--61},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691616654458},
  urldate = {2022-04-20},
  abstract = {A Bayesian perspective on Ioannidis's (2005) memorable statement that ``Most Published Research Findings Are False'' suggests a seemingly inescapable trade-off: It appears as if research hypotheses are based either on safe ground (high prior odds), yielding valid but unsurprising results, or on unexpected and novel ideas (low prior odds), inspiring risky and surprising findings that are inevitably often wrong. Indeed, research of two prominent types, sexy hypothesis testing and model testing, is often characterized by low priors (due to astounding hypotheses and conjunctive models) as well as low-likelihood ratios (due to nondiagnostic predictions of the yin-or-yang type). However, the trade-off is not inescapable: An alternative research approach, theory-driven cumulative science, aims at maximizing both prior odds and diagnostic hypothesis testing. The final discussion emphasizes the value of pluralistic science, within which exploratory phenomenon-driven research can play a similarly strong part as strict theory-testing science.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Y4GNGZNI\\Fiedler - 2017 - What Constitutes Strong Psychological Science The.pdf}
}

@article{fiedler2018,
  title = {The {{Regression Trap}} and {{Other Pitfalls}} of {{Replication Science}}\textemdash{{Illustrated}} by the {{Report}} of the {{Open Science Collaboration}}},
  author = {Fiedler, Klaus and Prager, Johannes},
  year = {2018},
  month = may,
  journal = {Basic and Applied Social Psychology},
  volume = {40},
  number = {3},
  pages = {115--124},
  issn = {0197-3533, 1532-4834},
  doi = {10.1080/01973533.2017.1421953},
  urldate = {2022-04-20},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9ZACSKZN\\Fiedler und Prager - 2018 - The Regression Trap and Other Pitfalls of Replicat.pdf}
}

@book{fisher1925,
  title = {Statistical Methods for Research Workers},
  author = {Fisher, {\relax RA}},
  year = {1925},
  publisher = {{Oliver and Boyd}},
  address = {{London}}
}

@article{fisher1939,
  title = {``{{STUDENT}}''},
  author = {Fisher, R. A.},
  year = {1939},
  month = jan,
  journal = {Annals of Eugenics},
  volume = {9},
  number = {1},
  pages = {1--9},
  issn = {20501420},
  doi = {10.1111/j.1469-1809.1939.tb02192.x},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\MQZMYZDU\\Fisher - 1939 - “STUDENT”.pdf}
}

@article{fisher1950,
  title = {Statistical {{Methods}} for {{Research Workers}}},
  author = {Fisher, R A},
  year = {1950},
  pages = {374},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\3DIABGN2\\Fisher - Statistical Methods for Rese.arch Workers.pdf}
}

@article{fisher1955,
  title = {Statistical {{Methods}} and {{Scientific Induction}}},
  author = {Fisher, Ronald},
  year = {1955},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {17},
  number = {1},
  pages = {69--78},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1955.tb00180.x},
  urldate = {2022-04-08},
  langid = {english}
}

@article{fleishman1980,
  title = {Confidence {{Intervals}} for {{Correlation Ratios}}},
  author = {Fleishman, Allen I.},
  year = {1980},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {40},
  number = {3},
  pages = {659--670},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/001316448004000309},
  urldate = {2022-08-12},
  abstract = {It is suggested that for fixed effects experiments, the traditional statistical question is inappropriate. It is suggested that variance ratios\textemdash the signal to noise ratio ({$\sigma$}               2               a               /{$\sigma$}               2               e               ) and the correlation ratio ({$\sigma$}               2               a               /{$\sigma$}               2               t               )\textemdash are more desirable. Criticisms of these values are explored. Finally confidence intervals and percentiles for the random and fixed effects model's signal to noise and correlation ratio are given. The latter being dependent on the evaluation of a family of non-central F distributions. The latter will also give the confidence interval and percentiles of the multiple correlation coefficient in the regression model. The lower limit of the confidence interval can test the traditional null hypothesis. The upper limit of this interval is suggested to be a test which would allow the investigator to accept the null hypothesis.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XBJLF2VT\\Fleishman - 1980 - Confidence Intervals for Correlation Ratios.pdf}
}

@article{fleishman1980a,
  title = {Confidence {{Intervals}} for {{Correlation Ratios}}},
  author = {Fleishman, Allen I.},
  year = {1980},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {40},
  number = {3},
  pages = {659--670},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/001316448004000309},
  urldate = {2023-02-27},
  abstract = {It is suggested that for fixed effects experiments, the traditional statistical question is inappropriate. It is suggested that variance ratios\textemdash the signal to noise ratio ({$\sigma$}               2               a               /{$\sigma$}               2               e               ) and the correlation ratio ({$\sigma$}               2               a               /{$\sigma$}               2               t               )\textemdash are more desirable. Criticisms of these values are explored. Finally confidence intervals and percentiles for the random and fixed effects model's signal to noise and correlation ratio are given. The latter being dependent on the evaluation of a family of non-central F distributions. The latter will also give the confidence interval and percentiles of the multiple correlation coefficient in the regression model. The lower limit of the confidence interval can test the traditional null hypothesis. The upper limit of this interval is suggested to be a test which would allow the investigator to accept the null hypothesis.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QKP6EV3S\\Fleishman - 1980 - Confidence Intervals for Correlation Ratios.pdf}
}

@article{fraser,
  title = {Crisis in {{Science}}? Or {{Crisis}} in {{Statistics}}! {{Mixed}} Messages in {{Statistics}} with Impact on {{Science}}},
  author = {Fraser, D A S and Reid, N},
  pages = {14},
  abstract = {Gelman and Loken (2014) draw attention to a ``statistical crisis in science'' and describe how risks with multiple p-values can be present even in the analysis of a single data set. There is indeed a crisis, as p-values are everywhere, in science, engineering, medicine, social science, health care, and the media; and conflicting results are misrepresenting the importance of p-values, and indeed of many disciplines themselves. We argue that risks of misinterpretation are widespread, but that the crisis is really in the discipline of statistics, and starts with mixed messages about the meaning and usage of p-values. These mixed messages then have downstream effects that seriously misinform scientific endeavours. What are these mixed messages concerning p-values? And should statistics continue with such messages that compromise the discipline? We discuss this and offer recommendations.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\W8TGI4EQ\\Fraser und Reid - Crisis in Science or Crisis in Statistics! Mixed .pdf}
}

@article{freedman1989,
  title = {Comparison of {{Bayesian}} with Group Sequential Methods for Monitoring Clinical Trials},
  author = {Freedman, Laurence S. and Spiegelhalter, David J.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4},
  pages = {357--367},
  issn = {01972456},
  doi = {10.1016/0197-2456(89)90001-9},
  urldate = {2021-09-23},
  abstract = {We describe some problems with applying methods based on classical sequential analysis to monitoring clinical trials. A Bayesian method is developed and the boundaries are compared with frequentist schemes. For the examples chosen, the Bayesian boundaries can be quite similar to those obtained from Pocock and O'Brien and Fleming (OBF) rules, depending on the choice of prior distribution. They converge less rapidly than OBF's but more rapidly than Pocock's. In general the Bayesian methods provide the same desirable features as frequentist methods, without sacrificing flexibility and simplicity of interpretation.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\RAUYC8NP\\Freedman und Spiegelhalter - 1989 - Comparison of Bayesian with group sequential metho.pdf}
}

@article{fritz2012,
  title = {Effect Size Estimates: {{Current}} Use, Calculations, and Interpretation.},
  shorttitle = {Effect Size Estimates},
  author = {Fritz, Catherine O. and Morris, Peter E. and Richler, Jennifer J.},
  year = {2012},
  journal = {Journal of Experimental Psychology: General},
  volume = {141},
  number = {1},
  pages = {2--18},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0024338},
  urldate = {2023-04-13},
  abstract = {The Publication Manual of the American Psychological Association (American Psychological Association, 2001, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial ␩2 was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen's d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\W8LG8DJK\\Fritz et al. - 2012 - Effect size estimates Current use, calculations, .pdf}
}

@article{funder2019,
  title = {Evaluating Effect Size in Psychological Research: {{Sense}} and Nonsense},
  shorttitle = {Evaluating Effect Size in Psychological Research},
  author = {Funder, David C. and Ozer, Daniel J.},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245919847202},
  urldate = {2023-03-09},
  abstract = {Effect sizes are underappreciated and often misinterpreted\textemdash the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size ( r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AKANQJRR\\Funder und Ozer - 2019 - Evaluating Effect Size in Psychological Research .pdf}
}

@article{gallistel2009,
  title = {The {{Importance}} of {{Proving}} the {{Null}}},
  author = {Gallistel, C. R.},
  year = {2009},
  month = apr,
  journal = {Psychological review},
  volume = {116},
  number = {2},
  pages = {439--453},
  issn = {0033-295X},
  doi = {10.1037/a0015251},
  urldate = {2023-04-17},
  abstract = {Null hypotheses are simple, precise, and theoretically important. Conventional statistical analysis cannot support them; Bayesian analysis can. The challenge in a Bayesian analysis is to formulate a suitably vague alternative, because the vaguer the alternative is (the more it spreads out the unit mass of prior probability), the more the null is favored. A general solution is a sensitivity analysis: Compute the odds for or against the null as a function of the limit(s) on the vagueness of the alternative. If the odds on the null approach 1 from above as the hypothesized maximum size of the possible effect approaches 0, then the data favor the null over any vaguer alternative to it. The simple computations and the intuitive graphic representation of the analysis are illustrated by the analysis of diverse examples from the current literature. They pose 3 common experimental questions: (a) Are 2 means the same? (b) Is performance at chance? (c) Are factors additive?},
  pmcid = {PMC2859953},
  pmid = {19348549},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UDIG5FQ2\\Gallistel - 2009 - The Importance of Proving the Null.pdf}
}

@article{ganwang1997,
  title = {Bias Reduction via Resampling for Estimation Following Sequential Tests: {{Bias}} Reduction via Resampling for Estimation},
  shorttitle = {Bias Reduction via Resampling for Estimation Following Sequential Tests},
  author = {Gan Wang, You and Yan Leung, Denis Heng},
  year = {1997},
  month = jan,
  journal = {Sequential Analysis},
  volume = {16},
  number = {3},
  pages = {249--267},
  issn = {0747-4946, 1532-4176},
  doi = {10.1080/07474949708836386},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IVDS9BL4\\Gan Wang und Yan Leung - 1997 - Bias reduction via resampling for estimation follo.pdf}
}

@article{gao2020,
  title = {P-Values \textendash{} a Chronic Conundrum},
  author = {Gao, Jian},
  year = {2020},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {167},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01051-6},
  urldate = {2023-04-12},
  abstract = {In medical research and practice, the p-value is arguably the most often used statistic and yet it is widely misconstrued as the probability of the type I error, which comes with serious consequences. This misunderstanding can greatly affect the reproducibility in research, treatment selection in medical practice, and model specification in empirical analyses. By using plain language and concrete examples, this paper is intended to elucidate the p-value confusion from its root, to explicate the difference between significance and hypothesis testing, to illuminate the consequences of the confusion, and to present a viable alternative to the conventional p-value.},
  langid = {english},
  keywords = {Calibrated P-values,Hypothesis testing,P-values,Research reproducibility,Significance testing,Type I error},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KN3K8JQI\\Gao - 2020 - P-values – a chronic conundrum.pdf}
}

@misc{gelman2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  number = {arXiv:2011.01808},
  eprint = {2011.01808},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2022-06-27},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\VRCJCDUQ\\Gelman et al. - 2020 - Bayesian Workflow.pdf}
}

@article{gibson2021,
  title = {The {{Role}} of p -{{Values}} in {{Judging}} the {{Strength}} of {{Evidence}} and {{Realistic Replication Expectations}}},
  author = {Gibson, Eric W.},
  year = {2021},
  month = jan,
  journal = {Statistics in Biopharmaceutical Research},
  volume = {13},
  number = {1},
  pages = {6--18},
  issn = {1946-6315},
  doi = {10.1080/19466315.2020.1724560},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\BHMRZ2FL\\Gibson - 2021 - The Role of p -Values in Judging the Streng.pdf}
}

@book{gigerenzer1989,
  title = {The Empire of Chance: {{How}} Probability Changed Science and Everyday Life},
  shorttitle = {The Empire of Chance},
  author = {Gigerenzer, Gerd},
  year = {1989},
  publisher = {{Cambridge University Press}},
  abstract = {This book tells how quantitative ideas of chance have transformed the natural and social sciences as well as everyday life over the past three centuries. A continuous narrative connects the earliest application of probability and statistics in gambling and insurance to the most recent forays into law, medicine, polling, and baseball. Separate chapters explore the theoretical and methodological impact on biology, physics, and psychology. In contrast to the literature on the mathematical development of probability and statistics, this book centers on how these technical innovations recreated our conceptions of nature, mind, and society.},
  googlebooks = {Bw2yKfpvts8C},
  isbn = {978-0-521-39838-1},
  langid = {english},
  keywords = {1 paper}
}

@article{gigerenzer2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {10535357},
  doi = {10.1016/j.socec.2004.09.033},
  urldate = {2021-09-23},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\4M5TCDF2\\Gigerenzer - 2004 - Mindless statistics.pdf}
}

@article{gilbert2016,
  title = {Comment on "{{Estimating}} the Reproducibility of Psychological Science"},
  author = {Gilbert, D. T. and King, G. and Pettigrew, S. and Wilson, T. D.},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aad7243},
  urldate = {2021-09-23},
  abstract = {One way the Open Science Collaboration's recent report (1, hereinafter referred to as OSC2015) defines a ``successful replication'' is as one in which the 95\% confidence interval (CI) from the replication captures the point estimate from the original published result (see OSC2015 Table 1, column 10). This does not make statistical sense because the effect that they were trying to replicate was that of the original study, not of the replication. Thus, we inverted the calculation to determine whether the point estimates from the replications were captured within the 95\% CI of the original article. Making this switch has a negligible impact on the numbers reported below and changes no conclusions.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PU8ADBSP\\Gilbert et al. - 2016 - Comment on Estimating the reproducibility of psyc.pdf}
}

@article{glass1972,
  title = {Consequences of Failure to Meet Assumptions Underlying the Fixed Effects Analyses of Variance and Covariance},
  author = {Glass, Gene V and Peckham, Percy D and Sanders, James R},
  year = {1972},
  journal = {Review of Educational Research},
  volume = {42},
  number = {3},
  pages = {237--288},
  doi = {10.3102/00346543042003237},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Y42KJVTW\\Glass et al. - Consequences of Failure to Meet Assumptions Underl.pdf}
}

@article{goodman1993,
  title = {P {{Values}}, {{Hypothesis Tests}}, and {{Likelihood}}: {{Implications}} for {{Epidemiology}} of a {{Neglected Historical Debate}}},
  shorttitle = {P {{Values}}, {{Hypothesis Tests}}, and {{Likelihood}}},
  author = {Goodman, Steven N.},
  year = {1993},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {137},
  number = {5},
  pages = {485--496},
  issn = {1476-6256, 0002-9262},
  doi = {10.1093/oxfordjournals.aje.a116700},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZIWI3EFP\\Goodman - 1993 - p Values, Hypothesis Tests, and Likelihood Implic.pdf}
}

@article{goodman2007c,
  title = {Stopping at {{Nothing}}? {{Some Dilemmas}} of {{Data Monitoring}} in {{Clinical Trials}}},
  shorttitle = {Stopping at {{Nothing}}?},
  author = {Goodman, Steven N.},
  year = {2007},
  month = jun,
  journal = {Annals of Internal Medicine},
  volume = {146},
  number = {12},
  pages = {882},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-146-12-200706190-00010},
  urldate = {2023-03-14},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TIBE8GCZ\\Goodman - 2007 - Stopping at Nothing Some Dilemmas of Data Monitor.pdf}
}

@article{goodman2008,
  title = {A {{Dirty Dozen}}: {{Twelve P-Value Misconceptions}}},
  shorttitle = {A {{Dirty Dozen}}},
  author = {Goodman, Steven},
  year = {2008},
  month = jul,
  journal = {Seminars in Hematology},
  series = {Interpretation of {{Quantitative Research}}},
  volume = {45},
  number = {3},
  pages = {135--140},
  issn = {0037-1963},
  doi = {10.1053/j.seminhematol.2008.04.003},
  urldate = {2023-02-15},
  abstract = {The P value is a measure of statistical evidence that appears in virtually all medical research papers. Its interpretation is made extraordinarily difficult because it is not part of any formal system of statistical inference. As a result, the P value's inferential meaning is widely and often wildly misconstrued, a fact that has been pointed out in innumerable papers and books appearing since at least the 1940s. This commentary reviews a dozen of these common misinterpretations and explains why each is wrong. It also reviews the possible consequences of these improper understandings or representations of its meaning. Finally, it contrasts the P value with its Bayesian counterpart, the Bayes' factor, which has virtually all of the desirable properties of an evidential measure that the P value lacks, most notably interpretability. The most serious consequence of this array of P-value misconceptions is the false belief that the probability of a conclusion being in error can be calculated from the data in a single experiment without reference to external evidence or the plausibility of the underlying mechanism.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\FLKWR64M\\Goodman - 2008 - A Dirty Dozen Twelve P-Value Misconceptions.pdf;C\:\\Users\\Admin\\Zotero\\storage\\QEXBFVPW\\S0037196308000620.html}
}

@article{goulet-pelletier2018,
  title = {A Review of Effect Sizes and Their Confidence Intervals, {{Part I}}: {{The Cohen}}'s d Family},
  shorttitle = {A Review of Effect Sizes and Their Confidence Intervals, {{Part I}}},
  author = {{Goulet-Pelletier}, Jean-Christophe and Cousineau, Denis},
  year = {2018},
  month = dec,
  journal = {The Quantitative Methods for Psychology},
  volume = {14},
  number = {4},
  pages = {242--265},
  issn = {2292-1354},
  doi = {10.20982/tqmp.14.4.p242},
  urldate = {2022-08-12},
  abstract = {Effect sizes and confidence intervals are important statistics to assess the magnitude and the precision of an effect. The various standardized effect sizes can be grouped in three categories depending on the experimental design: measures of the difference between two means (the d family), measures of strength of association (e.g., r, R2, {$\eta$}2, {$\omega$}2), and risk estimates (e.g., odds ratio, relative risk, phi; Kirk, 1996). Part I of this study reviews the d family, with a special focus on Cohen's d and Hedges' g for two-independent groups and two-repeated measures (or paired samples) designs. The present paper answers questions concerning the d family via Monte Carlo simulations. First, four different denominators are often proposed to standardize the mean difference in a repeated measures design. Which one should be used? Second, the literature proposes several approximations to estimate the standard error. Which one most closely estimates the true standard deviation of the distribution? Lastly, central and noncentral methods have been proposed to construct a confidence interval around d. Which method leads to more precise coverage, and how to calculate it? Results suggest that the best way to standardize the effect in both designs is by using the pooled standard deviation in conjunction with a correction factor to unbias d. Likewise, the best standard error approximation is given by substituting the gamma function from the true formula by its approximation. Lastly, results from the confidence interval simulations show that, under the normality assumption, the noncentral method is always superior, especially with small sample sizes. However, the central method is equivalent to the noncentral method when n is greater than 20 in each group for a between-group design and when n is greater than 24 pairs of observations for a repeated measures design. A practical guide to apply the findings of this study can be found after the general discussion.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\L59DISFQ\\p242.pdf}
}

@book{govindarajulu1975,
  title = {Sequential Statistical Procedures},
  author = {Govindarajulu, Z.},
  year = {1975},
  series = {Probability and Mathematical Statistics},
  number = {v. 26},
  publisher = {{Academic Press}},
  address = {{New York}},
  isbn = {978-0-12-294250-1},
  langid = {english},
  lccn = {QA279.7 .G68},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\C7RYJM5J\\Govindarajulu - 1975 - Sequential statistical procedures.pdf}
}

@book{govindarajulu2004,
  title = {Sequential Statistics},
  author = {Govindarajulu, Z.},
  year = {2004},
  publisher = {{World Scientific}},
  address = {{New Jersey}},
  isbn = {978-981-238-905-3},
  langid = {english},
  lccn = {QA279.7 .G685 2004},
  annotation = {OCLC: ocm57365781},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\C4GVZN2E\\Govindarajulu - 2004 - Sequential statistics.pdf}
}

@article{greenland2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  urldate = {2023-04-24},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so\textemdash and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english},
  keywords = {Confidence intervals,Hypothesis testing,Null testing,P value,Power,Significance tests,Statistical testing},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\MFXEDL9S\\Greenland et al. - 2016 - Statistical tests, P values, confidence intervals,.pdf}
}

@article{griffith2021,
  title = {The Statistics of Optimal Decision Making: {{Exploring}} the Relationship between Signal Detection Theory and Sequential Analysis},
  shorttitle = {The Statistics of Optimal Decision Making},
  author = {Griffith, Thom and Baker, Sophie-Anne and Lepora, Nathan F.},
  year = {2021},
  month = aug,
  journal = {Journal of Mathematical Psychology},
  volume = {103},
  pages = {102544},
  issn = {00222496},
  doi = {10.1016/j.jmp.2021.102544},
  urldate = {2023-02-22},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TREQPWGV\\Griffith et al. - 2021 - The statistics of optimal decision making Explori.pdf}
}

@book{grissom2005,
  title = {Effect Sizes for Research: A Broad Practical Approach},
  shorttitle = {Effect Sizes for Research},
  author = {Grissom, Robert J. and Kim, John J.},
  year = {2005},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Mahwah, N.J}},
  isbn = {978-0-8058-5014-7},
  lccn = {QA279 .G75 2005},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\E4XRP7SP\\Robert J. Grissom, John J. Kim - Effect Sizes for Research_ A Broad Practical Approach -Routledge Academic (2005).pdf}
}

@article{gruning2006,
  title = {Tobacco {{Industry Influence}} on {{Science}} and {{Scientists}} in {{Germany}}},
  author = {Gr{\"u}ning, Thilo and Gilmore, Anna B. and McKee, Martin},
  year = {2006},
  month = jan,
  journal = {American Journal of Public Health},
  volume = {96},
  number = {1},
  pages = {20--32},
  issn = {0090-0036, 1541-0048},
  doi = {10.2105/AJPH.2004.061507},
  urldate = {2022-04-20},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AWL8PFNY\\Grüning et al. - 2006 - Tobacco Industry Influence on Science and Scientis.pdf}
}

@article{gsponer2014,
  title = {A Practical Guide to {{Bayesian}} Group Sequential Designs},
  author = {Gsponer, Thomas and Gerber, Florian and Bornkamp, Bj{\"o}rn and Ohlssen, David and Vandemeulebroecke, Marc and Schmidli, Heinz},
  year = {2014},
  month = jan,
  journal = {Pharmaceutical Statistics},
  volume = {13},
  number = {1},
  pages = {71--80},
  issn = {15391604},
  doi = {10.1002/pst.1593},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CB4ECF89\\Gsponer et al. - 2014 - A practical guide to Bayesian group sequential des.pdf}
}

@article{hajnal1961,
  title = {A Two-Sample Sequential t-Test},
  author = {Hajnal, J.},
  year = {1961},
  journal = {Biometrika},
  volume = {48},
  number = {1/2},
  eprint = {2333131},
  eprinttype = {jstor},
  pages = {65--75},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2333131},
  urldate = {2023-05-20},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2MC4MR5T\\Hajnal - 1961 - A Two-Sample Sequential t-Test.pdf}
}

@article{hajnal2020,
  title = {A {{Two-Sample Sequential}} t-{{Test}}},
  author = {Hajnal},
  year = {2020},
  pages = {12},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\REIGWWZI\\2020 - A Two-Sample Sequential t-Test.pdf}
}

@article{hall,
  title = {Some Sequential Analogs of {{Stein}}'s Two-Stage Test},
  author = {Hall, W J},
  pages = {12},
  abstract = {This paper presents several sequential analogs of Stein's two-stage test procedure for testing hypotheses about the mean of a normal population with unknown variance and with specified bounds on the error probabilities. These procedures are modifications of a procedure due to A. G. Baker (1950). When sequential experimentation is feasible, they provide alternatives to the sequential normal test (variance known) or the sequential \textsterling -test. If the variance is assumed known, the procedures may still be recommended since the added cost in additional observations may not be large on the average, and the performance of the tests does not depend on the validity of any assumption about the variance. Moreover, unlike the i-test, these procedures do not require that the alternative hypothesis be specified in standard deviation units.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IINN5A4W\\Hall - Some sequential analogs of Stein's two-stage test.pdf}
}

@article{hartung2002,
  title = {Small Sample Properties of Tests on Homogeneity in One\textemdash Way {{Anova}} and {{Meta}}\textemdash Analysis},
  author = {Hartung, Joachim and Arga{\c c}, Dogan and Makambi, Kepher H.},
  year = {2002},
  month = apr,
  journal = {Statistical Papers},
  volume = {43},
  number = {2},
  pages = {197--235},
  issn = {0932-5026, 1613-9798},
  doi = {10.1007/s00362-002-0097-8},
  urldate = {2022-08-09},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\K6FRE7JQ\\Hartung et al. - 2002 - Small sample properties of tests on homogeneity in.pdf}
}

@article{harwell,
  title = {Summarizing {{Monte Carlo Results}} in {{Methodological Research}}: {{The One-}} and {{Two-Factor Fixed Effects ANOVA Cases}}},
  author = {Harwell, Michael R and Rubinstein, Elaine N and Hayes, William S and Olds, Corley C},
  pages = {25},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9KL9FQ5V\\Harwell et al. - Summarizing Monte Carlo Results in Methodological .pdf}
}

@misc{heisig2018,
  title = {Why {{You Should Always Include}} a {{Random Slope}} for the {{Lower-Level Variable Involved}} in a {{Cross-Level Interaction}}},
  author = {Heisig, Jan Paul and Schaeffer, Merlin},
  year = {2018},
  month = jan,
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/bwqtd},
  urldate = {2023-07-03},
  abstract = {Mixed effects multilevel models are often used to investigate cross-level interactions, a specific type of context effect that may be understood as an upper-level variable moderating the association between a lower-level predictor and the outcome. We argue that multilevel models involving cross-level interactions should always include random slopes on the lower-level components of those interactions. Failure to do so will usually result in severely anti-conservative statistical inference. Monte Carlo simulations and illustrative empirical analyses highlight the practical relevance of the issue. Using European Social Survey data, we examine a total 30 cross-level interactions. Introducing a random slope term on the lower-level variable involved in a cross-level interaction, reduces the absolute t-ratio by 31\% or more in three quarters of cases, with an average reduction of 42\%. Many practitioners seem to be unaware of these issues. Roughly half of the cross-level interaction estimates published in the European Sociological Review between 2011 and 2016 are based on models that omit the crucial random slope term. Detailed analysis of the associated test statistics suggests that many of the estimates would not meet conventional standards of statistical significance if estimated using the correct specification. This raises the question how much robust evidence of cross-level interactions sociology has actually produced over the past decades.},
  langid = {american},
  keywords = {Comparative and Historical Sociology,Comparative Research,Context Effects,Econometrics,Economics,Ethnomethodology and Conservation Analysis,Hierarchical Data,History of Sociology,Mathematical Sociology,Methodology,Models and Methods,Multi-level and Hierarchichal Models,Political Science,Social and Behavioral Sciences,Sociology},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\URJPRZ9H\\Heisig und Schaeffer - 2018 - Why You Should Always Include a Random Slope for t.pdf}
}

@misc{hogervorst2021,
  title = {Deploy to {{Shinyapps}}.Io from {{Github Actions}} | {{R-bloggers}}},
  author = {Hogervorst, Roel M.},
  year = {2021},
  month = feb,
  urldate = {2021-10-04},
  abstract = {Last week I spend a few hours figuring out how to auto deploy a shiny app on 2 apps on shinyapps.io from github. You can see the result on this github repository. This github repository is connected to two shiny apps on shinyapps.io. Here is what I en...},
  langid = {american},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Q6BZXEBT\\deploy-to-shinyapps-io-from-github-actions.html}
}

@article{hoy2017,
  title = {Sci-{{Hub}}: {{What Librarians Should Know}} and {{Do}} about {{Article Piracy}}},
  shorttitle = {Sci-{{Hub}}},
  author = {Hoy, Matthew B.},
  year = {2017},
  month = jan,
  journal = {Medical Reference Services Quarterly},
  volume = {36},
  number = {1},
  pages = {73--78},
  issn = {0276-3869, 1540-9597},
  doi = {10.1080/02763869.2017.1259918},
  urldate = {2022-04-20},
  abstract = {The high cost of journal articles has driven many researchers to turn to a new way of getting access: ``pirate'' article sites. SciHub, the largest and best known of these sites, currently offers instant access to more than 58 million journal articles. Users attracted by the ease of use and breadth of the collection may not realize that these articles are often obtained using stolen credentials and downloading them may be illegal. This article will briefly describe Sci-Hub and how it works, the legal and ethical issues it raises, and the problems it may cause for librarians. Librarians should be aware of Sci-Hub and the ways it may change their patrons' expectations. They should also understand the risks Sci-Hub can pose to their patrons and their institutions.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZE4S8C2Z\\Hoy - 2017 - Sci-Hub What Librarians Should Know and Do about .pdf}
}

@article{hubbard2008,
  title = {Why {{{\emph{P}}}} {{Values Are Not}} a {{Useful Measure}} of {{Evidence}} in {{Statistical Significance Testing}}},
  author = {Hubbard, Raymond and Lindsay, R. Murray},
  year = {2008},
  month = feb,
  journal = {Theory \& Psychology},
  volume = {18},
  number = {1},
  pages = {69--88},
  issn = {0959-3543, 1461-7447},
  doi = {10.1177/0959354307086923},
  urldate = {2023-04-12},
  abstract = {Reporting p values from statistical significance tests is common in psychology's empirical literature. Sir Ronald Fisher saw the p value as playing a useful role in knowledge development by acting as an `objective' measure of inductive evidence against the null hypothesis. We review several reasons why the p value is an unobjective and inadequate measure of evidence when statistically testing hypotheses. A common theme throughout many of these reasons is that p values exaggerate the evidence against H0. This, in turn, calls into question the validity of much published work based on comparatively small, including .05, p values. Indeed, if researchers were fully informed about the limitations of the p value as a measure of evidence, this inferential index could not possibly enjoy its ongoing ubiquity. Replication with extension research focusing on sample statistics, effect sizes, and their confidence intervals is a better vehicle for reliable knowledge development than using p values. Fisher would also have agreed with the need for replication research.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\83QBJYN9\\Hubbard und Lindsay - 2008 - Why P Values Are Not a Useful Measure of Ev.pdf}
}

@article{hutson2018,
  title = {Artificial Intelligence Faces Reproducibility Crisis},
  author = {Hutson, Matthew},
  year = {2018},
  month = feb,
  journal = {Science},
  volume = {359},
  number = {6377},
  pages = {725--726},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.359.6377.725},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\S333FU46\\Hutson - 2018 - Artificial intelligence faces reproducibility cris.pdf}
}

@book{iglewicz1993,
  title = {How to Detect and Handle Outliers},
  author = {Iglewicz, Boris and Hoaglin, David C.},
  year = {1993},
  series = {{{ASQC}} Basic References in Quality Control},
  number = {v. 16},
  publisher = {{ASQC Quality Press}},
  address = {{Milwaukee, Wis}},
  isbn = {978-0-87389-247-6},
  langid = {english},
  lccn = {QA276.7 .I35 1993},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2K6YRZZL\\Iglewicz und Hoaglin - 1993 - How to detect and handle outliers.pdf}
}

@article{ioannidis2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2021-09-23},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\YXBSFJZW\\Ioannidis - 2005 - Why Most Published Research Findings Are False.PDF}
}

@article{ioannidis2018,
  title = {The {{Proposal}} to {{Lower}} {{{\emph{P}}}} {{Value Thresholds}} to .005},
  author = {Ioannidis, John P. A.},
  year = {2018},
  month = apr,
  journal = {JAMA},
  volume = {319},
  number = {14},
  pages = {1429},
  issn = {0098-7484},
  doi = {10.1001/jama.2018.1536},
  urldate = {2022-01-17},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\H3ZHCQC9\\Ioannidis - 2018 - The Proposal to Lower P Value Thresholds to.pdf}
}

@book{jeffreys1998,
  title = {Theory of Probability},
  author = {Jeffreys, Harold},
  year = {1998},
  series = {Oxford Classic Texts in the Physical Sciences},
  edition = {3rd ed},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford [Oxfordshire] : New York}},
  isbn = {978-0-19-850368-2},
  langid = {english},
  lccn = {QA273 .J4 1998},
  keywords = {Probabilities},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CJLGJV2S\\Jeffreys - 1998 - Theory of probability.pdf}
}

@article{john2012a,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611430953},
  urldate = {2023-02-15},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\A7BHV53H\\John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf}
}

@article{kelley2007,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  year = {2007},
  journal = {Journal of Statistical Software},
  volume = {20},
  number = {8},
  issn = {1548-7660},
  doi = {10.18637/jss.v020.i08},
  urldate = {2022-08-09},
  abstract = {The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F , and {$\chi$}2 distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F , and {$\chi$}2 distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.},
  langid = {english},
  keywords = {notion,wichtig},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\WMMVGEFR\\Kelley - 2007 - Confidence Intervals for Standardized Effect Sizes.pdf}
}

@article{kelley2007a,
  title = {Methods for the {{Behavioral}}, {{Educational}}, and {{Social Sciences}}: {{An R}} Package},
  shorttitle = {Methods for the {{Behavioral}}, {{Educational}}, and {{Social Sciences}}},
  author = {Kelley, Ken},
  year = {2007},
  month = nov,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {4},
  pages = {979--984},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BF03192993},
  urldate = {2022-08-09},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ARUIHEKZ\\Kelley - 2007 - Methods for the Behavioral, Educational, and Socia.pdf}
}

@article{keysers2020,
  title = {Using {{Bayes}} Factor Hypothesis Testing in Neuroscience to Establish Evidence of Absence},
  author = {Keysers, Christian and Gazzola, Valeria and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {23},
  number = {7},
  pages = {788--799},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-0660-4},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5RNSNEER\\Keysers et al. - 2020 - Using Bayes factor hypothesis testing in neuroscie.pdf}
}

@article{kim2017,
  title = {Understanding One-Way {{ANOVA}} Using Conceptual Figures},
  author = {Kim, Tae Kyun},
  year = {2017},
  month = jan,
  journal = {Korean Journal of Anesthesiology},
  volume = {70},
  number = {1},
  pages = {22--26},
  publisher = {{The Korean Society of Anesthesiologists}},
  doi = {10.4097/kjae.2017.70.1.22},
  urldate = {2022-07-11},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JW9RBY9S\\Kim - 2017 - Understanding one-way ANOVA using conceptual figur.pdf;C\:\\Users\\Admin\\Zotero\\storage\\U2CFFG3E\\1156679.html}
}

@article{klebel2020,
  title = {Peer Review and Preprint Policies Are Unclear at Most Major Journals},
  author = {Klebel, Thomas and Reichmann, Stefan and Polka, Jessica and McDowell, Gary and Penfold, Naomi and Hindle, Samantha and {Ross-Hellauer}, Tony},
  editor = {Useche, Sergio A.},
  year = {2020},
  month = oct,
  journal = {PLOS ONE},
  volume = {15},
  number = {10},
  pages = {e0239518},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0239518},
  urldate = {2022-04-20},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QNUDYA7C\\Klebel et al. - 2020 - Peer review and preprint policies are unclear at m.pdf}
}

@article{kleijnen2021,
  title = {Sequential Probability Ratio Tests: Conservative and Robust},
  shorttitle = {Sequential Probability Ratio Tests},
  author = {Kleijnen, Jack P C and Shi, Wen},
  year = {2021},
  month = jan,
  journal = {SIMULATION},
  volume = {97},
  number = {1},
  pages = {33--43},
  issn = {0037-5497, 1741-3133},
  doi = {10.1177/0037549720954916},
  urldate = {2021-09-23},
  abstract = {Because computers (except for parallel computers) generate simulation outputs sequentially, we recommend sequential probability ratio tests (SPRTs) for the statistical analysis of these outputs. However, until now simulation analysts have ignored SPRTs. To change this situation, we review SPRTs for the simplest case; namely, the case of choosing between two hypothesized values for the mean simulation output. For this case, the classic SPRT of Wald (Wald A. Sequential tests of statistical hypotheses. Ann Math Stat 1945; 16: 117\textendash 186) allows general types of distribution, including normal distributions with known variances. A modification permits unknown variances that are estimated. Hall (Hall WJ. Some sequential analogs of Stein's two-stage test. Biometrika 1962; 49: 367\textendash 378) developed a SPRT that assumes normal distributions with unknown variances estimated from a pilot sample. A modification uses a fully sequential variance estimator. In this paper, we quantify the performance of the various SPRTs, using several Monte Carlo experiments. In experiment \#1, simulation outputs are normal. Whereas Wald's SPRT with estimated variance gives too high error rates, Hall's original and modified SPRTs are ``conservative''; that is, the actual error rates are smaller than those prespecified (nominal). Furthermore, our experiment shows that the most efficient SPRT is Hall's modified SPRT. In experiment \#2, we estimate the robustness of these SPRTs for non-normal output. For these two experiments, we provide details on their design and analysis; these details may also be useful for simulation experiments in general.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IK3WF5NR\\Kleijnen und Shi - 2021 - Sequential probability ratio tests conservative a.pdf}
}

@book{knight2022,
  title = {Applied {{Mathematics}} with {{Open-Source Software}}: {{Operational Research Problems}} with {{Python}} and {{R}}},
  shorttitle = {Applied {{Mathematics}} with {{Open-Source Software}}},
  author = {Knight, Vincent and Palmer, Geraint},
  year = {2022},
  month = mar,
  edition = {First},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  doi = {10.1201/9780429328534},
  urldate = {2022-05-19},
  isbn = {978-0-429-32853-4},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\F42DAEXK\\Knight und Palmer - 2022 - Applied Mathematics with Open-Source Software Ope.pdf}
}

@misc{krass,
  title = {Bayes Factors},
  author = {Krass, R. E.},
  urldate = {2021-11-19},
  howpublished = {https://martinschnuerch.com/wp-content/uploads/2021/11/kass\_1995\_journal\_of\_the\_american\_statistical\_association.pdf},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UQZM6ER3\\kass_1995_journal_of_the_american_statistical_association.pdf}
}

@article{kuhlmann2012,
  title = {Mediator-Based Encoding Strategies in Source Monitoring in Young and Older Adults.},
  author = {Kuhlmann, Beatrice G. and Touron, Dayna R.},
  year = {2012},
  month = sep,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {38},
  number = {5},
  pages = {1352--1364},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/a0027863},
  urldate = {2023-03-27},
  abstract = {Past research has examined the contribution of mediator-based encoding strategies (interactive imagery and sentence generation) to individual (particularly age-related) differences in associative memory exclusively in the paired-associates paradigm. In the present study, we examined young and older adults' mediator-based strategy use on source-monitoring tasks. Participants spontaneously used mediator-based strategies to encode about 30\% to 40\% of word\textendash source pairs and were able to follow instructions to use the specific mediatorbased strategy of interactive imagery; mediator-based strategy use was associated with higher source memory and explained variance in source memory. There were no age-related differences in the patterns of mediatorbased strategy production and utilization. Age-related differences in source memory were explained by age-related declines in the ability to bind information in memory (incidental memory for digit\textendash symbol associations) but not by encoding strategy production. Results underscore the importance of assessing encoding strategy use for understanding individual differences in source memory.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KSJ4HI3G\\Kuhlmann und Touron - 2012 - Mediator-based encoding strategies in source monit.pdf}
}

@article{kuhlmann2016,
  title = {Aging and Memory Improvement through Semantic Clustering: {{The}} Role of List-Presentation Format.},
  shorttitle = {Aging and Memory Improvement through Semantic Clustering},
  author = {Kuhlmann, Beatrice G. and Touron, Dayna R.},
  year = {2016},
  month = nov,
  journal = {Psychology and Aging},
  volume = {31},
  number = {7},
  pages = {771--785},
  issn = {1939-1498, 0882-7974},
  doi = {10.1037/pag0000117},
  urldate = {2023-03-27},
  abstract = {The present study examined how the presentation format of the study list influences younger and older adults' semantic clustering. Spontaneous clustering did not differ between age groups or between an individual-words (presentation of individual study words in consecution) and a whole-list (presentation of the whole study list at once for the same total duration) presentation format in 132 younger (18 \textendash 30 years, M ϭ 19.7) and 120 older (60 \textendash{} 84 years, M ϭ 69.5) adults. However, after instructions to use semantic clustering (second list) age-related differences in recall magnified, indicating a utilization deficiency, and both age groups achieved higher recall in the whole-list than in the individual-words format. While this whole-list benefit was comparable across age groups, it is notable that older adults were only able to improve their average recall performance after clustering instructions in the whole-list but not in the individual-words format. In both formats, instructed clustering was correlated with processing resources (processing speed and, especially, working memory capacity), particularly in older adults. Spontaneous clustering, however, was not related to processing resources but to metacognitive beliefs about the efficacy and difficulty of semantic clustering, neither of which indicated awareness of the benefits of the whole-list presentation format in either age group. Taken together, the findings demonstrate that presentation format has a nontrivial influence on the utilization of semantic clustering in adults. The analyses further highlight important differences between output-based and list-based clustering measures.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IC2VHZ7L\\Kuhlmann und Touron - 2016 - Aging and memory improvement through semantic clus.pdf}
}

@article{kunert2016,
  title = {Internal Conceptual Replications Do Not Increase Independent Replication Success},
  author = {Kunert, Richard},
  year = {2016},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {5},
  pages = {1631--1638},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1030-9},
  urldate = {2022-04-20},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\NS682JJJ\\Kunert - 2016 - Internal conceptual replications do not increase i.pdf}
}

@article{lakens2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: {{A}} Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Dani{\"e}l},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  pages = {863},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  urldate = {2022-08-01},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between withinand between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UNR3JYIQ\\Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf}
}

@article{lakens2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  month = dec,
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  urldate = {2023-03-21},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9ZTAXD2W\\Lakens - 2014 - Performing high-powered studies efficiently with s.pdf}
}

@article{lakens2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  urldate = {2023-03-09},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\4ZUIFCEK\\Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf}
}

@misc{lakens2021,
  title = {Why P-Values Are Not Measures of Evidence},
  author = {Lakens, Daniel},
  year = {2021},
  month = dec,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/7ng4w},
  urldate = {2022-01-10},
  abstract = {The recommendations by Muff and colleagues are an incoherent approach to statistical inferences, and should only be used if one wants to signal a misunderstanding of p-values. Coherent alternatives to quantify evidence exist, such as likelihoods and Bayes factors. Therefore, researchers should not follow the recommendation by Muff and colleagues to report p = 0.08 as `weak evidence', p = 0.03 as `moderate evidence', and p = 0.168 as `no evidence'.},
  langid = {american},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\8ABU23RI\\Lakens - 2021 - Why p-values are not measures of evidence.pdf}
}

@article{lakens2021a,
  title = {Invited Commentary: {{Comparing}} the Independent Segments Procedure with Group Sequential Designs.},
  shorttitle = {Invited Commentary},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {498--500},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000400},
  urldate = {2023-03-09},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KZXSFMJ2\\Lakens - 2021 - Invited commentary Comparing the independent segm.pdf}
}

@techreport{lakens2021b,
  type = {Preprint},
  title = {Group Sequential Designs: {{A}} Tutorial},
  shorttitle = {Group Sequential Designs},
  author = {Lakens, Daniel and Pahlke, Friedrich and Wassmer, Gernot},
  year = {2021},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/x4azm},
  urldate = {2023-03-09},
  abstract = {This tutorial illustrates how to design, analyze, and report group sequential designs. In these designs, groups of observations are collected and repeatedly analyzed, while controlling error rates. Compared to a fixed sample size design, where data is analyzed only once, group sequential designs offer the possibility to stop the study at interim looks at the data either for efficacy or futility. Hence, they provide greater flexibility and are more efficient in the sense that due to early stopping the expected sample size is smaller as compared to the sample size in the design with no interim look. In this tutorial we illustrate how to use the R package 'rpact' and the associated Shiny app to design studies that control the Type I error rate when repeatedly analyzing data, even when neither the number of looks at the data, nor the exact timing of looks at the data, is specified. Specifically for *t*-tests, we illustrate how to perform an a-priori power analysis for group sequential designs, and explain how to stop the data collection for futility by rejecting the presence of an effect of interest based on a beta-spending function. Finally, we discuss how to report adjusted effect size estimates and confidence intervals. The recent availability of accessible software such as 'rpact' makes it possible for psychologists to benefit from the efficiency gains provided by group sequential designs.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\L7C5SQWA\\Lakens et al. - 2021 - Group Sequential Designs A Tutorial.pdf}
}

@article{lakens2022,
  title = {Sample Size Justification},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2022-07-04},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  langid = {english},
  keywords = {1 paper}
}

@article{lan1983,
  title = {Discrete Sequential Boundaries for Clinical Trials},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  year = {1983},
  journal = {Biometrika},
  volume = {70},
  number = {3},
  eprint = {2336502},
  eprinttype = {jstor},
  pages = {659--663},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2336502},
  urldate = {2023-04-27},
  abstract = {Pocock (1977), O'Brien \& Fleming (1979) and Slud \& Wei (1982) have proposed different methods to construct discrete sequential boundaries for clinical trials. These methods require that the total number of decision times be specified in advance. In the present paper, we propose a more flexible way to construct discrete sequential boundaries. The method is based on the choice of a function, {$\alpha$}*(t), which characterizes the rate at which the error level {$\alpha$} is spent. The boundary at a decision time is determined by {$\alpha$}*(t), and by past and current decision times, but does not depend on the future decision times or the total number of decision times.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\VD28IACT\\Lan und DeMets - 1983 - Discrete Sequential Boundaries for Clinical Trials.pdf}
}

@article{lang2017,
  title = {Is Intermediately Inspecting Statistical Data Necessarily a Bad Research Practice?},
  author = {Lang, Albert-Georg},
  year = {2017},
  month = may,
  journal = {The Quantitative Methods for Psychology},
  volume = {13},
  number = {2},
  pages = {127--140},
  issn = {2292-1354},
  doi = {10.20982/tqmp.13.2.p127},
  urldate = {2023-04-13},
  abstract = {Intermediately inspecting the statistical data of a running experiment is justifiably referred to as a bad research practice. With only a few intermediate inspections, Type I error rates inflate to a multiple of the previously defined critical alpha. On the other hand, there are research areas where intermediately inspecting data is extremely desirable if not even necessary. For this reason, in medical research, mathematical methods are known as ``group-sequential testing'' which compensate Type I error cumulation by adjusting critical alpha. In the field of psychological research, these methods are widely unknown or at least used very rarely. One reason may be that group-sequential tests focus on test statistics based on the normal distribution, mainly the t-test, while in psychological research often more complex experimental designs are used. The computer program APriot has been developed to enable the user to conduct Monte-Carlo simulations of what happens when intermediately inspecting the data of an ANOVA. The simulations show clearly how bad a research practice intermediately inspecting data (without adjusting alpha) is. Further, it is shown that in many cases adjusted values of alpha can be found by simulations such that the ANOVA can be used together with group-sequential testing similarly as the t-test. A last set of demonstrations shows how the power and the required number of participants of a groupsequential test can be estimated and that group-sequential testing can be favorable from an economic point of view.},
  langid = {english},
  keywords = {1 paper,GS},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QEGM829U\\Lang - 2017 - Is intermediately inspecting statistical data nece.pdf}
}

@article{langsrud,
  title = {{{ANOVA}} for Unbalanced Data: {{Use Type II}} Instead of {{Type III}} Sums of Squares},
  author = {Langsrud, {\O}yvind},
  pages = {5},
  abstract = {Methods for analyzing unbalanced factorial designs can be traced back to Yates (1934). Today, most major statistical programs perform, by default, unbalanced ANOVA based on Type III sums of squares (Yates's weighted squares of means). As criticized by Nelder and Lane (1995), this analysis is founded on unrealistic models\textemdash models with interactions, but without all corresponding main effects. The Type II analysis (Yates's method of fitting constants) is usually not preferred because of the underlying assumption of no interactions. This argument is, however, also founded on unrealistic models. Furthermore, by considering the power of the two methods, it is clear that Type II is preferable.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\8TNJBCDV\\Langsrud - ANOVA for unbalanced data Use Type II instead of .pdf}
}

@article{lantz2013,
  title = {The Impact of Sample Non-Normality on {{ANOVA}} and Alternative Methods},
  author = {Lantz, Bj{\"o}rn},
  year = {2013},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {66},
  number = {2},
  pages = {224--244},
  issn = {2044-8317},
  doi = {10.1111/j.2044-8317.2012.02047.x},
  urldate = {2021-11-16},
  abstract = {In this journal, Zimmerman (2004, 2011) has discussed preliminary tests that researchers often use to choose an appropriate method for comparing locations when the assumption of normality is doubtful. The conceptual problem with this approach is that such a two-stage process makes both the power and the significance of the entire procedure uncertain, as type I and type II errors are possible at both stages. A type I error at the first stage, for example, will obviously increase the probability of a type II error at the second stage. Based on the idea of Schmider et al. (2010), which proposes that simulated sets of sample data be ranked with respect to their degree of normality, this paper investigates the relationship between population non-normality and sample non-normality with respect to the performance of the ANOVA, Brown\textendash Forsythe test, Welch test, and Kruskal\textendash Wallis test when used with different distributions, sample sizes, and effect sizes. The overall conclusion is that the Kruskal\textendash Wallis test is considerably less sensitive to the degree of sample normality when populations are distinctly non-normal and should therefore be the primary tool used to compare locations when it is known that populations are not at least approximately normal.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PGMKIA4H\\Lantz - 2013 - The impact of sample non-normality on ANOVA and al.pdf;C\:\\Users\\Admin\\Zotero\\storage\\BQUDAEY6\\j.2044-8317.2012.02047.html}
}

@article{lariviere2015,
  title = {The {{Oligopoly}} of {{Academic Publishers}} in the {{Digital Era}}},
  author = {Larivi{\`e}re, Vincent and Haustein, Stefanie and Mongeon, Philippe},
  editor = {Glanzel, Wolfgang},
  year = {2015},
  month = jun,
  journal = {PLOS ONE},
  volume = {10},
  number = {6},
  pages = {e0127502},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0127502},
  urldate = {2022-04-20},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\P4QVUYTU\\Larivière et al. - 2015 - The Oligopoly of Academic Publishers in the Digita.pdf}
}

@article{lee1980,
  title = {A Monte Carlo Study on the Robustness of the Two-Sample Sequential {\emph{t}} Test},
  author = {Lee, Hyunshik and Fung, Karen Yuen},
  year = {1980},
  month = apr,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {10},
  number = {3-4},
  pages = {297--307},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949658008810377},
  urldate = {2022-05-04},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\L2M7T7LC\\Lee und Fung - 1980 - A monte carlo study on the robustness of the two-s.pdf}
}

@article{leek2017,
  title = {Is {{Most Published Research Really False}}?},
  author = {Leek, Jeffrey T. and Jager, Leah R.},
  year = {2017},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {4},
  number = {1},
  pages = {109--122},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-060116-054104},
  urldate = {2021-09-23},
  abstract = {There has been an increasing concern in both the scientific and lay communities that most published medical findings are false. But what does it mean to be false? Here we describe the range of definitions of false discoveries in the scientific literature. We summarize the philosophical, statistical, and experimental evidence for each type of false discovery. We discuss common underpinning problems with the scientific and data analytic practices and point to tools and behaviors that can be implemented to reduce the problems with published scientific results.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\6W9LKFX2\\Leek und Jager - 2017 - Is Most Published Research Really False.pdf}
}

@incollection{lehmann2012,
  title = {The {{Fisher}}, {{Neyman-Peerson Theories}} of {{Testing Hypotheses}}: {{One Theory}} or {{Two}}?},
  shorttitle = {The {{Fisher}}, {{Neyman-Peerson Theories}} of {{Testing Hypotheses}}},
  booktitle = {Selected {{Works}} of {{E}}. {{L}}. {{Lehmann}}},
  author = {Lehmann, E. L.},
  editor = {Rojo, Javier},
  year = {2012},
  pages = {201--208},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4614-1412-4_19},
  urldate = {2021-09-23},
  isbn = {978-1-4614-1411-7 978-1-4614-1412-4},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\V3RDYZQD\\Lehmann - 2012 - The Fisher, Neyman-Peerson Theories of Testing Hyp.pdf}
}

@incollection{lehmann2012a,
  title = {The {{Neyman-Pearson Theory}} after {{Fifty Years}}},
  booktitle = {Selected {{Works}} of {{E}}. {{L}}. {{Lehmann}}},
  author = {Lehmann, E. L.},
  editor = {Rojo, Javier},
  year = {2012},
  pages = {1047--1060},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4614-1412-4_88},
  urldate = {2021-09-23},
  abstract = {To commemorate the 50th anniversary of the Neyman-Pearson paradigm, this paper sketches some aspects of its development during the past half-century. In particular, the relevance of this approach to data analysis and Bayesian statistics is discussed.},
  isbn = {978-1-4614-1411-7 978-1-4614-1412-4},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\X6BTS3X2\\Lehmann - 2012 - The Neyman-Pearson Theory after Fifty Years.pdf}
}

@article{lewandowsky2020,
  title = {Low Replicability Can Support Robust and Efficient Science},
  author = {Lewandowsky, Stephan and Oberauer, Klaus},
  year = {2020},
  month = dec,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {358},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-14203-0},
  urldate = {2022-04-20},
  abstract = {Abstract             There is a broad agreement that psychology is facing a replication crisis. Even some seemingly well-established findings have failed to replicate. Numerous causes of the crisis have been identified, such as underpowered studies, publication bias, imprecise theories, and inadequate statistical procedures. The replication crisis is real, but it is less clear how it should be resolved. Here we examine potential solutions by modeling a scientific community under various different replication regimes. In one regime, all findings are replicated before publication to guard against subsequent replication failures. In an alternative regime, individual studies are published and are replicated after publication, but only if they attract the community's interest. We find that the publication of potentially non-replicable studies minimizes cost and maximizes efficiency of knowledge gain for the scientific community under a variety of assumptions. Provided it is properly managed, our findings suggest that low replicability can support robust and efficient science.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Y9YMZ7D6\\Lewandowsky und Oberauer - 2020 - Low replicability can support robust and efficient.pdf}
}

@article{liao2016,
  title = {Outlier {{Impact}} and {{Accommodation Methods}}: {{Multiple Comparisons}} of {{Type I Error Rates}}},
  shorttitle = {Outlier {{Impact}} and {{Accommodation Methods}}},
  author = {Liao, Hongjing and Li, Yanju and Brooks, Gordon},
  year = {2016},
  month = may,
  journal = {Journal of Modern Applied Statistical Methods},
  volume = {15},
  number = {1},
  pages = {452--471},
  issn = {1538-9472},
  doi = {10.22237/jmasm/1462076520},
  urldate = {2022-09-16},
  abstract = {A Monte Carlo simulation study was conducted to examine outliers' influence on Type I error rates in ANOVA and Welch tests, and the effectiveness of two outlier accommodation methods: nonparametric rank based method and Winsorizing. Recommendations are given regarding outlier handling with different sample sizes and number of outliers.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\VXXGFPG6\\Liao et al. - 2016 - Outlier Impact and Accommodation Methods Multiple.pdf}
}

@article{liao2017,
  title = {Outlier Impact and Accommodation on Power},
  author = {Liao, Hongjing and Li, Yanju and Brooks, Gordon P.},
  year = {2017},
  month = may,
  journal = {Journal of Modern Applied Statistical Methods},
  volume = {16},
  number = {1},
  pages = {261--278},
  issn = {1538-9472},
  doi = {10.22237/jmasm/1493597640},
  urldate = {2022-11-09},
  abstract = {The outliers' influence on power rates in ANOVA and Welch tests at various conditions was examined and compared with the effectiveness of nonparametric methods and Winsorizing in minimizing the impact of outliers. Results showed that, considering both power and Type I error, a nonparametric test is the safest choice to control the inflation of Type I error with a decent sample size and yield relatively high power.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\4DVKGLL4\\Liao et al. - 2017 - Outlier impact and accommodation on power.pdf}
}

@article{lindley1993,
  title = {The {{Analysis}} of {{Experimental Data}}: {{The Appreciation}} of {{Tea}} and {{Wine}}},
  shorttitle = {The {{Analysis}} of {{Experimental Data}}},
  author = {Lindley, Dennis V},
  year = {1993},
  month = mar,
  journal = {Teaching Statistics},
  volume = {15},
  number = {1},
  pages = {22--25},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/j.1467-9639.1993.tb00252.x},
  urldate = {2021-09-23},
  abstract = {A classical experiment on the tasting of tea is used to show that many standard methods of analysis of the resulting data are unsatisfactory. A similar experiment with wine is used to show how a more sensible method may be developed.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\GPDFR6A6\\Lindley - 1993 - The Analysis of Experimental Data The Appreciatio.pdf}
}

@article{liu2000,
  title = {Maximum Likelihood Estimate Following Sequential Probability Ratio Tests: {{Sequential}} Probability Ratio Tests},
  shorttitle = {Maximum Likelihood Estimate Following Sequential Probability Ratio Tests},
  author = {Liu, Aiyi},
  year = {2000},
  month = jan,
  journal = {Sequential Analysis},
  volume = {19},
  number = {1-2},
  pages = {63--75},
  issn = {0747-4946, 1532-4176},
  doi = {10.1080/07474940008836440},
  urldate = {2021-09-23},
  abstract = {We consider 1Yald.s (1947) sequential probability ratio tests for a Browman motion X ( t ) with drift 8. Expressions are derived for t h e distribution and moment generating functions, first a n d second moments of t h e maximum likelihood estimate (1ILE). Asymptotic behavior of the \textbackslash ILE is also discussed.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\BW9L3ZIQ\\Liu - 2000 - Maximum likelihood estimate following sequential p.pdf}
}

@incollection{liu2003,
  title = {A Simple Low-Bias Estimate Following a Sequential Test with Linear Boundaries},
  booktitle = {Institute of {{Mathematical Statistics Lecture Notes}} - {{Monograph Series}}},
  author = {Liu, Aiyi},
  year = {2003},
  pages = {47--58},
  publisher = {{Institute of Mathematical Statistics}},
  address = {{Beachwood, OH}},
  doi = {10.1214/lnms/1215092389},
  urldate = {2022-07-11},
  isbn = {978-0-940600-58-4},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SYPZJMYA\\Liu - 2003 - A simple low-bias estimate following a sequential .pdf}
}

@article{livingston2004,
  title = {Who Was Student and Why Do We Care so Much about His T-Test?1},
  shorttitle = {Who Was Student and Why Do We Care so Much about His T-Test?},
  author = {Livingston, Edward H},
  year = {2004},
  month = may,
  journal = {Journal of Surgical Research},
  volume = {118},
  number = {1},
  pages = {58--65},
  issn = {00224804},
  doi = {10.1016/j.jss.2004.02.003},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\B7WPJC8I\\Livingston - 2004 - Who was student and why do we care so much about h.pdf}
}

@article{lix1996,
  title = {Consequences of Assumption Violations Revisited: {{A}} Quantitative Review of Alternatives to the One-Way Analysis of Variance {{F}} Test},
  author = {Lix, Lisa M and Keselman, Joanne C and Keselman, H J},
  year = {1996},
  journal = {Review of Educational Research},
  volume = {66},
  number = {4},
  pages = {579--619},
  doi = {10.2307/1170654},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KKVX5FW2\\Lix et al. - Consequences of Assumption Violations Revisited A.pdf}
}

@article{loken2017,
  title = {Measurement Error and the Replication Crisis},
  author = {Loken, Eric and Gelman, Andrew},
  year = {2017},
  month = feb,
  journal = {Science},
  volume = {355},
  number = {6325},
  pages = {584--585},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aal3618},
  urldate = {2021-09-27},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\D7I44QEZ\\Loken und Gelman - 2017 - Measurement error and the replication crisis.pdf}
}

@article{lovakov2021,
  title = {Empirically Derived Guidelines for Effect Size Interpretation in Social Psychology},
  author = {Lovakov, Andrey and Agadullina, Elena R.},
  year = {2021},
  journal = {European Journal of Social Psychology},
  volume = {51},
  number = {3},
  pages = {485--504},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2752},
  urldate = {2023-04-13},
  abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UCPK82MY\\Lovakov und Agadullina - 2021 - Empirically derived guidelines for effect size int.pdf;C\:\\Users\\Admin\\Zotero\\storage\\IXCT7XRP\\ejsp.html}
}

@misc{ly2017,
  title = {A {{Tutorial}} on {{Fisher Information}}},
  author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
  year = {2017},
  month = oct,
  number = {arXiv:1705.01064},
  eprint = {1705.01064},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; lastly, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{62-01, 62B10 (Primary), 62F03, 62F12, 62F15, 62B10 (Secondary)},Mathematics - Statistics Theory},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JSA4JUPH\\Ly et al. - 2017 - A Tutorial on Fisher Information.pdf}
}

@article{marsman2017,
  title = {Bayesian Benefits with {{JASP}}},
  author = {Marsman, Maarten and Wagenmakers, Eric-Jan},
  year = {2017},
  month = sep,
  journal = {European Journal of Developmental Psychology},
  volume = {14},
  number = {5},
  pages = {545--555},
  issn = {1740-5629, 1740-5610},
  doi = {10.1080/17405629.2016.1259614},
  urldate = {2021-09-23},
  abstract = {We illustrate the Bayesian approach to data analysis using the newly developed statistical software program JASP. With JASP, researchers are able to take advantage of the benefits that the Bayesian framework has to offer in terms of parameter estimation and hypothesis testing. The Bayesian advantages are discussed using real data on the relation between Quality of Life and Executive Functioning in children with Autism Spectrum Disorder.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZKBJMMN7\\Marsman und Wagenmakers - 2017 - Bayesian benefits with JASP.pdf}
}

@article{masicampo2012,
  title = {A Peculiar Prevalence of {\emph{p}} Values Just below .05},
  author = {Masicampo, E.J. and Lalande, Daniel R.},
  year = {2012},
  month = nov,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {65},
  number = {11},
  pages = {2271--2279},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470218.2012.711335},
  urldate = {2023-04-12},
  abstract = {In null hypothesis significance testing (NHST), p values are judged relative to an arbitrary threshold for significance (.05). The present work examined whether that standard influences the distribution of p values reported in the psychology literature. We examined a large subset of papers from three highly regarded journals. Distributions of p were found to be similar across the different journals. Moreover, p values were much more common immediately below .05 than would be expected based on the number of p values occurring in other ranges. This prevalence of p values just below the arbitrary criterion for significance was observed in all three journals. We discuss potential sources of this pattern, including publication bias and researcher degrees of freedom.},
  langid = {english}
}

@article{maxwell2015,
  title = {Is Psychology Suffering from a Replication Crisis? {{What}} Does ``Failure to Replicate'' Really Mean?},
  shorttitle = {Is Psychology Suffering from a Replication Crisis?},
  author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
  year = {2015},
  journal = {American Psychologist},
  volume = {70},
  number = {6},
  pages = {487--498},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/a0039400},
  urldate = {2021-09-23},
  abstract = {Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and metaanalysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology's alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5EVFMW8Z\\Maxwell et al. - 2015 - Is psychology suffering from a replication crisis.pdf}
}

@book{maxwell2017,
  title = {Designing Experiments and Analyzing Data: {{A}} Model Comparison Perspective},
  shorttitle = {Designing Experiments and Analyzing Data},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  year = {2017},
  edition = {Third edition},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  isbn = {978-1-138-89228-6},
  langid = {english},
  lccn = {QA279 .M384 2017},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\WXUD2EUZ\\Maxwell et al. - 2017 - Designing experiments and analyzing data a model .pdf}
}

@book{mayo2018,
  title = {Statistical {{Inference}} as {{Severe Testing}}: {{How}} to {{Get Beyond}} the {{Statistics Wars}}},
  shorttitle = {Statistical {{Inference}} as {{Severe Testing}}},
  author = {Mayo, Deborah G.},
  year = {2018},
  month = sep,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781107286184},
  urldate = {2021-09-23},
  isbn = {978-1-107-28618-4 978-1-107-05413-4 978-1-107-66464-7},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\AHLY4SDM\\Mayo - 2018 - Statistical Inference as Severe Testing How to Ge.pdf}
}

@article{meeks2022,
  title = {The {{Gerontologist Adopts New Transparency}} and {{Openness Guidelines}}},
  author = {Meeks, Suzanne and Bookwala, Jamila and Bowers, Barbara J and Degenholtz, Howard B and {de Medeiros}, Kate and Heyn, Patricia C and Kriebernegg, Ulla},
  year = {2022},
  month = jan,
  journal = {The Gerontologist},
  pages = {gnab154},
  issn = {0016-9013},
  doi = {10.1093/geront/gnab154},
  urldate = {2022-01-13},
  abstract = {Open science is a movement to increase transparency and replicability in research, responding in part to frequent failure of research groups other than those of the original authors to replicate published research findings. Although the open science movement emerged in disciplines including medicine, psychology, and economics, Isaacowitz and Lind (2019) have argued that such transparency initiatives are also needed and appropriate for gerontology research outside of these disciplines. Because we value the integrity and translational relevance of the research we publish, the editorial team at The Gerontologist, in consultation with our Editorial Board, has decided to adopt additional author guidelines to promote transparency and openness. Going forward, the journal will adhere to Level 1 of the Transparency and Openness Promotion (TOP) guidelines (https://www.cos.io/initiatives/top-guidelines). The purpose of these guidelines is to increase reproducibility of research.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\WP4YXEP7\\Meeks et al. - 2022 - The Gerontologist Adopts New Transparency a.pdf}
}

@article{mendes2009,
  title = {{{ANOVA F}} ve {{Welch Testi}} Ile {{Bunlar\i n Permutasyon Versiyonlar\i n\i n}} 1. {{Tip Hata}} ve {{Testin G\"uc\"u Bak\i m\i ndan Kar\c{s}\i la\c{s}t\i r\i lmas\i}}},
  author = {Mende{\c s}, Mehmet and Akkartal, Erkut},
  year = {2009},
  journal = {Kafkas Universitesi Veteriner Fakultesi Dergisi},
  issn = {1300-6045},
  doi = {10.9775/kvfd.2009.1507},
  urldate = {2022-08-09},
  abstract = {We compared Analysis of Variance (F) and the Welch test (W) with their respective permutation versions (PF and PW) in terms of Type I error rate ({$\alpha$}) and test power (1-{$\beta$}) by Monte Carlo simulation technique. Simulation results showed that when the variances were homogeneous, the permutation versions of F and W tests displayed more reliable results in terms of protecting Type I error rate at nominal level, regardless of distribution shape and sample size. Violation of homogeneity of variances adversely affected all tests. Regardless of sample size and effect size, the PF test was slightly more powerful compared to the F test as long as the variances were homogeneous and the distributions were skewed ({$\chi$}2 (3) and Exp [0.75]). The PF and F tests had similar power levels when the distributions were symmetrical (Beta (5.5)). The W test was more powerful with homogenous variances, while the PW test was slightly superior with heterogonous variances except for unbalanced sample sizes (i.e., 5:10:15).},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Q24J7TUY\\Mendeş und Akkartal - 2009 - ANOVA F ve Welch Testi ile Bunların Permutasyon Ve.pdf}
}

@article{miller2021,
  title = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing: {{The}} Independent Segments Procedure},
  shorttitle = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {486--497},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/met0000350},
  urldate = {2023-03-22},
  abstract = {We propose a new sequential hypothesis testing procedure in which data are collected and analyzed in a series of independent segments. As in fixed-sample hypothesis testing and in previous sequential procedures, the overall {$\alpha$} level can be set to any desired value. Like other sequential procedures, the independent segments procedure generally requires smaller samples than fixed-sample procedures\textemdash often approximately 30\% smaller\textemdash to achieve the same {$\alpha$} level and statistical power. Relative to other sequential procedures, the new method has the advantages that it is simpler to use, requires fewer assumptions, and can be used with a wider array of statistical tests. Thus, in some circumstances the independent segments procedure may provide an attractive option for increasing the efficiency of statistical testing. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XL8RGF75\\Miller und Ulrich - 2021 - A simple, general, and efficient method for sequen.pdf}
}

@article{miller2022,
  title = {Optimizing {{Research Output}}: {{How Can Psychological Research Methods Be Improved}}?},
  shorttitle = {Optimizing {{Research Output}}},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2022},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {691--718},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-020821-094927},
  urldate = {2023-03-09},
  abstract = {Recent evidence suggests that research practices in psychology and many other disciplines are far less effective than previously assumed, which has led to what has been called a ``crisis of confidence'' in psychological research (e.g., Pashler \& Wagenmakers 2012). In response to the perceived crisis, standard research practices have come under intense scrutiny, and various changes have been suggested to improve them. The burgeoning field of metascience seeks to use standard quantitative data-gathering and modeling techniques to understand the reasons for inefficiency, to assess the likely effects of suggested changes, and ultimately to tell psychologists how to do better science. We review the pros and cons of suggested changes, highlighting the many complex research trade-offs that must be addressed to identify better methods.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EXL9VK5R\\Miller und Ulrich - 2022 - Optimizing Research Output How Can Psychological .pdf}
}

@article{moder,
  title = {Alternatives to {{F-Test}} in {{One Way ANOVA}} in Case of Heterogeneity of Variances (a Simulation Study)},
  author = {Moder, Karl},
  pages = {12},
  abstract = {Several articles deal with the effects of inhomogeneous variances in one way analysis of variance (ANOVA). A very early investigation of this topic was done by Box (1954). He supposed, that in balanced designs with moderate heterogeneity of variances deviations of the empirical type I error rate (on experiments based realized {$\alpha$}) to the nominal one (predefined {$\alpha$} for H0) are small. Similar conclusions are drawn by Wellek (2003). For not so moderate heterogeneity (e.g. {$\sigma$}1 : {$\sigma$}2 :\ldots{} = 3 :1:\ldots{} ) Moder (2007) showed, that empirical type I error rate is far beyond the nominal one, even with balanced designs. In unbalanced designs the difficulties get bigger. Several attempts were made to get over this problem. One proposal is to use a more stringent {$\alpha$} level (e.g. 2.5\% instead of 5\%) (Keppel \& Wickens, 2004). Another recommended remedy is to transform the original scores by square root, log, and other variance reducing functions (Keppel \& Wickens, 2004, Heiberger \& Holland, 2004). Some authors suggest the use of rank based alternatives to Ftest in analysis of variance (Vargha \& Delaney, 1998). Only a few articles deal with two or multifactorial designs. There is some evidence, that in a two or multi-factorial design type I error rate is approximately met if the number of factor levels tends to infinity for a certain factor while the number of levels is fixed for the other factors (Akritas \& S., 2000, Bathke, 2004).},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PG7F3HBS\\Moder - Alternatives to F-Test in One Way ANOVA in case of.pdf}
}

@article{mordkoff2019,
  title = {A {{Simple Method}} for {{Removing Bias From}} a {{Popular Measure}} of {{Standardized Effect Size}}: {{Adjusted Partial Eta Squared}}},
  shorttitle = {A {{Simple Method}} for {{Removing Bias From}} a {{Popular Measure}} of {{Standardized Effect Size}}},
  author = {Mordkoff, J. Toby},
  year = {2019},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {3},
  pages = {228--232},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245919855053},
  urldate = {2022-06-07},
  abstract = {Accurate estimates of population effect size are critical to empirical science, for both reporting experimental results and conducting a priori power analyses. Unfortunately, the current most-popular measure of standardized effect size, partial eta squared ([Formula: see text]), is known to have positive bias. Two less-biased alternatives, partial epsilon squared ([Formula: see text]) and partial omega squared ([Formula: see text]), have both existed for decades, but neither is often employed. Given that researchers appear reluctant to abandon [Formula: see text], this article provides a simple method for removing bias from this measure, to produce a value referred to as adjusted partial eta squared (adj [Formula: see text]). Some of the many benefits of adopting this measure are briefly discussed.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\R285KZF5\\Mordkoff - 2019 - A Simple Method for Removing Bias From a Popular M.pdf}
}

@article{morey2011,
  title = {Bayes Factor Approaches for Testing Interval Null Hypotheses.},
  author = {Morey, Richard D. and Rouder, Jeffrey N.},
  year = {2011},
  month = dec,
  journal = {Psychological Methods},
  volume = {16},
  number = {4},
  pages = {406--419},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0024377},
  urldate = {2023-04-17},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\E7PNQRDK\\Morey und Rouder - 2011 - Bayes factor approaches for testing interval null .pdf}
}

@article{muff2021,
  title = {Rewriting Results Sections in the Language of Evidence},
  author = {Muff, Stefanie and Nilsen, Erlend B. and O'Hara, Robert B. and Nater, Chlo{\'e} R.},
  year = {2021},
  month = nov,
  journal = {Trends in Ecology \& Evolution},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0169-5347},
  doi = {10.1016/j.tree.2021.10.009},
  urldate = {2022-01-10},
  langid = {english},
  pmid = {34799145},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5535US6W\\Muff et al. - 2021 - Rewriting results sections in the language of evid.pdf;C\:\\Users\\Admin\\Zotero\\storage\\5VGKXMXG\\S0169-5347(21)00284-6.html}
}

@article{muggli2001,
  title = {The {{Smoke You Don}}'t {{See}}: {{Uncovering Tobacco Industry Scientific Strategies Aimed Against Environmental Tobacco Smoke Policies}}},
  shorttitle = {The {{Smoke You Don}}'t {{See}}},
  author = {Muggli, Monique E. and Forster, Jean L. and Hurt, Richard D. and Repace, James L.},
  year = {2001},
  month = sep,
  journal = {American Journal of Public Health},
  volume = {91},
  number = {9},
  pages = {1419--1423},
  issn = {0090-0036, 1541-0048},
  doi = {10.2105/AJPH.91.9.1419},
  urldate = {2022-04-20},
  abstract = {Objectives.This review details the tobacco industry's scientific campaign aimed against policies addressing environmental tobacco smoke (ETS) and efforts to undermine US regulatory agencies from approximately 1988 to 1993. Methods.The public availability of more than 40 million internal, once-secret tobacco company documents allowed an unedited and historical look at tobacco industry strategies. Results. The analysis showed that the tobacco industry went to great lengths to battle the ETS issue worldwide by camouflaging its involvement and creating an impression of legitimate, unbiased scientific research. Conclusions.There is a need for further international monitoring of industry-produced science and for significant improvements in tobacco document accessibility. (Am J Public Health. 2001;91:1419\textendash 1423)},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\D6VCI9VK\\Muggli et al. - 2001 - The Smoke You Don't See Uncovering Tobacco Indust.pdf}
}

@book{murphy2014,
  title = {Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests},
  shorttitle = {Statistical Power Analysis},
  author = {Murphy, Kevin R. and Myors, Brett and Wolach, Allen H.},
  year = {2014},
  edition = {Fourth edition},
  publisher = {{Routledge, Taylor \& Francis Group}},
  address = {{New York}},
  isbn = {978-1-84872-587-4 978-1-84872-588-1},
  langid = {english},
  lccn = {QA277 .M87 2014},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\T8JQSKQP\\Murphy et al. - 2014 - Statistical power analysis a simple and general m.pdf}
}

@article{myung2003,
  title = {Tutorial on Maximum Likelihood Estimation},
  author = {Myung, In Jae},
  year = {2003},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {47},
  number = {1},
  pages = {90--100},
  issn = {0022-2496},
  doi = {10.1016/S0022-2496(02)00028-7},
  urldate = {2023-05-08},
  abstract = {In this paper, I provide a tutorial exposition on maximum likelihood estimation (MLE). The intended audience of this tutorial are researchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. Unlike least-squares estimation which is primarily a descriptive tool, MLE is a preferred method of parameter estimation in statistics and is an indispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. The purpose of this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of some of the basic principles.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TURWT595\\Myung - 2003 - Tutorial on maximum likelihood estimation.pdf;C\:\\Users\\Admin\\Zotero\\storage\\ZDWGQS9V\\S0022249602000287.html}
}

@phdthesis{nakazawa,
  title = {{Examining the Advantages and Disadvantages of Pilot Studies: Monte-Carlo Simulations}},
  shorttitle = {{Examining the Advantages and Disadvantages of Pilot Studies}},
  author = {Nakazawa, Masato},
  address = {{Ann Arbor, United States}},
  urldate = {2023-04-26},
  abstract = {Estimating population effect size accurately and precisely plays a vital role in achieving a desired level of statistical power as well as drawing correct conclusions from empirical results. While a number of common practices of effect-size estimation have been documented (e.g., relying on one's experience and intuition, and conducting pilot studies), their relative advantages and disadvantages have been insufficiently investigated. To establish a practical guideline for researchers in this respect, this project compared the accuracy and precision of effect-size estimation, resulting power, and economic implications across pilot and non-pilot conditions. Furthermore, to model the potential advantages of pilot studies in finding and correcting flaws before main studies are run, varying amounts of random error variance and varying degrees of success at its removal\textemdash often neglected aspects in simulation studies\textemdash were introduced in Experiment 2. The main findings include the following. First, pilot studies with up to 30 subjects were utterly ineffective in achieving the desired power of 0.80 at a small population effect size even under the best-case scenario. At this effect size intuitive estimation without pilot studies appears to be the preferred method of achieving the desired power. Second, the pilot studies performed better at medium and large population effect sizes, achieving comparable or even greater power to that in the non-pilot condition. The relative advantages of pilot studies were particularly evident when moderate to large error variances were present, and a portion of it had been removed through conducting pilot studies. These broad findings are discussed in the context of flexible design: study design can be modified flexibly in accordance with the researcher's particular goals.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781267283191},
  langid = {Englisch},
  keywords = {Economic performance,Estimation of population effect size,Pilot studies,Power,Psychology},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IUCGJITY\\Nakazawa - Examining the Advantages and Disadvantages of Pilo.pdf}
}

@article{nardini2013,
  title = {Bias and {{Conditioning}} in {{Sequential Medical Trials}}},
  author = {Nardini, Cecilia and Sprenger, Jan},
  year = {2013},
  month = dec,
  journal = {Philosophy of Science},
  volume = {80},
  number = {5},
  pages = {1053--1064},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/673732},
  urldate = {2022-07-11},
  abstract = {Randomized controlled trials are currently the gold standard within evidence-based medicine. Usually they are monitored for early signs of effectiveness or harm. However, evidence from trials stopped early is often charged with bias toward implausibly large effects. To our mind, this skeptical attitude is unfounded and caused by the failure to perform appropriate conditioning in the statistical analysis of the evidence. We contend that conditional hypothesis tests give a superior appreciation of the obtained evidence and significantly improve the practice of sequential medical trials, while staying firmly rooted in frequentist methodology.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\FHG35GH3\\Nardini und Sprenger - 2013 - Bias and Conditioning in Sequential Medical Trials.pdf}
}

@article{nelson2018,
  title = {Psychology's {{Renaissance}}},
  author = {Nelson, Leif D. and Simmons, Joseph and Simonsohn, Uri},
  year = {2018},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {69},
  number = {1},
  pages = {511--534},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122216-011836},
  urldate = {2021-09-23},
  abstract = {In 2010\textendash 2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists' concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TGR3KPXW\\Nelson et al. - 2018 - Psychology's Renaissance.pdf}
}

@article{neyman1933,
  title = {The Testing of Statistical Hypotheses in Relation to Probabilities a Priori},
  author = {Neyman, J. and Pearson, E. S.},
  year = {1933},
  month = oct,
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {29},
  number = {4},
  pages = {492--510},
  issn = {0305-0041, 1469-8064},
  doi = {10.1017/S030500410001152X},
  urldate = {2021-09-23},
  abstract = {In a recent paper we have discussed certain general principles underlying the determination of the most efficient tests of statistical hypotheses, but the method of approach did not involve any detailed consideration of the question of               a priori               probability. We propose now to consider more fully the bearing of the earlier results on this question and in particular to discuss what statements of value to the statistician in reaching his final judgment can be made from an analysis of observed data, which would not be modified by any change in the probabilities               a priori               . In dealing with the problem of statistical estimation, R. A. Fisher has shown how, under certain conditions, what may be described as rules of behaviour can be employed which will lead to results independent of these probabilities; in this connection he has discussed the important conception of what he terms fiducial limits. But the testing of statistical hypotheses cannot be treated as a problem in estimation, and it is necessary to discuss afresh in what sense tests can be employed which are independent of               a priori               probability laws.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\R4P4DZ7U\\Neyman und Pearson - 1933 - The testing of statistical hypotheses in relation .pdf}
}

@article{neyman1933a,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
  author = {Neyman, J. and Pearson, E. S.},
  year = {1933},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {231},
  pages = {289--337},
  issn = {0264-3952, 2053-9258},
  doi = {10.1098/rsta.1933.0009},
  urldate = {2021-09-23},
  abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a               posteriori               of the possible ``causes" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a ``system'' or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character,               x               , of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and               vice versa               . Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character,               x               , of the observed facts were properly chosen\textemdash were, in fact, a character which he terms ``en quelque sorte remarquable.''},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HK5B2W3L\\1933 - IX. On the problem of the most efficient tests of .pdf}
}

@article{neyman1942,
  title = {Basic {{Ideas}} and {{Some Recent Results}} of the {{Theory}} of {{Testing Statistical Hypotheses}}},
  author = {Neyman, J.},
  year = {1942},
  journal = {Journal of the Royal Statistical Society},
  volume = {105},
  number = {4},
  eprint = {10.2307/2980436},
  eprinttype = {jstor},
  pages = {292},
  issn = {09528385},
  doi = {10.2307/2980436},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\3K5MPCCM\\Neyman - 1942 - Basic Ideas and Some Recent Results of the Theory .pdf}
}

@article{neyman1957,
  title = {"{{Inductive Behavior}}" as a {{Basic Concept}} of {{Philosophy}} of {{Science}}},
  author = {Neyman, J.},
  year = {1957},
  journal = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
  volume = {25},
  number = {1/3},
  eprint = {1401671},
  eprinttype = {jstor},
  pages = {7},
  issn = {03731138},
  doi = {10.2307/1401671},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\BD8R3JX8\\Neyman - 1957 - Inductive Behavior as a Basic Concept of Philoso.pdf}
}

@article{nickerson2000,
  title = {Null Hypothesis Significance Testing: {{A}} Review of an Old and Continuing Controversy.},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, Raymond S.},
  year = {2000},
  journal = {Psychological Methods},
  volume = {5},
  number = {2},
  pages = {241--301},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.5.2.241},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZEFAW3AI\\Nickerson - 2000 - Null hypothesis significance testing A review of .pdf}
}

@article{nuzzo2014,
  title = {Scientific Method: {{Statistical}} Errors},
  shorttitle = {Scientific Method},
  author = {Nuzzo, Regina},
  year = {2014},
  month = feb,
  journal = {Nature},
  volume = {506},
  number = {7487},
  pages = {150--152},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/506150a},
  urldate = {2023-02-15},
  abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
  copyright = {2014 Nature Publishing Group},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\YKSK4FWT\\Nuzzo - 2014 - Scientific method Statistical errors.pdf;C\:\\Users\\Admin\\Zotero\\storage\\7BFTBMZT\\506150a.html}
}

@article{oberauer2019,
  title = {Addressing the Theory Crisis in Psychology},
  author = {Oberauer, Klaus and Lewandowsky, Stephan},
  year = {2019},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {5},
  pages = {1596--1618},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-019-01645-2},
  urldate = {2022-04-20},
  abstract = {A worrying number of psychological findings are not replicable. Diagnoses of the causes of this ``replication crisis,'' and recommendations to address it, have nearly exclusively focused on methods of data collection, analysis, and reporting. We argue that a further cause of poor replicability is the often weak logical link between theories and their empirical tests. We propose a distinction between discovery-oriented and theory-testing research. In discovery-oriented research, theories do not strongly imply hypotheses by which they can be tested, but rather define a search space for the discovery of effects that would support them. Failures to find these effects do not question the theory. This endeavor necessarily engenders a high risk of Type I errors\textemdash that is, publication of findings that will not replicate. Theory-testing research, by contrast, relies on theories that strongly imply hypotheses, such that disconfirmation of the hypothesis provides evidence against the theory. Theory-testing research engenders a smaller risk of Type I errors. A strong link between theories and hypotheses is best achieved by formalizing theories as computational models. We critically revisit recommendations for addressing the ``replication crisis,'' including the proposal to distinguish exploratory from confirmatory research, and the preregistration of hypotheses and analysis plans.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CYDNGXRD\\Oberauer und Lewandowsky - 2019 - Addressing the theory crisis in psychology.pdf}
}

@article{obrien,
  title = {8 {{Unified Power Analysis}} for T-{{Tests}} through {{Multivariate Hypotheses}}},
  author = {O'Brien, Ralph G and Muller, Keith E},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\32UX95JR\\O'Brien und Muller - 8 Unified Power Analysis for t-Tests through Multi.pdf}
}

@article{obrien1979,
  title = {A Multiple Testing Procedure for Clinical Trials},
  author = {O'Brien, Peter C. and Fleming, Thomas R.},
  year = {1979},
  journal = {Biometrics},
  volume = {35},
  number = {3},
  eprint = {2530245},
  eprinttype = {jstor},
  pages = {549--556},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2530245},
  urldate = {2023-04-27},
  abstract = {A multiple testing procedure is proposed for comparing two treatments when response to treatment is both dichotomous (i.e., success or failure) and immediate. The proposed test statistic for each test is the usual (Pearson) chi-square statistic based on all data collected to that point. The maximum number (N) of tests and the number (m1 + m2) of observations collected between successive tests is fixed in advance. The overall size of the procedure is shown to be controlled with virtually the same accuracy as the single sample chi-square test based on N(m1 + m2) observations. The power is also found to be virtually the same. However, by affording the opportunity to terminate early when one treatment performs markedly better than the other, the multiple testing procedure may eliminate the ethical dilemmas that often accompany clinical trials.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TCLGXCIL\\O'Brien und Fleming - 1979 - A Multiple Testing Procedure for Clinical Trials.pdf}
}

@article{okada2013,
  title = {Is {{Omega Squared Less Biased}}? A {{Comparison}} of {{Three Major Effect Size Indices}} in {{One-Way Anova}}},
  shorttitle = {Is {{Omega Squared Less Biased}}?},
  author = {Okada, Kensuke},
  year = {2013},
  month = jul,
  journal = {Behaviormetrika},
  volume = {40},
  number = {2},
  pages = {129--147},
  issn = {1349-6964},
  doi = {10.2333/bhmk.40.129},
  urldate = {2023-03-01},
  abstract = {The purpose of this study is to find less biased effect size index in one-way analysis of variance (ANOVA) by performing a thorough Monte Carlo study with 1,000,000 replications per condition. Our results show that contrary to common belief, epsilon squared is the least biased among the threemajorindices, while omega squared produces the least root mean squared errors, for all conditions. Although eta squared results in the least standard deviation, this does not necessarily make it a good estimator because a considerable amount of bias still occurs when the sample size is small.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\MQC9RYHA\\Okada - 2013 - Is Omega Squared Less Biased a Comparison of Thre.pdf}
}

@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\V6WUAKKL\\Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{opperman2019,
  title = {Sequential Probability Ratio Test for Skew Normal Distribution},
  author = {Opperman, Logan and Ning, Wei},
  year = {2019},
  month = jun,
  journal = {Communications in Statistics - Simulation and Computation},
  pages = {1--14},
  issn = {0361-0918, 1532-4141},
  doi = {10.1080/03610918.2019.1614623},
  urldate = {2021-09-23},
  abstract = {We consider a sequence of independent observations from a standard skew normal distribution SN\dh k\TH{} with the shape parameter k 2 R: For testing the null hypothesis H0 : k {$\frac{1}{4}$} k0 versus H1 : k {$\frac{1}{4}$} k1; Neyman-Pearson lemma provides a uniformly most powerful (UMP) test with a fixed sample size. In this paper, we adapt Wald's sequential probability ratio test for testing such hypotheses with the sample size being a random variable with the specified Type I and Type II errors. Approximations for the operating characteristic (OC) and average sample number (ASN) functions are derived. Simulations are conducted to indicate the performance of the proposed test under different scenarios. A real example is given to illustrate the testing process.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DKLLCAKR\\Opperman und Ning - 2019 - Sequential probability ratio test for skew normal .pdf}
}

@article{osborne,
  title = {The Power of Outliers (and Why Researchers Should {{ALWAYS}} Check for Them)},
  author = {Osborne, Jason W. and Overbay, Amy},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/QF69-7K43},
  urldate = {2022-10-19},
  abstract = {There has been much debate in the literature regarding what to do with extreme or influential data points. The goal of this paper is to summarize the various potential causes of extreme scores in a data set (e.g., data recording or entry errors, motivated mis-reporting, sampling errors, and legitimate sampling), how to detect them, and whether they should be removed or not.  Another goal of this paper was to explore how significantly a small proportion of outliers can affect even simple analyses.  The examples show a strong beneficial effect of removal of extreme scores.  Accuracy tended to increase significantly and substantially, and errors of inference tended to drop significantly and substantially once extreme scores were removed},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EGYXYVSL\\Krantz - 1999 - The Null Hypothesis Testing Controversy in Psychol.pdf;C\:\\Users\\Admin\\Zotero\\storage\\P7LM43HY\\Osborne und Overbay - The power of outliers (and why researchers should .pdf}
}

@article{pausch2019,
  title = {{Die Berechnung des Konfidenzintervalls f\"ur die Effektgr\"o\ss e Cohen's d}},
  author = {Pausch, Viola},
  year = {2019},
  month = mar,
  journal = {Jahrbuch Musikpsychologie},
  volume = {28},
  pages = {e29},
  issn = {2569-5665},
  doi = {10.5964/jbdgm.2018v28.29},
  urldate = {2022-08-09},
  abstract = {The effect size Cohen's d allows for a quantitative and metric-free estimation of an effect. This effect can be the result of the deviation of a mean value from a certain value or the mean difference between two samples. The precision of this estimation is given by the width of a confidence interval for the effect size Cohen's d. The aim of this article is to show the importance of noncentral t distributions for a precise estimation of confidence intervals for Cohen's d and to explain how to compute them. On the Open Science Framework online platform, two programs in R are freely available that calculate the confidence intervals for Cohen's d for one or two samples based on the following input variables: confidence level (e.g. 95\%), sample size(s), mean(s) and standard deviation(s). The article concludes by illustrating the discussed approach with an example.},
  langid = {ngerman},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SGN7E6MI\\Pausch - 2019 - Die Berechnung des Konfidenzintervalls für die Eff.pdf}
}

@article{peikert2021,
  title = {A {{Reproducible Data Analysis Workflow With R Markdown}}, {{Git}}, {{Make}}, and {{Docker}}},
  author = {Peikert, Aaron and Brandmaier, Andreas M.},
  year = {2021},
  month = may,
  journal = {Quantitative and Computational Methods in Behavioral Sciences},
  pages = {1--27},
  issn = {2699-8432},
  doi = {10.5964/qcmb.3763},
  urldate = {2023-06-15},
  abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles, and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), 2) the analysis exactly reproduces at a later point in time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible. While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Combining containerization, dependence management, version management, and dynamic document generation, the proposed workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
  copyright = {Copyright (c) 2021 Aaron Peikert, Andreas M. Brandmaier},
  langid = {english},
  keywords = {containerization,dependency management,dynamic document generation,open science,R,reproducibility,version management},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DVMSQ9F3\\Peikert und Brandmaier - 2021 - A Reproducible Data Analysis Workflow With R Markd.pdf}
}

@article{perezgonzalez2015,
  title = {Fisher, {{Neyman-Pearson}} or {{NHST}}? {{A}} Tutorial for Teaching Data Testing},
  shorttitle = {Fisher, {{Neyman-Pearson}} or {{NHST}}?},
  author = {Perezgonzalez, Jose D.},
  year = {2015},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00223},
  urldate = {2021-09-23},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\WAD45YAL\\Perezgonzalez - 2015 - Fisher, Neyman-Pearson or NHST A tutorial for tea.pdf}
}

@article{pernet2011,
  title = {{{LIMO EEG}}: {{A Toolbox}} for {{Hierarchical LInear MOdeling}} of {{ElectroEncephaloGraphic Data}}},
  shorttitle = {{{LIMO EEG}}},
  author = {Pernet, Cyril R. and Chauveau, Nicolas and Gaspar, Carl and Rousselet, Guillaume A.},
  year = {2011},
  month = feb,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2011},
  pages = {e831409},
  publisher = {{Hindawi}},
  issn = {1687-5265},
  doi = {10.1155/2011/831409},
  urldate = {2022-02-21},
  abstract = {Magnetic- and electric-evoked brain responses have traditionally been analyzed by comparing the peaks or mean amplitudes of signals from selected channels and averaged across trials. More recently, tools have been developed to investigate single trial response variability (e.g., EEGLAB) and to test differences between averaged evoked responses over the entire scalp and time dimensions (e.g., SPM, Fieldtrip). LIMO EEG is a Matlab toolbox (EEGLAB compatible) to analyse evoked responses over all space and time dimensions, while accounting for single trial variability using a simple hierarchical linear modelling of the data. In addition, LIMO EEG provides robust parametric tests, therefore providing a new and complementary tool in the analysis of neural evoked responses.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\NFKUWEI6\\Pernet et al. - 2011 - LIMO EEG A Toolbox for Hierarchical LInear MOdeli.pdf;C\:\\Users\\Admin\\Zotero\\storage\\HFLCH244\\831409.html}
}

@article{pinheiro1997,
  title = {Estimating and Reducing Bias in Group Sequential Designs with {{Gaussian}} Independent Increment Structure},
  author = {Pinheiro, J.},
  year = {1997},
  month = dec,
  journal = {Biometrika},
  volume = {84},
  number = {4},
  pages = {831--845},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/84.4.831},
  urldate = {2022-07-11},
  abstract = {A concern in the use of sequential testing procedures is the bias associated with the estimates of treatment differences. Clinical trials that stop early because of evidence of therapeutic benefit are prone to exaggerate the magnitude of the treatment effect. We consider methods for estimating and reducing the bias of treatment differences estimators in group sequential designs with Gaussian independent increment structure. We derive an analytical expression for the bias and give an easy-to-calculate approximate bound for its variation. A simulation estimate of the bias, based on a Gaussian independent increment structure, is also described and a related bias reduced estimator is considered.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\J9UVY5LV\\Pinheiro - 1997 - Estimating and reducing bias in group sequential d.pdf}
}

@article{pocock1977,
  title = {Group Sequential Methods in the Design and Analysis of Clinical Trials},
  author = {Pocock, Stuart J.},
  year = {1977},
  month = aug,
  journal = {Biometrika},
  volume = {64},
  number = {2},
  pages = {191--199},
  issn = {0006-3444},
  doi = {10.1093/biomet/64.2.191},
  urldate = {2023-04-27},
  abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SDXLUXWW\\POCOCK - 1977 - Group sequential methods in the design and analysi.pdf;C\:\\Users\\Admin\\Zotero\\storage\\XX4732CP\\384776.html}
}

@article{pocock1989,
  title = {Practical Problems in Interim Analyses, with Particular Regard to Estimation},
  author = {Pocock, Stuart J. and Hughes, Michael D.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4, Supplement 1},
  pages = {209--221},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90059-7},
  urldate = {2023-03-14},
  abstract = {This article considers some of the practical problems inherent in interim analyses and stopping rules for randomized clinical trials. Topics covered include group sequential designs, trials with unplanned interim analyses, estimation problems in clinical trials with planned interim analyses, and the balance between individual and collective ethics. Particular attention is paid to the fact that clinical trials that stop early are prone to exaggerate the magnitude of treatment effect. Accordingly, a Bayesian ``shrinkage'' method of analysis is proposed to help quantify the extent to which surprisingly large point and interval estimates of treatment difference in clinical trials that stop early should be moderated.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LD9N8GPJ\\Pocock und Hughes - 1989 - Practical problems in interim analyses, with parti.pdf;C\:\\Users\\Admin\\Zotero\\storage\\FKDGSHGF\\0197245689900597.html}
}

@misc{pownall2021,
  title = {Embedding Open and Reproducible Science into Teaching: {{A}} Bank of Lesson Plans and Resources},
  shorttitle = {Embedding Open and Reproducible Science into Teaching},
  author = {Pownall, Madeleine and Azevedo, Flavio and Aldoh, Alaa and Elsherif, Mahmoud and Vasilev, Martin Rachev and Pennington, Charlotte Rebecca and Robertson, Olly and Tromp, Myrthe Vel and Liu, Meng and Makel, Matthew C. and Tonge, Natasha April and Moreau, David and Horry, Ruth and Shaw, John J. and Tzavella, Loukia and McGarrigle, Ronan and Talbot, Catherine V. and FORRT and Parsons, Sam},
  year = {2021},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/fgv79},
  urldate = {2022-01-04},
  abstract = {[Note: This paper is in press at Scholarship of Teaching and Learning in Psychology]. Recently, there has been a growing emphasis on embedding open and reproducible approaches into research. One essential step in accomplishing this larger goal is to embed such practices into undergraduate and postgraduate research training. However, this often requires substantial time and resources to implement. Also, while many pedagogical resources are regularly developed for this purpose, they are not often openly and actively shared with the wider community. The creation and public sharing of open educational resources is useful for educators who wish to embed open scholarship and reproducibility into their teaching and learning. In this article, we describe and openly share a bank of teaching resources and lesson plans on the broad topics of open scholarship, open science, replication, and reproducibility that can be integrated into taught courses, to support educators and instructors. These resources were created as part of the Society for the Improvement of Psychological Science (SIPS) hackathon at the 2021 Annual Conference, and we detail this collaborative process in the article. By sharing these open pedagogical resources, we aim to reduce the labour required to develop and implement open scholarship content to further the open scholarship and open educational materials movement.},
  langid = {american}
}

@article{pramanik2021,
  title = {A Modified Sequential Probability Ratio Test},
  author = {Pramanik, Sandipan and Johnson, Valen E. and Bhattacharya, Anirban},
  year = {2021},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {101},
  pages = {102505},
  issn = {00222496},
  doi = {10.1016/j.jmp.2021.102505},
  urldate = {2021-11-04},
  abstract = {We describe a modified sequential probability ratio test that can be used to reduce the average sample size required to perform statistical hypothesis tests at specified levels of significance and power. Examples are provided for z tests, t tests, and tests of binomial success probabilities. A description of a software package to implement the test designs is provided. We compare the sample sizes required in fixed design tests conducted at 5\% significance levels to the average sample sizes required in sequential tests conducted at 0.5\% significance levels, and we find that the two sample sizes are approximately equal.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\96EMIYBI\\Pramanik et al. - 2021 - A modified sequential probability ratio test.pdf}
}

@article{pramanik2021a,
  title = {A Modified Sequential Probability Ratio Test},
  author = {Pramanik, Sandipan and Johnson, Valen E. and Bhattacharya, Anirban},
  year = {2021},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {101},
  pages = {102505},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2021.102505},
  urldate = {2023-06-15},
  abstract = {We describe a modified sequential probability ratio test that can be used to reduce the average sample size required to perform statistical hypothesis tests at specified levels of significance and power. Examples are provided for z tests, t tests, and tests of binomial success probabilities. A description of a software package to implement the test designs is provided. We compare the sample sizes required in fixed design tests conducted at 5\% significance levels to the average sample sizes required in sequential tests conducted at 0.5\% significance levels, and we find that the two sample sizes are approximately equal.},
  langid = {english},
  keywords = {Bayes factor,MaxSPRT,Sequential Bayes factor,Sequential design,Sequential probability ratio test,Significance test,Uniformly most powerful Bayesian test},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\FGXKU5D7\\Pramanik et al. - 2021 - A modified sequential probability ratio test.pdf;C\:\\Users\\Admin\\Zotero\\storage\\G65W45QX\\S0022249621000109.html}
}

@book{proschan2006,
  title = {Statistical Monitoring of Clinical Trials: {{A}} Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, KK Gordon and Wittes, Janet Turk},
  year = {2006},
  publisher = {{Springer Science \& Business Media}},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IHIQ5L2B\\books.html}
}

@article{protzko2021,
  title = {Testing the Structure of Human Cognitive Ability Using Evidence Obtained from the Impact of Brain Lesions over Abilities},
  author = {Protzko, John and Colom, Roberto},
  year = {2021},
  month = nov,
  journal = {Intelligence},
  volume = {89},
  pages = {101581},
  issn = {01602896},
  doi = {10.1016/j.intell.2021.101581},
  urldate = {2021-11-04},
  abstract = {Here we examine three classes of models regarding the structure of human cognition: common cause models, sampling/network models, and interconnected models. That disparate models can accommodate one of the most globally replicated psychological phenomena\textemdash namely, the positive manifold\textemdash is an extension of under\- determination of theory by data. Statistical fit indices are an insufficient and sometimes intractable method of demarcating between the theories; strict tests and further evidence should be brought to bear on understanding the potential causes of the positive manifold. The cognitive impact of focal cortical lesions allows testing the necessary causal connections predicted by competing models. This evidence shows focal cortical lesions lead to local, not global (across all abilities), deficits. Only models that can accommodate a deficit in a given ability without effects on other covarying abilities can accommodate focal lesion evidence. After studying how different models pass this test, we suggest bifactor models (class: common cause models) and bond models (class: sampling models) are best supported. In short, competing psychometric models can be informed when their implied causal connections and predictions are tested.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\M6N4TLGS\\Protzko und Colom - 2021 - Testing the structure of human cognitive ability u.pdf}
}

@article{richard2003,
  title = {One Hundred Years of Social Psychology Quantitatively Described},
  author = {Richard, F. D. and Bond, Charles F. and {Stokes-Zoota}, Juli J.},
  year = {2003},
  month = dec,
  journal = {Review of General Psychology},
  volume = {7},
  number = {4},
  pages = {331--363},
  issn = {1089-2680, 1939-1552},
  doi = {10.1037/1089-2680.7.4.331},
  urldate = {2023-03-28},
  abstract = {This article compiles results from a century of social psychological research, more than 25,000 studies of 8 million people. A large number of social psychological conclusions are listed alongside meta-analytic information about the magnitude and variability of the corresponding effects. References to 322 meta-analyses of social psychological phenomena are presented, as well as statistical effect-size summaries. Analyses reveal that social psychological effects typically yield a value of r equal to.21 and that, in the typical research literature, effects vary from study to study in ways that produce a standard deviation in r of.15. Uses, limitations, and implications of this large-scale compilation are noted.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\MAKG5KRM\\Richard et al. - 2003 - One Hundred Years of Social Psychology Quantitativ.pdf}
}

@article{roberts2007,
  title = {The Power of Personality: {{The}} Comparative Validity of Personality Traits, Socioeconomic Status, and Cognitive Ability for Predicting Important Life Outcomes},
  shorttitle = {The Power of Personality},
  author = {Roberts, Brent W. and Kuncel, Nathan R. and Shiner, Rebecca and Caspi, Avshalom and Goldberg, Lewis R.},
  year = {2007},
  month = dec,
  journal = {Perspectives on Psychological Science},
  volume = {2},
  number = {4},
  pages = {313--345},
  issn = {1745-6916, 1745-6924},
  doi = {10.1111/j.1745-6916.2007.00047.x},
  urldate = {2023-03-28},
  abstract = {The ability of personality traits to predict important life outcomes has traditionally been questioned because of the putative small effects of personality. In this article, we compare the predictive validity of personality traits with that of socioeconomic status (SES) and cognitive ability to test the relative contribution of personality traits to predictions of three critical outcomes: mortality, divorce, and occupational attainment. Only evidence from prospective longitudinal studies was considered. In addition, an attempt was made to limit the review to studies that controlled for important background factors. Results showed that the magnitude of the effects of personality traits on mortality, divorce, and occupational attainment was indistinguishable from the effects of SES and cognitive ability on these outcomes. These results demonstrate the influence of personality traits on important life outcomes, highlight the need to more routinely incorporate measures of personality into quality of life surveys, and encourage further research about the developmental origins of personality traits and the processes by which these traits influence diverse life outcomes.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\8MHZY46E\\Roberts et al. - 2007 - The Power of Personality The Comparative Validity.pdf}
}

@misc{rogerginersorolla2018,
  title = {Powering {{Your Interaction}}},
  author = {{rogerginersorolla}, Author},
  year = {2018},
  month = jan,
  journal = {Approaching Significance},
  urldate = {2022-01-18},
  abstract = {With all the manuscripts I see, as editor-in-chief of Journal of Experimental Social Psychology, it's clear that authors are following a wide variety of standards for statistical power analys\ldots},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZPGNNAXA\\powering-your-interaction-2.html}
}

@article{rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.86.3.638},
  urldate = {2023-05-02},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5FQ6V2ZB\\Rosenthal - The file drawer problem and tolerance for null res.pdf;C\:\\Users\\Admin\\Zotero\\storage\\7SZC7WD3\\1979-27602-001.html}
}

@article{rossi1990,
  title = {Statistical Power of Psychological Research: {{What}} Have We Gained in 20 Years?},
  shorttitle = {Statistical Power of Psychological Research},
  author = {Rossi, Joseph S.},
  year = {1990},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {58},
  number = {5},
  pages = {646--656},
  issn = {1939-2117, 0022-006X},
  doi = {10.1037/0022-006X.58.5.646},
  urldate = {2023-02-15},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PDKKGZ8K\\Rossi - 1990 - Statistical power of psychological research What .pdf}
}

@article{rouder2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  urldate = {2021-09-23},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\RPP8TDBG\\Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the n.pdf}
}

@article{rouder2009a,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.2.225},
  urldate = {2023-04-17},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  langid = {english},
  keywords = {Akaike Information Criterion,Marginal Likelihood,Posterior Odds,Prior Standard Deviation,Subliminal Priming},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HDYYZ98N\\Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the n.pdf}
}

@article{rouder2014,
  title = {Optional Stopping: {{No}} Problem for {{Bayesians}}},
  shorttitle = {Optional Stopping},
  author = {Rouder, Jeffrey N.},
  year = {2014},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {2},
  pages = {301--308},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-014-0595-4},
  urldate = {2023-02-22},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\VUPRL5HB\\Rouder - 2014 - Optional stopping No problem for Bayesians.pdf}
}

@techreport{rouder2021,
  type = {Preprint},
  title = {Principles of {{Model Specification}} in {{ANOVA Designs}}},
  author = {Rouder, Jeffrey and Schnuerch, Martin and Haaf, Julia M. and Morey, Richard Donald},
  year = {2021},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/e56ab},
  urldate = {2022-05-10},
  abstract = {ANOVA\textemdash the workhorse of experimental psychology\textemdash seems well understood in that behavioral sciences have agreedupon contrasts and reporting conventions. Yet, we argue this consensus hides considerable flaws in common ANOVA procedures, and these flaws become especially salient in the within-subject and mixed-model cases. The main thesis is that these flaws are in model specification. The specifications underlying common use are deficient from a substantive perspective, that is, they do not match reality in behavioral experiments. The problem, in particular, is that specifications rely on coincidental rather than robust statements about reality. We provide specifications that avoid making arguments based on coincidences, and note these Bayes factor model comparisons among these specifications are already convenient in the BayesFactor package. Finally, we argue that model specification necessarily and critically reflects substantive concerns, and, consequently, is ultimately the responsibility of substantive researchers. Source code for this project is at github/PerceptionAndCognitionLab/stat aov2.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TW5CS6V6\\Rouder et al. - 2021 - Principles of Model Specification in ANOVA Designs.pdf}
}

@article{rozeboom1960,
  title = {The Fallacy of the Null-Hypothesis Significance Test.},
  author = {Rozeboom, William W.},
  year = {1960},
  journal = {Psychological Bulletin},
  volume = {57},
  number = {5},
  pages = {416--428},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/h0042040},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CY953RTJ\\Rozeboom1960.pdf}
}

@article{rushton1950,
  title = {On a Sequential T-Test},
  author = {Rushton, S},
  year = {1950},
  journal = {Biometrika},
  volume = {37},
  pages = {326--333},
  doi = {10.2307/2332385},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\BPQJXCQQ\\Rushton - 2021 - On a Sequential t-Test.pdf}
}

@article{rushton1952,
  title = {On a {{Two-Sided Sequential}} t-{{Test}}},
  author = {Rushton, S.},
  year = {1952},
  month = dec,
  journal = {Biometrika},
  volume = {39},
  number = {3/4},
  eprint = {2334026},
  eprinttype = {jstor},
  pages = {302},
  issn = {00063444},
  doi = {10.2307/2334026},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\34ZUGIQC\\Rushton - 1952 - On a Two-Sided Sequential t-Test.pdf}
}

@article{ryan2019,
  title = {Using {{Bayesian}} Adaptive Designs to Improve Phase {{III}} Trials: A Respiratory Care Example},
  shorttitle = {Using {{Bayesian}} Adaptive Designs to Improve Phase {{III}} Trials},
  author = {Ryan, Elizabeth G. and Bruce, Julie and Metcalfe, Andrew J. and Stallard, Nigel and Lamb, Sarah E. and Viele, Kert and Young, Duncan and Gates, Simon},
  year = {2019},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {99},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0739-3},
  urldate = {2021-09-23},
  abstract = {Background: Bayesian adaptive designs can improve the efficiency of trials, and lead to trials that can produce high quality evidence more quickly, with fewer patients and lower costs than traditional methods. The aim of this work was to determine how Bayesian adaptive designs can be constructed for phase III clinical trials in critical care, and to assess the influence that Bayesian designs would have on trial efficiency and study results. Methods: We re-designed the High Frequency OSCillation in Acute Respiratory distress syndrome (OSCAR) trial using Bayesian adaptive design methods, to allow for the possibility of early stopping for success or futility. We constructed several alternative designs and studied their operating characteristics via simulation. We then performed virtual re-executions by applying the Bayesian adaptive designs using the OSCAR data to demonstrate the practical applicability of the designs. Results: We constructed five alternative Bayesian adaptive designs and identified a preferred design based on the simulated operating characteristics, which had similar power to the original design but recruited fewer patients on average. The virtual re-executions showed the Bayesian sequential approach and original OSCAR trial yielded similar trial conclusions. However, using a Bayesian sequential design could have led to a reduced sample size and earlier completion of the trial. Conclusions: Using the OSCAR trial as an example, this case study found that Bayesian adaptive designs can be constructed for phase III critical care trials. If the OSCAR trial had been run using one of the proposed Bayesian adaptive designs, it would have terminated at a smaller sample size with fewer deaths in the trial, whilst reaching the same conclusions. We recommend the wider use of Bayesian adaptive approaches in phase III clinical trials. Trial registration: OSCAR Trial registration ISRCTN, ISRCTN10416500. Retrospectively registered 13 June 2007.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HKTVVLVH\\Ryan et al. - 2019 - Using Bayesian adaptive designs to improve phase I.pdf}
}

@article{saltelli2017,
  title = {What Is Science's Crisis Really about?},
  author = {Saltelli, Andrea and Funtowicz, Silvio},
  year = {2017},
  month = aug,
  journal = {Futures},
  volume = {91},
  pages = {5--11},
  issn = {00163287},
  doi = {10.1016/j.futures.2017.05.010},
  urldate = {2021-09-23},
  abstract = {Present day reasoning about difficulties in science reproducibility, science governance, and the use of science for policy could benefit from a philosophical and historical perspective. This would show that the present crisis was anticipated by some scholars of these disciplines, and that diagnoses were offered which are not yet mainstream among crisis-aware disciplines, from statistics to medicine, from bibliometrics to biology. Diagnoses in turn open the path to possible solutions. This discussion is urgent given the impact of the crises on public trust in institutions. We ask whether the present crisis may be seminal in terms of drawing attention to alternative visions for the role of Science in society, and its relevant institutional arrangements. We finish by offering a number of suggestions in this direction.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\W2HU6UMY\\Saltelli und Funtowicz - 2017 - What is science’s crisis really about.pdf}
}

@article{schmider2010,
  title = {Is It Really Robust?: {{Reinvestigating}} the Robustness of {{ANOVA}} against Violations of the Normal Distribution Assumption},
  shorttitle = {Is It Really Robust?},
  author = {Schmider, Emanuel and Ziegler, Matthias and Danay, Erik and Beyer, Luzi and B{\"u}hner, Markus},
  year = {2010},
  month = jan,
  journal = {Methodology},
  volume = {6},
  number = {4},
  pages = {147--151},
  issn = {1614-1881, 1614-2241},
  doi = {10.1027/1614-2241/a000016},
  urldate = {2022-07-11},
  abstract = {Empirical evidence to the robustness of the analysis of variance (ANOVA) concerning violation of the normality assumption is presented by means of Monte Carlo methods. High-quality samples underlying normally, rectangularly, and exponentially distributed basic populations are created by drawing samples which consist of random numbers from respective generators, checking their goodness of fit, and allowing only the best 10\% to take part in the investigation. A one-way fixed-effect design with three groups of 25 values each is chosen. Effect-sizes are implemented in the samples and varied over a broad range. Comparing the outcomes of the ANOVA calculations for the different types of distributions, gives reason to regard the ANOVA as robust. Both, the empirical type I error {$\alpha$} and the empirical type II error {$\beta$} remain constant under violation. Moreover, regression analysis identifies the factor ``type of distribution'' as not significant in explanation of the ANOVA results.},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LWDGVKID\\Schmider et al. - 2010 - Is It Really Robust Reinvestigating the Robustne.pdf}
}

@article{schmidt2016,
  title = {The Crisis of Confidence in Research Findings in Psychology: {{Is}} Lack of Replication the Real Problem? {{Or}} Is It Something Else?},
  shorttitle = {The Crisis of Confidence in Research Findings in Psychology},
  author = {Schmidt, Frank L. and Oh, In-Sue},
  year = {2016},
  month = jun,
  journal = {Archives of Scientific Psychology},
  volume = {4},
  number = {1},
  pages = {32--37},
  issn = {2169-3269},
  doi = {10.1037/arc0000029},
  urldate = {2021-09-23},
  abstract = {Many people have worried that research studies are not repeated or replicated to ensure the results are real. But in most research areas meta-analyses combining across numerous studies on a question have been published. This shows most studies are being replicated. Some argue that if an attempt to replicate a study produces a result that is not statistically significant, this indicates that the first study was in error. This is usually not the case, because many replication studies have only a limited probability of ``detecting'' a real effect (called low statistical power). This also applies if the initial study does not get statistically significant results\textemdash the effect or relation may exist but the study failed to detect it. So even a study like that needs to be replicated. Meta-analyses, which combine the results of multiple studies, can solve these problems but only if the problems of publication bias and questionable research practices are first solved. Publication bias occurs when only studies that ``get the `right' results'' are published, and questionable research practices distort the research results reported. Both are described in this article.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JSFSIUKE\\Schmidt und Oh - 2016 - The crisis of confidence in research findings in p.pdf}
}

@article{schnuerch,
  title = {Dissertation {{Martin Schnuerch}}},
  author = {Schnuerch, Martin},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ECPSX26D\\Schnuerch - Dissertation Martin Schnuerch.pdf}
}

@article{schnuerch2020,
  title = {Controlling Decision Errors with Minimal Costs: {{The}} Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  year = {2020},
  month = apr,
  journal = {Psychological Methods},
  volume = {25},
  number = {2},
  pages = {206--226},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000234},
  urldate = {2021-09-23},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\G42Z22M6\\Schnuerch und Erdfelder - 2020 - Controlling decision errors with minimal costs Th.pdf}
}

@article{schnuerch2020a,
  title = {Sequential Hypothesis Tests for Multinomial Processing Tree Models},
  author = {Schnuerch, Martin and Erdfelder, Edgar and Heck, Daniel W.},
  year = {2020},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {95},
  pages = {102326},
  issn = {00222496},
  doi = {10.1016/j.jmp.2020.102326},
  urldate = {2023-04-24},
  abstract = {Stimulated by William H. Batchelder's seminal contributions in the 1980s and 1990s, multinomial processing tree (MPT) modeling has become a powerful and frequently used method in various research fields, most prominently in cognitive psychology and social cognition research. MPT models allow for estimation of, and statistical tests on, parameters that represent psychological processes underlying responses to cognitive tasks. Therefore, their use has also been proposed repeatedly for purposes of psychological assessment, for example, in clinical settings to identify specific cognitive deficits in individuals. However, a considerable drawback of individual MPT analyses emerges from the limited number of data points per individual, resulting in estimation bias, large standard errors, and low power of statistical tests. Classical test procedures such as Neyman\textendash Pearson tests often require very large sample sizes to ensure sufficiently low Type 1 and Type 2 error probabilities. Herein, we propose sequential probability ratio tests (SPRTs) as an efficient alternative. Unlike Neyman\textendash Pearson tests, sequential tests continuously monitor the data and terminate when a predefined criterion is met. As a consequence, SPRTs typically require only about half of the Neyman\textendash Pearson sample size without compromising error probability control. We illustrate the SPRT approach to statistical inference for simple hypotheses in single-parameter MPT models. Moreover, a large-sample approximation, based on ML theory, is presented for typical MPT models with more than one unknown parameter. We evaluate the properties of the proposed test procedures by means of simulations. Finally, we discuss benefits and limitations of sequential MPT analysis.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\I9IVWS4Q\\Schnuerch et al. - 2020 - Sequential hypothesis tests for multinomial proces.pdf}
}

@article{schnuerch2022,
  title = {Waldian t Tests: {{Sequential Bayesian}} t Tests with Controlled Error Probabilities.},
  shorttitle = {Waldian t Tests},
  author = {Schnuerch, Martin and Heck, Daniel W. and Erdfelder, Edgar},
  year = {2022},
  month = apr,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000492},
  urldate = {2023-03-09},
  abstract = {Bayesian t tests have become increasingly popular alternatives to null-hypothesis significance testing (NHST) in psychological research. In contrast to NHST, they allow for the quantification of evidence in favor of the null hypothesis and for optional stopping. A major drawback of Bayesian t tests, however, is that error probabilities of statistical decisions remain uncontrolled. Previous approaches in the literature to remedy this problem require time-consuming simulations to calibrate decision thresholds. In this article, we propose a sequential probability ratio test that combines Bayesian t tests with simple decision criteria developed by Abraham Wald in 1947. We discuss this sequential procedure, which we call Waldian t test, in the context of three recently proposed specifications of Bayesian t tests. Waldian t tests preserve the key idea of Bayesian t tests by assuming a distribution for the effect size under the alternative hypothesis. At the same time, they control expected frequentist error probabilities, with the nominal Type I and Type II error probabilities serving as upper bounds to the actual expected error rates under the specified statistical models. Thus, Waldian t tests are fully justified from both a Bayesian and a frequentist point of view. We highlight the relationship between Bayesian and frequentist error probabilities and critically discuss the implications of conventional stopping criteria for sequential Bayesian t tests. Finally, we provide a user-friendly web application that implements the proposed procedure for interested researchers.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XELAYMT2\\Schnuerch et al. - 2022 - Waldian t tests Sequential Bayesian t tests with .pdf}
}

@article{schonbrodt2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences.},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  year = {2017},
  month = jun,
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {322--339},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000061},
  urldate = {2021-09-23},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\NN9HJJMT\\Schönbrodt et al. - 2017 - Sequential hypothesis testing with Bayes factors .pdf}
}

@article{schonbrodt2018,
  title = {Bayes Factor Design Analysis: {{Planning}} for Compelling Evidence},
  shorttitle = {Bayes Factor Design Analysis},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {128--142},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1230-y},
  urldate = {2021-09-23},
  abstract = {A sizeable literature exists on the use of frequentist power analysis in the null-hypothesis significance testing (NHST) paradigm to facilitate the design of informative experiments. In contrast, there is almost no literature that discusses the design of experiments when Bayes factors (BFs) are used as a measure of evidence. Here we explore Bayes Factor Design Analysis (BFDA) as a useful tool to design studies for maximum efficiency and informativeness. We elaborate on three possible BF designs, (a) a fixed-n design, (b) an open-ended Sequential Bayes Factor (SBF) design, where researchers can test after each participant and can stop data collection whenever there is strong evidence for either H1 or H0, and (c) a modified SBF design that defines a maximal sample size where data collection is stopped regardless of the current state of evidence. We demonstrate how the properties of each design (i.e., expected strength of evidence, expected sample size, expected probability of misleading evidence, expected probability of weak evidence) can be evaluated using Monte Carlo simulations and equip researchers with the necessary information to compute their own Bayesian design analyses.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\FT5CZ473\\Schönbrodt und Wagenmakers - 2018 - Bayes factor design analysis Planning for compell.pdf}
}

@article{seo,
  title = {A {{Review}} and {{Comparison}} of {{Methods}} for {{Detecting Outliers}} in {{Univariate Data Sets}}},
  author = {Seo, Songwon},
  pages = {59},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZRYCMNVX\\Seo - A Review and Comparison of Methods for Detecting O.pdf}
}

@article{shi2019,
  title = {Control of {{Type I Error Rates}} in {{Bayesian Sequential Designs}}},
  author = {Shi, Haolun and Yin, Guosheng},
  year = {2019},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {2},
  issn = {1936-0975},
  doi = {10.1214/18-BA1109},
  urldate = {2021-09-23},
  abstract = {Bayesian approaches to phase II clinical trial designs are usually based on the posterior distribution of the parameter of interest and calibration of certain threshold for decision making. If the posterior probability is computed and assessed in a sequential manner, the design may involve the problem of multiplicity, which, however, is often a neglected aspect in Bayesian trial designs. To effectively maintain the overall type I error rate, we propose solutions to the problem of multiplicity for Bayesian sequential designs and, in particular, the determination of the cutoff boundaries for the posterior probabilities. We present both theoretical and numerical methods for finding the optimal posterior probability boundaries with {$\alpha$}-spending functions that mimic those of the frequentist group sequential designs. The theoretical approach is based on the asymptotic properties of the posterior probability, which establishes a connection between the Bayesian trial design and the frequentist group sequential method. The numerical approach uses a sandwich-type searching algorithm, which immensely reduces the computational burden. We apply least-square fitting to find the {$\alpha$}-spending function closest to the target. We discuss the application of our method to single-arm and doublearm cases with binary and normal endpoints, respectively, and provide a real trial example for each case.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ET69U9UC\\Shi und Yin - 2019 - Control of Type I Error Rates in Bayesian Sequenti.pdf}
}

@article{shrout2018,
  title = {Psychology, {{Science}}, and {{Knowledge Construction}}: {{Broadening Perspectives}} from the {{Replication Crisis}}},
  shorttitle = {Psychology, {{Science}}, and {{Knowledge Construction}}},
  author = {Shrout, Patrick E. and Rodgers, Joseph L.},
  year = {2018},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {69},
  number = {1},
  pages = {487--510},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122216-011845},
  urldate = {2021-09-23},
  abstract = {Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregistration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LDKQU87T\\Shrout und Rodgers - 2018 - Psychology, Science, and Knowledge Construction B.pdf}
}

@article{simmons2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  urldate = {2021-09-23},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\TEAQHAFI\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@misc{simon2021,
  title = {How to {{Use Sci-Hub}} to Download {{PDF}} ({{Full Text}}) with {{Zotero}}},
  author = {Simon},
  year = {2021},
  month = dec,
  journal = {Medium},
  urldate = {2022-08-09},
  abstract = {Add a custom PDF resolver in Zotero},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2AY48S6M\\use-sci-hub-with-zotero-as-a-fall-back-pdf-resolver-cf139eb2cea7.html}
}

@article{skovlund1988,
  title = {The Correction of a Two Sample Sequential T-Test Developed by {{Hajnal}}},
  author = {Skovlund, Eva},
  year = {1988},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {6},
  number = {2},
  pages = {165--175},
  issn = {01679473},
  doi = {10.1016/0167-9473(88)90047-3},
  urldate = {2021-09-23},
  abstract = {The two-sample sequential t-test developed by Hajnal is one of the few existing two-sample sequential tests for a continuous response variable which may be useful for clinical trials. Simulation investigations of this test show that its boundaries do not give exactly the stated significance level and power. Other simulation experiments are the basis for empirical correction of these boundaries.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EL3RK7A6\\Skovlund - 1988 - The correction of a two sample sequential t-test d.pdf}
}

@article{smith,
  title = {Factorial {{ANOVA}} with Unbalanced Data: {{A}} Fresh Look at the Types of Sums of Squares},
  author = {Smith, Carrie E and Cribbie, Robert},
  pages = {20},
  abstract = {In this paper we endeavour to provide a largely non-technical description of the issues surrounding unbalanced factorial ANOVA and review the arguments made for and against the use of Type I, Type II and Type III sums of squares. Though the issue of which is the `best' approach has been debated in the literature for decades, to date confusion remains around how the procedures differ and which is most appropriate. We ultimately recommend use of the Type II sums of squares for analysis of main effects because when no interaction is present it tests meaningful hypotheses and is the most statistically powerful alternative.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HBU2INXM\\Smith und Cribbie - Factorial ANOVA with unbalanced data A fresh look.pdf}
}

@article{smithson2001,
  title = {Correct {{Confidence Intervals}} for {{Various Regression Effect Sizes}} and {{Parameters}}: {{The Importance}} of {{Noncentral Distributions}} in {{Computing Intervals}}},
  shorttitle = {Correct {{Confidence Intervals}} for {{Various Regression Effect Sizes}} and {{Parameters}}},
  author = {Smithson, Michael},
  year = {2001},
  month = aug,
  journal = {Educational and Psychological Measurement},
  volume = {61},
  number = {4},
  pages = {605--632},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/00131640121971392},
  urldate = {2022-08-12},
  abstract = {The advantages that confidence intervals have over null-hypothesis significance testing have been presented on many occasions to researchers in psychology. This article provides a practical introduction to methods of constructing confidence intervals for multiple and partial R               2               and related parameters in multiple regression models based on ``noncentral'' F and {$\chi$}               2               distributions. Until recently, these techniques have not been widely available due to their neglect in popular statistical textbooks and software. These difficulties are addressed here via freely available SPSS scripts and software and illustrations of their use. The article concludes with discussions of implications for the interpretation of findings in terms of noncentral confidence intervals, alternative measures of effect size, the relationship between noncentral confidence intervals and power analysis, and the design of studies.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\X58UTV3D\\Smithson - 2001 - Correct Confidence Intervals for Various Regressio.pdf}
}

@article{stallard2020,
  title = {Comparison of {{Bayesian}} and Frequentist Group-Sequential Clinical Trial Designs},
  author = {Stallard, Nigel and Todd, Susan and Ryan, Elizabeth G. and Gates, Simon},
  year = {2020},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {4},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0892-8},
  urldate = {2021-09-23},
  abstract = {Background: There is a growing interest in the use of Bayesian adaptive designs in late-phase clinical trials. This includes the use of stopping rules based on Bayesian analyses in which the frequentist type I error rate is controlled as in frequentist group-sequential designs. Methods: This paper presents a practical comparison of Bayesian and frequentist group-sequential tests. Focussing on the setting in which data can be summarised by normally distributed test statistics, we evaluate and compare boundary values and operating characteristics. Results: Although Bayesian and frequentist group-sequential approaches are based on fundamentally different paradigms, in a single arm trial or two-arm comparative trial with a prior distribution specified for the treatment difference, Bayesian and frequentist group-sequential tests can have identical stopping rules if particular critical values with which the posterior probability is compared or particular spending function values are chosen. If the Bayesian critical values at different looks are restricted to be equal, O'Brien and Fleming's design corresponds to a Bayesian design with an exceptionally informative negative prior, Pocock's design to a Bayesian design with a non-informative prior and frequentist designs with a linear alpha spending function are very similar to Bayesian designs with slightly informative priors. This contrasts with the setting of a comparative trial with independent prior distributions specified for treatment effects in different groups. In this case Bayesian and frequentist group-sequential tests cannot have the same stopping rule as the Bayesian stopping rule depends on the observed means in the two groups and not just on their difference. In this setting the Bayesian test can only be guaranteed to control the type I error for a specified range of values of the control group treatment effect. Conclusions: Comparison of frequentist and Bayesian designs can encourage careful thought about design parameters and help to ensure appropriate design choices are made.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\B7ZNC9ZT\\Stallard et al. - 2020 - Comparison of Bayesian and frequentist group-seque.pdf}
}

@article{stefan2019,
  title = {A Tutorial on {{Bayes Factor Design Analysis}} Using an Informed Prior},
  author = {Stefan, Angelika M. and Gronau, Quentin F. and Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan},
  year = {2019},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {3},
  pages = {1042--1058},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01189-8},
  urldate = {2021-09-23},
  abstract = {Well-designed experiments are likely to yield compelling evidence with efficient sample sizes. Bayes Factor Design Analysis (BFDA) is a recently developed methodology that allows researchers to balance the informativeness and efficiency of their experiment (Scho\textasciidieresis nbrodt \& Wagenmakers, Psychonomic Bulletin \& Review, 25(1), 128\textendash 142 2018). With BFDA, researchers can control the rate of misleading evidence but, in addition, they can plan for a target strength of evidence. BFDA can be applied to fixed-N and sequential designs. In this tutorial paper, we provide an introduction to BFDA and analyze how the use of informed prior distributions affects the results of the BFDA. We also present a user-friendly web-based BFDA application that allows researchers to conduct BFDAs with ease. Two practical examples highlight how researchers can use a BFDA to plan for informative and efficient research designs.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2RA7L4FB\\Stefan et al. - 2019 - A tutorial on Bayes Factor Design Analysis using a.pdf}
}

@article{stefan2022,
  title = {A {{Two-Stage Bayesian Sequential Assessment}} of {{Exploratory Hypotheses}}},
  author = {Stefan, Angelika M. and Lengersdorff, Lukas L. and Wagenmakers, Eric-Jan},
  year = {2022},
  month = nov,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {40350},
  issn = {2474-7394},
  doi = {10.1525/collabra.40350},
  urldate = {2022-11-28},
  abstract = {Separating confirmatory and exploratory analyses is vital for ensuring the credibility of research results. Here, we present a two-stage Bayesian sequential procedure that combines a maximum of exploratory freedom in the first stage with a strictly confirmatory regimen in the second stage. It allows for flexible sampling schemes and a statistically coherent carry-over of information from the exploratory to the confirmatory stage. We believe that this procedure will facilitate preregistration as well as the formulation of precise hypotheses in the field of psychology and can be integrated elegantly into the registered report publishing framework. We demonstrate the methodology with a simulated application example from the field of social neuroscience.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\H7ELKMED\\Stefan et al. - 2022 - A Two-Stage Bayesian Sequential Assessment of Expl.pdf;C\:\\Users\\Admin\\Zotero\\storage\\PC42CWNT\\A-Two-Stage-Bayesian-Sequential-Assessment-of.html}
}

@misc{stefan2022a,
  title = {Interim {{Design Analysis Using Bayes Factor Forecasts}}},
  author = {Stefan, Angelika and Gronau, Quentin Frederik and Wagenmakers, Eric-Jan},
  year = {2022},
  month = jul,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/9sazk},
  urldate = {2022-07-22},
  abstract = {A fundamental part of experimental design is to determine the sample size of a study. However, sparse information about population parameters and effect sizes before data collection renders effective sample size planning challenging. Specifically, sparse information may lead research designs to be based on inaccurate a-priori assumptions, causing studies to use resources inefficiently or to produce inconclusive results. Despite its deleterious impact on sample size planning, many prominent methods for experimental design fail to adequately address the challenge of sparse a-priori information. Here we propose a Bayesian Monte Carlo methodology for interim design analyses that allows researchers to analyze and adapt their sampling plans throughout the course of a study. At any point in time, the methodology uses the best available knowledge about parameters to make projections about expected evidence trajectories. Two simulated application examples demonstrate how interim design analyses can be integrated into common designs to inform sampling plans on the fly. The proposed methodology addresses the problem of sample size planning with sparse a-priori information and yields research designs that are efficient, informative, and flexible.},
  langid = {american},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\37LLQYEF\\Stefan et al. - 2022 - Interim Design Analysis Using Bayes Factor Forecas.pdf}
}

@techreport{stefan2022b,
  type = {Preprint},
  title = {Big {{Little Lies}}: {{A Compendium}} and {{Simulation}} of p-{{Hacking Strategies}}},
  shorttitle = {Big {{Little Lies}}},
  author = {Stefan, Angelika and Sch{\"o}nbrodt, Felix D.},
  year = {2022},
  month = mar,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/xy2dk},
  urldate = {2022-05-17},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of twelve p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7AHTLN88\\Stefan und Schönbrodt - 2022 - Big Little Lies A Compendium and Simulation of p-.pdf}
}

@article{stefan2022c,
  title = {Efficiency in Sequential Testing: {{Comparing}} the Sequential Probability Ratio Test and the Sequential {{Bayes}} Factor Test},
  shorttitle = {Efficiency in Sequential Testing},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D. and Evans, Nathan J. and Wagenmakers, Eric-Jan},
  year = {2022},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {54},
  number = {6},
  pages = {3100--3117},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01754-8},
  urldate = {2023-02-22},
  abstract = {In a sequential hypothesis test, the analyst checks at multiple steps during data collection whether sufficient evidence has accrued to make a decision about the tested hypotheses. As soon as sufficient information has been obtained, data collection is terminated. Here, we compare two sequential hypothesis testing procedures that have recently been proposed for use in psychological research: Sequential Probability Ratio Test (SPRT; Psychological Methods, 25(2), 206\textendash 226, 2020) and the Sequential Bayes Factor Test (SBFT; Psychological Methods, 22(2), 322\textendash 339, 2017). We show that although the two methods have different philosophical roots, they share many similarities and can even be mathematically regarded as two instances of an overarching hypothesis testing framework. We demonstrate that the two methods use the same mechanisms for evidence monitoring and error control, and that differences in efficiency between the methods depend on the exact specification of the statistical models involved, as well as on the population truth. Our simulations indicate that when deciding on a sequential design within a unified sequential testing framework, researchers need to balance the needs of test efficiency, robustness against model misspecification, and appropriate uncertainty quantification. We provide guidance for navigating these design decisions based on individual preferences and simulation-based design analyses.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QGV2KEE8\\Stefan et al. - 2022 - Efficiency in sequential testing Comparing the se.pdf}
}

@article{steiger2004,
  title = {Beyond the {{F}} Test: {{Effect}} Size Confidence Intervals and Tests of Close Fit in the Analysis of Variance and Contrast Analysis.},
  shorttitle = {Beyond the f Test},
  author = {Steiger, James H.},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {164--182},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.164},
  urldate = {2022-08-12},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\F522GWKT\\Steiger - 2004 - Beyond the F Test Effect Size Confidence Interval.pdf}
}

@article{steinhilber2023,
  title = {Sprtt: {{Sequential}} Probability Ratio Test Toolbox {\emph{(}}{{{\emph{Version}}}}{\emph{ 0.2.0) [}}{{{\emph{R Package}}}}{\emph{]}}},
  author = {Steinhilber, Meike and Schnuerch, Martin and Schubert, Anna-Lena},
  year = {2023},
  doi = {https://CRAN.R-project.org/package=sprtt},
  keywords = {1 paper,software}
}

@article{steinhilber2023a,
  title = {Sequential One-Way {{ANOVA}}: {{Increasing}} Efficiency in Psychological Hypothesis Testing Using a Variant of Sequential Probability Ratio Tests},
  author = {Steinhilber, Meike and Schnuerch, Martin and Schubert, Anna-Lena},
  year = {2023},
  journal = {PsyArXiv},
  doi = {10.31234/osf.io/m64ne}
}

@article{stroebe2014,
  title = {The {{Alleged Crisis}} and the {{Illusion}} of {{Exact Replication}}},
  author = {Stroebe, Wolfgang and Strack, Fritz},
  year = {2014},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {1},
  pages = {59--71},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691613514450},
  urldate = {2021-09-23},
  abstract = {There has been increasing criticism of the way psychologists conduct and analyze studies. These critiques as well as failures to replicate several high-profile studies have been used as justification to proclaim a ``replication crisis'' in psychology. Psychologists are encouraged to conduct more ``exact'' replications of published studies to assess the reproducibility of psychological research. This article argues that the alleged ``crisis of replicability'' is primarily due to an epistemological misunderstanding that emphasizes the phenomenon instead of its underlying mechanisms. As a consequence, a replicated phenomenon may not serve as a rigorous test of a theoretical hypothesis because identical operationalizations of variables in studies conducted at different times and with different subject populations might test different theoretical constructs. Therefore, we propose that for meaningful replications, attempts at reinstating the original circumstances are not sufficient. Instead, replicators must ascertain that conditions are realized that reflect the theoretical variable(s) manipulated (and/or measured) in the original study.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\B7QNG8D7\\Stroebe und Strack - 2014 - The Alleged Crisis and the Illusion of Exact Repli.pdf}
}

@article{szucs2017,
  title = {When Null Hypothesis Significance Testing Is Unsuitable for Research: {{A}} Reassessment},
  shorttitle = {When Null Hypothesis Significance Testing Is Unsuitable for Research},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  pages = {390},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  urldate = {2023-02-15},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology, and biomedical science in general. We review these shortcomings and suggest that, after sustained negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\H7FIY3GW\\Szucs und Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{szucs2017a,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = mar,
  journal = {PLOS Biology},
  volume = {15},
  number = {3},
  pages = {e2000797},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  urldate = {2023-02-15},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64\textendash 1.46) for nominally statistically significant results and D = 0.24 (0.11\textendash 0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7V6UCETH\\Szucs und Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf}
}

@article{takayanagi,
  title = {{{AN EXAMINATION O F GRADUATE STUDENTS}}' {{STATISTICAL JUDGMENTS}}: {{STATISTICAL AND FUZZY SET APPROACHES}} '{{L}}'},
  author = {Takayanagi, Ko and Cliff, Norman},
  pages = {17},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HDUK6VW4\\Takayanagi und Cliff - AN EXAMINATION O F GRADUATE STUDENTS' STATISTICAL .pdf}
}

@article{tantaratana1977,
  title = {Truncated Sequential Probability Ratio Test},
  author = {Tantaratana, Sawasd and Thomas, John B.},
  year = {1977},
  month = jan,
  journal = {Information Sciences},
  volume = {13},
  number = {3},
  pages = {283--300},
  issn = {00200255},
  doi = {10.1016/0020-0255(77)90050-0},
  urldate = {2021-09-23},
  abstract = {It is well known that in the testing of a simple hypothesis H versus a simple alternative K, the sequential probability ratio test (SPRT) has the smallest average sample number (ASN) under H and K. Compared to the corresponding best fixed sample size (FSS) test, the saving in the average number of samples under H or K in the SPRT is significant. However, when the parameter values of the sample distribution lie between those hypothesized under H and K, the ASN for the SPRT can become much larger than the sample six of the corresponding FSS test, especially for small probabilities of error. It is shown here that a properly truncated SPRT can eliminate this undesirable feature. For small probabilities of error, truncating the SPRT at the sample size needed for the corresponding FSS teat serves as a remedy, while the test is essentially unaffected when the samples are distributed according to H or K.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\74IX38LB\\Tantaratana und Thomas - 1977 - Truncated sequential probability ratio test.pdf}
}

@article{templ2015,
  title = {Statistical {{Disclosure Control}} for {{Micro-Data Using}} the {{R Package sdcMicro}}},
  author = {Templ, Matthias and Kowarik, Alexander and Meindl, Bernhard},
  year = {2015},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {67},
  pages = {1--36},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i04},
  urldate = {2021-12-16},
  abstract = {The demand for data from surveys, censuses or registers containing sensible information on people or enterprises has increased significantly over the last years. However, before data can be provided to the public or to researchers, confidentiality has to be respected for any data set possibly containing sensible information about individual units. Confidentiality can be achieved by applying statistical disclosure control (SDC) methods to the data in order to decrease the disclosure risk of data.The R package sdcMicro serves as an easy-to-handle, object-oriented S4 class implementation of SDC methods to evaluate and anonymize confidential micro-data sets. It includes all popular disclosure risk and perturbation methods. The package performs automated recalculation of frequency counts, individual and global risk measures, information loss and data utility statistics after each anonymization step. All methods are highly optimized in terms of computational costs to be able to work with large data sets. Reporting facilities that summarize the anonymization process can also be easily used by practitioners. We describe the package and demonstrate its functionality with a complex household survey test data set that has been distributed by the International Household Survey Network.},
  copyright = {Copyright (c) 2015 Matthias Templ, Alexander Kowarik, Bernhard Meindl},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DJ55VWGJ\\Templ et al. - 2015 - Statistical Disclosure Control for Micro-Data Usin.pdf}
}

@book{templ2017,
  title = {Statistical {{Disclosure Control}} for {{Microdata}}},
  author = {Templ, Matthias},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50272-4},
  urldate = {2021-12-16},
  isbn = {978-3-319-50270-0 978-3-319-50272-4},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DMMP7MTH\\Templ - 2017 - Statistical Disclosure Control for Microdata.pdf}
}

@misc{tendeiro2022,
  title = {Diagnosing the {{Misuse}} of the {{Bayes}} Factor in {{Applied Research}}},
  author = {Tendeiro, Jorge and Kiers, Henk and Hoekstra, Rink and Wong, Tsz Keung and Morey, Richard D.},
  year = {2022},
  month = nov,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/du3fc},
  urldate = {2023-05-16},
  abstract = {Hypothesis testing is often used for inference in the social sciences. In particular, null hypothesis significance testing (NHST) and its p-value are ubiquitous in published research for decades. Much more recently, null hypothesis Bayesian testing (NHBT) and its Bayes factor also started to be more commonplace in applied research. Following preliminary work by Wong and colleagues, we investigated how, and to what extent, researchers misapply the Bayes factor in applied psychological research by means of a literature study. Based on a final sample of 167 papers, our results indicate that, not unlike NHST and the p-value, also the use of NHBT and the Bayes factor shows signs of misconceptions. We pondered over the root causes of the identified problems. We also provided suggestions to improve the current state of affairs. This paper is aimed to assist researchers to draw the best inferences possible while using NHBT and the Bayes factor in applied research.},
  langid = {american},
  keywords = {Quantitative Methods,Social and Behavioral Sciences,Statistical Methods},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\I4GF24VL\\Tendeiro et al. - 2022 - Diagnosing the Misuse of the Bayes factor in Appli.pdf}
}

@book{thompson2006,
  title = {Foundations of Behavioral Statistics: An Insight-Based Approach},
  shorttitle = {Foundations of Behavioral Statistics},
  author = {Thompson, Bruce},
  year = {2006},
  publisher = {{Guilford Press}},
  address = {{New York}},
  isbn = {978-1-59385-285-6},
  langid = {english},
  lccn = {BF39 .T473 2006},
  annotation = {OCLC: ocm62896508},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\63IV4DV7\\Thompson - 2006 - Foundations of behavioral statistics an insight-b.pdf}
}

@article{tomarken1986,
  title = {Comparison of {{ANOVA}} Alternatives under Variance Heterogeneity and Specific Noncentrality Structures.},
  author = {Tomarken, Andrew J. and Serlin, Ronald C.},
  year = {1986},
  journal = {Psychological Bulletin},
  volume = {99},
  number = {1},
  pages = {90--99},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.99.1.90},
  urldate = {2022-08-09},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XX7NS9LW\\Tomarken und Serlin - 1986 - Comparison of ANOVA alternatives under variance he.pdf}
}

@article{tomczak2014,
  title = {The Need to Report Effect Size Estimates Revisited. {{An}} Overview of Some Recommended Measures of Effect Size},
  author = {Tomczak, Maciej and Tomczak, Ewa},
  year = {2014},
  volume = {1},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KDAX5DSR\\Tomczak und Tomczak - 2014 - The need to report effect size estimates revisited.pdf}
}

@article{tomko2019,
  title = {Using {{REDCap}} for Ambulatory Assessment: {{Implementation}} in a Clinical Trial for Smoking Cessation to Augment in-Person Data Collection},
  shorttitle = {Using {{REDCap}} for Ambulatory Assessment},
  author = {Tomko, Rachel L. and Gray, Kevin M. and Oppenheimer, Stephanie R. and Wahlquist, Amy E. and McClure, Erin A.},
  year = {2019},
  month = jan,
  journal = {The American Journal of Drug and Alcohol Abuse},
  volume = {45},
  number = {1},
  pages = {26--41},
  issn = {0095-2990, 1097-9891},
  doi = {10.1080/00952990.2018.1437445},
  urldate = {2021-11-04},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JHCEY9E6\\Tomko et al. - 2019 - Using REDCap for ambulatory assessment Implementa.pdf}
}

@article{troncososkidmore2013,
  title = {Bias and Precision of Some Classical {{ANOVA}} Effect Sizes When Assumptions Are Violated},
  author = {Troncoso Skidmore, Susan and Thompson, Bruce},
  year = {2013},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {2},
  pages = {536--546},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0257-2},
  urldate = {2021-11-16},
  abstract = {Previous simulation research has focused on evaluating the impact of analytic assumption violations on statistics related to the F test and associated pCALCULATED values. The present article evaluated the bias of classical estimates of practical significance (i.e., effect size sample estimators \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$, \$\$ \{\textbackslash widehat\{\textbackslash varepsilon\}\^2\} \$\$, and \$\$ \{\textbackslash widehat\{\textbackslash omega\}\^2\} \$\$) in a one-way between-subjects univariate ANOVA when assumptions are violated. The simulation conditions modeled were selected on the basis of prior empirical research. Estimated (1) sampling error bias and (2) precision computed for each of the three effect size estimates for the 5,000 samples drawn for each of the 270 (5 parameter Cohen's d values \texttimes{} 3 group size ratios \texttimes{} 3 population distribution shapes \texttimes{} 3 variance ratios \texttimes{} 2 total ns) conditions were modeled for each of the k = 2, 3, and 4 group analyses. Our results corroborate the limited previous related research and suggest that \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$should not be used as an ANOVA effect size estimator, even though \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$is the only available choice in the menus in most commonly available software.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\F7BXQEDD\\Troncoso Skidmore und Thompson - 2013 - Bias and precision of some classical ANOVA effect .pdf}
}

@article{troncososkidmore2013a,
  title = {Bias and Precision of Some Classical {{ANOVA}} Effect Sizes When Assumptions Are Violated},
  author = {Troncoso Skidmore, Susan and Thompson, Bruce},
  year = {2013},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {2},
  pages = {536--546},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0257-2},
  urldate = {2023-03-02},
  abstract = {Previous simulation research has focused on evaluating the impact of analytic assumption violations on statistics related to the F test and associated pCALCULATED values. The present article evaluated the bias of classical estimates of practical significance (i.e., effect size sample estimators \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$, \$\$ \{\textbackslash widehat\{\textbackslash varepsilon\}\^2\} \$\$, and \$\$ \{\textbackslash widehat\{\textbackslash omega\}\^2\} \$\$) in a one-way between-subjects univariate ANOVA when assumptions are violated. The simulation conditions modeled were selected on the basis of prior empirical research. Estimated (1) sampling error bias and (2) precision computed for each of the three effect size estimates for the 5,000 samples drawn for each of the 270 (5 parameter Cohen's d values \texttimes{} 3 group size ratios \texttimes{} 3 population distribution shapes \texttimes{} 3 variance ratios \texttimes{} 2 total ns) conditions were modeled for each of the k = 2, 3, and 4 group analyses. Our results corroborate the limited previous related research and suggest that \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$should not be used as an ANOVA effect size estimator, even though \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$is the only available choice in the menus in most commonly available software.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\S4TSXS5E\\Troncoso Skidmore und Thompson - 2013 - Bias and precision of some classical ANOVA effect .pdf}
}

@article{ulrich2020,
  title = {Questionable Research Practices May Have Little Effect on Replicability},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2020},
  month = sep,
  journal = {eLife},
  volume = {9},
  pages = {e58237},
  issn = {2050-084X},
  doi = {10.7554/eLife.58237},
  urldate = {2022-04-20},
  abstract = {This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is underappreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\GA7NMBYE\\Ulrich und Miller - 2020 - Questionable research practices may have little ef.pdf}
}

@article{ulrich2021,
  title = {Alternative Sequential Methods in Statistical Testing: {{A}} Reply to {{Lakens}} (2021) and {{Erdfelder}} and {{Schnuerch}} (2021).},
  shorttitle = {Alternative Sequential Methods in Statistical Testing},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {507--512},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000420},
  urldate = {2023-03-09},
  abstract = {We recently developed a simple and general sequential sampling method for testing null hypotheses, the independent segments procedure (ISP; Miller \& Ulrich, 2021). In this reply, we discuss the comments of Erdfelder and Schnuerch (2021) and Lakens (2021), who consider alternative methods such as the sequential probability ratio test (SPRT) and the group sequential design (GSD), respectively. We evaluate the pros and cons of these alternatives and conclude that the ISP does have several advantages over these other methods, especially for psychological research. All of these sequential methods can save research resources because smaller sample sizes are required compared to standard nonsequential methods, so it seems appropriate for researchers to choose from a variety of sequential methods based on the practical requirements of their research.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\6AQWZLYL\\Ulrich und Miller - 2021 - Alternative sequential methods in statistical test.pdf}
}

@article{vanravenzwaaij2018,
  title = {A Simple Introduction to {{Markov Chain Monte}}\textendash{{Carlo}} Sampling},
  author = {{van Ravenzwaaij}, Don and Cassey, Pete and Brown, Scott D.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {143--154},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1015-8},
  urldate = {2021-11-19},
  abstract = {Markov Chain Monte\textendash Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ADGPSQ8Z\\van Ravenzwaaij et al. - 2018 - A simple introduction to Markov Chain Monte–Carlo .pdf}
}

@article{viechtbauer2010,
  title = {Conducting {{Meta-Analyses}} in {{R}} with the Metafor {{Package}}},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {36},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v036.i03},
  urldate = {2022-11-24},
  abstract = {The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way.  Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.},
  copyright = {Copyright (c) 2009 Wolfgang Viechtbauer},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PFAH7Z62\\Viechtbauer - 2010 - Conducting Meta-Analyses in R with the metafor Pac.pdf}
}

@article{vos2001,
  title = {A Minimax Procedure in the Context of Sequential Testing Problems in Psychodiagnostics},
  author = {Vos, Hans J.},
  year = {2001},
  month = may,
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {54},
  number = {1},
  pages = {139--159},
  issn = {00071102},
  doi = {10.1348/000711001159474},
  urldate = {2023-01-03},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KGX7ATAZ\\Vos - 2001 - A minimax procedure in the context of sequential t.pdf}
}

@article{vul2009,
  title = {Puzzlingly {{High Correlations}} in {{fMRI Studies}} of {{Emotion}}, {{Personality}}, and {{Social Cognition}}},
  author = {Vul, Edward and Harris, Christine and Winkielman, Piotr and Pashler, Harold},
  year = {2009},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {4},
  number = {3},
  pages = {274--290},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1111/j.1745-6924.2009.01125.x},
  urldate = {2021-09-28},
  abstract = {Functional magnetic resonance imaging (fMRI) studiesofemotion, personality, and social cognition have drawn much attention in recent years, with high-profile studies frequently reporting extremely high (e.g., {$>$}.8) correlations between brain activation and personality measures. We show that these correlations are higher than should be expected given the (evidently limited) reliability of both fMRI and personality measures. The high correlations are all the more puzzling because method sections rarely contain much detail about how the correlations were obtained. We surveyed authors of 55 articles that reported findings of this kind to determine a few details on how these correlations were computed. More than half acknowledged using a strategy that computes separate correlations for individual voxels and reports means of only those voxels exceeding chosen thresholds. We show how this nonindependent analysis inflates correlations while yielding reassuring-looking scattergrams. This analysis technique was used to obtain the vast majority of the implausibly high correlations in our survey sample. In addition, we argue that, in some cases, other analysis problems likely created entirely spurious correlations. We outline how the data from these studies could be reanalyzed with unbiased methods to provide accurate estimates of the correlations in question and urge authors to perform such reanalyses. The underlying problems described here appear to be common in fMRI research of many kinds\textemdash not just in studies of emotion, personality, and social cognition.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KFZ2WRQL\\Vul et al. - 2009 - Puzzlingly High Correlations in fMRI Studies of Em.pdf}
}

@article{wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03194105},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZX28EIZK\\Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf}
}

@article{wagenmakers2010,
  title = {Bayesian Hypothesis Testing for Psychologists: {{A}} Tutorial on the {{Savage}}\textendash{{Dickey}} Method},
  shorttitle = {Bayesian Hypothesis Testing for Psychologists},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
  year = {2010},
  month = may,
  journal = {Cognitive Psychology},
  volume = {60},
  number = {3},
  pages = {158--189},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2009.12.001},
  urldate = {2021-11-19},
  abstract = {In the field of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage\textendash Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method's validity, generality, and flexibility.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\K4JTHRH7\\Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf}
}

@article{wagenmakers2011,
  title = {Why Psychologists Must Change the Way They Analyze Their Data: {{The}} Case of Psi: {{Comment}} on {{Bem}} (2011).},
  shorttitle = {Why Psychologists Must Change the Way They Analyze Their Data},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J.},
  year = {2011},
  month = mar,
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {426--432},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/a0022790},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\G2TJ69J2\\Wagenmakers et al. - 2011 - Why psychologists must change the way they analyze.pdf}
}

@article{wagenmakers2021,
  title = {Seven Steps toward More Transparency in Statistical Practice},
  author = {Wagenmakers, Eric-Jan and Sarafoglou, Alexandra and Aarts, Sil and Albers, Casper and Algermissen, Johannes and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and {van Dongen}, Noah and Hoekstra, Rink and Moreau, David and {van Ravenzwaaij}, Don and Sluga, Alja{\v z} and Stanke, Franziska and Tendeiro, Jorge and Aczel, Balazs},
  year = {2021},
  month = nov,
  journal = {Nature Human Behaviour},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01211-8},
  urldate = {2021-11-15},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ABCYEQWF\\Wagenmakers et al. - 2021 - Seven steps toward more transparency in statistica.pdf}
}

@misc{wagenmakers2022,
  title = {History and {{Nature}} of the {{Jeffreys-Lindley Paradox}}},
  author = {Wagenmakers, Eric-Jan and Ly, Alexander},
  year = {2022},
  month = jul,
  number = {arXiv:2111.10191},
  eprint = {2111.10191},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  urldate = {2022-07-22},
  abstract = {The Jeffreys-Lindley paradox exposes a rift between Bayesian and frequentist hypothesis testing that strikes at the heart of statistical inference. Contrary to what most current literature suggests, the paradox was central to the Bayesian testing methodology developed by Sir Harold Jeffreys in the late 1930s. Jeffreys showed that the evidence against a point-null hypothesis \$\textbackslash mathcal\{H\}\_0\$ scales with \$\textbackslash sqrt\{n\}\$ and repeatedly argued that it would therefore be mistaken to set a threshold for rejecting \$\textbackslash mathcal\{H\}\_0\$ at a constant multiple of the standard error. Here we summarize Jeffreys's early work on the paradox and clarify his reasons for including the \$\textbackslash sqrt\{n\}\$ term. The prior distribution is seen to play a crucial role; by implicitly correcting for selection, small parameter values are identified as relatively surprising under \$\textbackslash mathcal\{H\}\_1\$. We highlight the general nature of the paradox by presenting both a fully frequentist and a fully Bayesian version. We also demonstrate that the paradox does not depend on assigning prior mass to a point hypothesis, as is commonly believed.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LHL6YUSJ\\2111.10191.pdf}
}

@article{wald1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, A.},
  year = {1945},
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  pages = {117--186},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\4XEITD76\\Wald - 1945 - Sequential Tests of Statistical Hypotheses.pdf}
}

@book{wald1947,
  title = {Sequential Analysis},
  author = {Wald, Abraham},
  year = {1947},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-486-61579-0},
  langid = {english},
  lccn = {QA279.7 .W34 1973},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ICGI6BRX\\Wald - 1973 - Sequential analysis.pdf}
}

@article{wald1948,
  title = {Optimum Character of the Sequential Probability Ratio Test},
  author = {Wald, A. and Wolfowitz, J.},
  year = {1948},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {19},
  number = {3},
  pages = {326--339},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177730197},
  urldate = {2021-09-23},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2LCRRDVP\\Wald und Wolfowitz - 1948 - Optimum Character of the Sequential Probability Ra.pdf}
}

@article{wasserstein2019,
  title = {Moving to a {{World Beyond}} `` {\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2023-04-12},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\3AHUEAW4\\Wasserstein et al. - 2019 - Moving to a World Beyond “ p  0.05”.pdf}
}

@book{wassmer2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  year = {2016},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-32562-0},
  urldate = {2023-04-13},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\8BUFY3JE\\Wassmer und Brannath - 2016 - Group Sequential and Confirmatory Adaptive Designs.pdf}
}

@article{welch2022,
  title = {On the {{Comparison}} of {{Several Mean Values}}: {{An Alternative Approach}}},
  author = {Welch, B L},
  year = {2022},
  pages = {8},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ANY6K7AA\\Welch - 2022 - On the Comparison of Several Mean Values An Alter.pdf}
}

@book{wetherill1986,
  title = {Sequential Methods in Statistics},
  author = {Wetherill, G. Barrie and Glazebrook, Kevin D.},
  year = {1986},
  series = {Monographs on Statistics and Applied Probability},
  edition = {3rd ed},
  publisher = {{Chapman and Hall}},
  address = {{London ; New York}},
  isbn = {978-0-412-28150-1},
  lccn = {QA279.7 .W47 1986},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\6NJSJDJT\\Wetherill_seqANOVA.pdf.pdf}
}

@article{wetzels2009,
  title = {How to Quantify Support for and against the Null Hypothesis: {{A}} Flexible {{WinBUGS}} Implementation of a Default {{Bayesian}} t Test},
  shorttitle = {How to Quantify Support for and against the Null Hypothesis},
  author = {Wetzels, Ruud and Raaijmakers, Jeroen G. W. and Jakab, Em{\"o}ke and Wagenmakers, Eric-Jan},
  year = {2009},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {4},
  pages = {752--760},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.4.752},
  urldate = {2023-04-17},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9I4G2D5A\\Wetzels et al. - 2009 - How to quantify support for and against the null h.pdf}
}

@article{whitehead,
  title = {On the Bias of Maximum Likelihood Estimation Following a Sequential Test},
  author = {Whitehead, John},
  pages = {9},
  abstract = {The bias of maximum likelihood estimates calculated at the end of a sequential procedure is investigated. For the two sequential designs considered in detail, the sequential probability ratio test and the triangular test, this bias is appreciable. A method of calculating an adjusted estimate with reduced bias is described, and an approximation to the standard error of the new estimate is provided. Examples of the implementation of the method are given, and its advantages and disadvantages relative to alternative approaches are discussed.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\GQ4MT4XM\\Whitehead - On the bias of maximum likelihood estimation follo.pdf}
}

@article{wilcox,
  title = {{{ANOVA}}: {{A Paradigm}} for {{Low Power}} and {{Misleading Measures}} of {{Effect Size}}?},
  author = {Wilcox, Rand R},
  pages = {27},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QZDA6VQK\\Wilcox - ANOVA A Paradigm for Low Power and Misleading Mea.pdf}
}

@article{wilcox1986,
  title = {New Monte Carlo Results on the Robustness of the Anova f, w and f Statistics},
  author = {Wilcox, Rand R. and Char1in, Ventura L. and Thompson, Karen L.},
  year = {1986},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {15},
  number = {4},
  pages = {933--943},
  issn = {0361-0918},
  doi = {10.1080/03610918608812553},
  urldate = {2022-07-22},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DU7LG2HV\\Wilcox et al. - 1986 - New monte carlo results on the robustness of the a.pdf}
}

@article{wilcox1988,
  title = {A New Alternative to the {{ANOVA F}} and New Results on {{James}}'s Second-Order Method},
  author = {Wilcox, Rand R.},
  year = {1988},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {41},
  number = {1},
  pages = {109--117},
  issn = {2044-8317},
  doi = {10.1111/j.2044-8317.1988.tb00890.x},
  urldate = {2022-08-09},
  abstract = {A new procedure is proposed for testing the hypothesis that the means of J independent normal distributions are equal. Extensive simulations indicate that the new procedure always gives the experimenter more control over the probability of a Type I error than does the Brown\textendash Forsythe or Welch adjusted degree of freedom techniques. For equal and reasonably large sample sizes the Welch procedure might be preferred, however, because it can have substantially more power, and it provides adequate though liberal control over Type I errors. However, Welch's procedure cannot always be recommended for reasons described in the paper. For unequal sample sizes both the Welch and Brown\textendash Forsythe procedures are known to be unsatisfactory in certain practical situations, while the new procedure has Type I error probabilities that are much closer to the nominal level. Moreover, the new procedure seems to give conservative results provided the sample sizes in each group are greater than or equal to 10. New results are also reported on James's second order method. Although computationally more tedious, James's procedure is recommended for general use.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2J2HXD3R\\Wilcox - 1988 - A new alternative to the ANOVA F and new results o.pdf;C\:\\Users\\Admin\\Zotero\\storage\\IPT825UI\\j.2044-8317.1988.tb00890.html}
}

@article{woelfle2011,
  title = {Open Science Is a Research Accelerator},
  author = {Woelfle, Michael and Olliaro, Piero and Todd, Matthew H.},
  year = {2011},
  month = oct,
  journal = {Nature Chemistry},
  volume = {3},
  number = {10},
  pages = {745--748},
  issn = {1755-4330, 1755-4349},
  doi = {10.1038/nchem.1149},
  urldate = {2022-04-20},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UAMQDHUG\\Woelfle et al. - 2011 - Open science is a research accelerator.pdf}
}

@article{zhu2017,
  title = {A {{Bayesian}} Sequential Design Using Alpha Spending Function to Control Type {{I}} Error},
  author = {Zhu, Han and Yu, Qingzhao},
  year = {2017},
  month = oct,
  journal = {Statistical Methods in Medical Research},
  volume = {26},
  number = {5},
  pages = {2184--2196},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280215595058},
  urldate = {2021-09-23},
  abstract = {We propose in this article a Bayesian sequential design using alpha spending functions to control the overall type I error in phase III clinical trials. We provide algorithms to calculate critical values, power, and sample sizes for the proposed design. Sensitivity analysis is implemented to check the effects from different prior distributions, and conservative priors are recommended. We compare the power and actual sample sizes of the proposed Bayesian sequential design with different alpha spending functions through simulations. We also compare the power of the proposed method with frequentist sequential design using the same alpha spending function. Simulations show that, at the same sample size, the proposed method provides larger power than the corresponding frequentist sequential design. It also has larger power than traditional Bayesian sequential design which sets equal critical values for all interim analyses. When compared with other alpha spending functions, O'Brien-Fleming alpha spending function has the largest power and is the most conservative in terms that at the same sample size, the null hypothesis is the least likely to be rejected at early stage of clinical trials. And finally, we show that adding a step of stop for futility in the Bayesian sequential design can reduce the overall type I error and reduce the actual sample sizes.},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\U4XIP6S5\\Zhu und Yu - 2017 - A Bayesian sequential design using alpha spending .pdf}
}

@misc{zotero-1180,
  title = {Cohen1992.Pdf},
  urldate = {2023-04-12},
  howpublished = {https://www2.psych.ubc.ca/\textasciitilde schaller/528Readings/Cohen1992.pdf}
}

@article{zotero-1283,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  doi = {10.1037/0033-2909.86.3.638},
  urldate = {2023-05-02},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JSLWUMQB\\APA PsycNet Record Access.pdf;C\:\\Users\\Admin\\Zotero\\storage\\BJH6TTJD\\1979-27602-001.html}
}

@misc{zotero-1285,
  title = {{{APA PsycNet Record Access}}},
  journal = {APA PsycNET},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.86.3.638},
  urldate = {2023-05-02},
  abstract = {APA PsycNet Record Access},
  howpublished = {https://psycnet.apa.org/recordAccess/institutional/1979-27602-001?returnUrl=https\%253A\%252F\%252Fpsycnet.apa.org\%252Frecord\%252F1979-27602-001},
  langid = {english},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UST8FD3Q\\1979-27602-001.html}
}

@misc{zotero-244,
  title = {Measurement Error and the Replication Crisis},
  doi = {10.1126/science.aal3618},
  urldate = {2021-09-27},
  howpublished = {https://www.science.org/doi/epdf/10.1126/science.aal3618},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\GMU923ZS\\science.html}
}

@misc{zotero-261,
  title = {Build a {{Chatbot}} with {{R}}},
  journal = {Build a Chatbot with R},
  urldate = {2021-10-04},
  abstract = {An introduction to the Telegram Bot API and the telegram.bot package},
  howpublished = {https://ebeneditos.github.io/telegram.bot/},
  langid = {american},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Q3WNBCA9\\telegram.bot.html}
}

@misc{zotero-409,
  title = {Microsoft {{Teams-Chatdateien}} \textendash{} {{OneDrive}}},
  urldate = {2022-06-07},
  howpublished = {https://jgumainz-my.sharepoint.com/personal/aschub02\_uni-mainz\_de/\_layouts/15/onedrive.aspx?id=\%2Fpersonal\%2Faschub02\%5Funi\%2Dmainz\%5Fde\%2FDocuments\%2FMicrosoft\%20Teams\%2DChatdateien\%2FMordkoff\%20\%282019\%29\%20eta\%20squared\%2Epdf\&parent=\%2Fpersonal\%2Faschub02\%5Funi\%2Dmainz\%5Fde\%2FDocuments\%2FMicrosoft\%20Teams\%2DChatdateien\&ga=1},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SXURDZIN\\onedrive.html}
}

@misc{zotero-514,
  title = {Noncentrality Interval Estimation and the Evaluation of Statistical Models},
  urldate = {2022-08-09},
  howpublished = {http://fwww.statpower.net/Steiger\%20Biblio/Steiger\&Fouladi97.PDF}
}
