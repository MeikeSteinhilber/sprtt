@article{agresti2005,
  title = {Comment: {{Randomized}} Confidence Intervals and the Mid-{{P}} Approach},
  shorttitle = {Comment},
  author = {Agresti, Alan and Gottard, Anna},
  year = {2005},
  month = nov,
  journal = {Statistical Science},
  volume = {20},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/088342305000000403},
  urldate = {2024-01-29},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\PHT5VWFA\Agresti und Gottard - 2005 - Comment Randomized Confidence Intervals and the M.pdf}
}

@article{albers2019,
  title = {The Problem with Unadjusted Multiple and Sequential Statistical Testing},
  author = {Albers, Casper},
  year = {2019},
  month = apr,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1921},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09941-0},
  urldate = {2023-03-22},
  abstract = {In research studies, the need for additional samples to obtain sufficient statistical power has often to be balanced with the experimental costs. One approach to this end is to sequentially collect data until you have sufficient measurements, e.g., when the p-value drops below 0.05. I outline that this approach is common, yet that unadjusted sequential sampling leads to severe statistical issues, such as an inflated rate of false positive findings. As a consequence, the results of such studies are untrustworthy. I identify the statistical methods that can be implemented in order to account for sequential sampling.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\74IQQM5B\Albers - 2019 - The problem with unadjusted multiple and sequentia.pdf}
}

@article{aust2022,
  title = {Papaja: {{Prepare}} American Psychological Association Journal Articles with {{R Markdown}}  {\emph{(}}{{{\emph{Version}}}}{\emph{ 0.1.1) [}}{{{\emph{R Package}}}}{\emph{]}}},
  shorttitle = {R {{Package}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2022},
  doi = {https://CRAN.R-project.org/package=papaja},
  keywords = {1 paper,software}
}

@article{barnard1949,
  title = {Statistical Inference},
  author = {Barnard, G. A.},
  year = {1949},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {11},
  number = {2},
  pages = {115--149},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1949.tb00028.x},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\ADXSQIJF\[No title found].pdf}
}

@article{bassler2010,
  title = {Stopping Randomized Trials Early for Benefit and Estimation of Treatment Effects: {{Systematic}} Review and Meta-Regression Analysis},
  shorttitle = {Stopping Randomized Trials Early for Benefit and Estimation of Treatment Effects},
  author = {Bassler, Dirk and Briel, Matthias and Montori, Victor M. and Lane, Melanie and Glasziou, Paul and Zhou, Qi and {Heels-Ansdell}, Diane and Walter, Stephen D. and Guyatt, Gordon H. and the {STOPIT-2 Study Group}, and},
  year = {2010},
  month = mar,
  journal = {JAMA},
  volume = {303},
  number = {12},
  pages = {1180--1187},
  issn = {0098-7484},
  doi = {10.1001/jama.2010.310},
  urldate = {2023-02-27},
  abstract = {Theory and simulation suggest that randomized controlled trials (RCTs) stopped early for benefit (truncated RCTs) systematically overestimate treatment effects for the outcome that precipitated early stopping.To compare the treatment effect from truncated RCTs with that from meta-analyses of RCTs addressing the same question but not stopped early (nontruncated RCTs) and to explore factors associated with overestimates of effect.Search of MEDLINE, EMBASE, Current Contents, and full-text journal content databases to identify truncated RCTs up to January 2007; search of MEDLINE, Cochrane Database of Systematic Reviews, and Database of Abstracts of Reviews of Effects to identify systematic reviews from which individual RCTs were extracted up to January 2008.Selected studies were RCTs reported as having stopped early for benefit and matching nontruncated RCTs from systematic reviews. Independent reviewers with medical content expertise, working blinded to trial results, judged the eligibility of the nontruncated RCTs based on their similarity to the truncated RCTs.Reviewers with methodological expertise conducted data extraction independently.The analysis included 91 truncated RCTs asking 63 different questions and 424 matching nontruncated RCTs. The pooled ratio of relative risks in truncated RCTs vs matching nontruncated RCTs was 0.71 (95\% confidence interval, 0.65-0.77). This difference was independent of the presence of a statistical stopping rule and the methodological quality of the studies as assessed by allocation concealment and blinding. Large differences in treatment effect size between truncated and nontruncated RCTs (ratio of relative risks \&lt;0.75) occurred with truncated RCTs having fewer than 500 events. In 39 of the 63 questions (62\%), the pooled effects of the nontruncated RCTs failed to demonstrate significant benefit.Truncated RCTs were associated with greater effect sizes than RCTs not stopped early. This difference was independent of the presence of statistical stopping rules and was greatest in smaller studies.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\Q5YAVVIG\\Bassler et al. - 2010 - Stopping Randomized Trials Early for Benefit and E.pdf;C\:\\Users\\Admin\\Zotero\\storage\\TT27LZRD\\185591.html}
}

@article{bell2019,
  title = {Fixed and Random Effects Models: Making an Informed Choice},
  shorttitle = {Fixed and Random Effects Models},
  author = {Bell, Andrew and Fairbrother, Malcolm and Jones, Kelvyn},
  year = {2019},
  month = mar,
  journal = {Quality \& Quantity},
  volume = {53},
  number = {2},
  pages = {1051--1074},
  issn = {1573-7845},
  doi = {10.1007/s11135-018-0802-x},
  urldate = {2024-02-12},
  abstract = {This paper assesses the options available to researchers analysing multilevel (including longitudinal) data, with the aim of supporting good methodological decision-making. Given the confusion in the literature about the key properties of fixed and random effects (FE and RE) models, we present these models' capabilities and limitations. We also discuss the within-between RE model, sometimes misleadingly labelled a `hybrid' model, showing that it is the most general of the three, with all the strengths of the other two. As such, and because it allows for important extensions\textemdash notably random slopes\textemdash we argue it should be used (as a starting point at least) in all multilevel analyses. We develop the argument through simulations, evaluating how these models cope with some likely mis-specifications. These simulations reveal that (1) failing to include random slopes can generate anti-conservative standard errors, and (2) assuming random intercepts are Normally distributed, when they are not, introduces only modest biases. These results strengthen the case for the use of, and need for, these models.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\7R4L857G\Bell et al. - 2019 - Fixed and random effects models making an informe.pdf}
}

@article{bhattacharjee2015,
  title = {Confidence Interval Estimation Following {{SPRT}} in a Normal Distribution with Equal Mean and Variance},
  author = {Bhattacharjee, Debanjan and Mukhopadhyay, Nitis},
  year = {2015},
  month = oct,
  journal = {Sequential Analysis},
  volume = {34},
  number = {4},
  pages = {504--531},
  issn = {0747-4946, 1532-4176},
  doi = {10.1080/07474946.2015.1099948},
  urldate = {2023-03-20},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\3B7YWD6Q\Bhattacharjee und Mukhopadhyay - 2015 - Confidence Interval Estimation Following SPRT in a.pdf}
}

@article{blanca2018,
  title = {Effect of Variance Ratio on {{ANOVA}} Robustness: {{Might}} 1.5 Be the Limit?},
  shorttitle = {Effect of Variance Ratio on {{ANOVA}} Robustness},
  author = {Blanca, Mar{\'i}a J. and Alarc{\'o}n, Rafael and Arnau, Jaume and Bono, Roser and Bendayan, Rebecca},
  year = {2018},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {50},
  number = {3},
  pages = {937--962},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0918-2},
  urldate = {2022-07-22},
  abstract = {Inconsistencies in the research findings on F-test robustness to variance heterogeneity could be related to the lack of a standard criterion to assess robustness or to the different measures used to quantify heterogeneity. In the present paper we use Monte Carlo simulation to systematically examine the Type I error rate of F-test under heterogeneity. One-way, balanced, and unbalanced designs with monotonic patterns of variance were considered. Variance ratio (VR) was used as a measure of heterogeneity (1.5, 1.6, 1.7, 1.8, 2, 3, 5, and 9), the coefficient of sample size variation as a measure of inequality between group sizes (0.16, 0.33, and 0.50), and the correlation between variance and group size as an indicator of the pairing between them (1, .50, 0, -.50, and -1). Overall, the results suggest that in terms of Type I error a VR above 1.5 may be established as a rule of thumb for considering a potential threat to F-test robustness under heterogeneity with unequal sample sizes.},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C:\Users\Admin\Zotero\storage\AXYQW4TR\Blanca et al. - 2018 - Effect of variance ratio on ANOVA robustness Migh.pdf}
}

@article{button2013,
  title = {Power Failure: {{Why}} Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2023-02-15},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\HT3743MN\Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{choodari-oskooei2023,
  title = {Point Estimation for Adaptive Trial Designs {{II}}: {{Practical}} Considerations and Guidance},
  shorttitle = {Point Estimation for Adaptive Trial Designs {{II}}},
  author = {{Choodari-Oskooei}, Babak and Robertson, David S. and Dimairo, Munya and Flight, Laura and Pallmann, Philip and Jaki, Thomas},
  year = {2023},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {42},
  number = {14},
  pages = {2496--2520},
  publisher = {{John Wiley and Sons}},
  issn = {1097-0258},
  urldate = {2024-02-02},
  abstract = {In adaptive clinical trials, the conventional end-of-trial point estimate of a treatment effect is prone to bias, that is, a systematic tendency to deviate from its true value. As stated in recent FDA guidance on adaptive designs, it is desirable to report estimates of treatment effects that reduce or remove this bias. However, it may be unclear which of the available estimators are preferable, and their use remains rare in practice. This article is the second in a two-part series that studies the issue of bias in point estimation for adaptive trials. Part I provided a methodological review of approaches to remove or reduce the potential bias in point estimation for adaptive designs. In part II, we discuss how bias can affect standard estimators and assess the negative impact this can have. We review current practice for reporting point estimates and illustrate the computation of different estimators using a real adaptive trial example (including code), which we use as a basis for a simulation study. We show that while on average the values of these estimators can be similar, for a particular trial realization they can give noticeably different values for the estimated treatment effect. Finally, we propose guidelines for researchers around the choice of estimators and the reporting of estimates following an adaptive design. The issue of bias should be considered throughout the whole lifecycle of an adaptive design, with the estimation strategy prespecified in the statistical analysis plan. When available, unbiased or bias-reduced estimates are to be preferred.},
  copyright = {open},
  langid = {english},
  keywords = {1 paper,current reading,sequential estimation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\U3D9PADR\\Choodari-Oskooei et al. - 2023 - Point estimation for adaptive trial designs II Pr.pdf;C\:\\Users\\Admin\\Zotero\\storage\\VMLRHYA4\\10167874.html}
}

@article{coburger2001,
  title = {Conditional Point Estimation in Adaptive Group Sequential Test Designs},
  author = {Coburger, Silke and Wassmer, Gernot},
  year = {2001},
  journal = {Biometrical Journal},
  volume = {43},
  number = {7},
  pages = {821--833},
  issn = {1521-4036},
  doi = {10.1002/1521-4036(200111)43:7<821::AID-BIMJ821>3.0.CO;2-F},
  urldate = {2024-01-31},
  abstract = {It is well known that point estimates in group sequential designs are biased. This also applies to adaptive designs that enable, e.g., data driven reassessments of group sample sizes. For triangular designs, Whitehead (1986) (Biometrika 73, 573\textendash 581) proposed a bias adjusted estimate. But this estimate is not feasible in adaptive designs although it is in group sequential designs. Furthermore, there is a waste of information because it does not use the information at which stage the trial was stopped. We present a modification which does use this information and which is applicable to adaptive designs. The modified estimate achieves an improvement in group sequential designs and shows similar results in adaptive designs.},
  copyright = {\textcopyright{} 2001 WILEY-VCH Verlag Berlin GmbH, Fed. Rep. of Germany},
  langid = {english},
  keywords = {1 paper,current reading,sequential estimation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JJ4AHISS\\Coburger und Wassmer - 2001 - Conditional Point Estimation in Adaptive Group Seq.pdf;C\:\\Users\\Admin\\Zotero\\storage\\CSTSWJ9H\\1521-4036(200111)437821AID-BIMJ8213.0.html}
}

@article{coburger2003,
  title = {Sample {{Size Reassessment}} in {{Adaptive Clinical Trials Using}} a {{Bias Corrected Estimate}}},
  author = {Coburger, Silke and Wassmer, Gernot},
  year = {2003},
  journal = {Biometrical Journal},
  volume = {45},
  number = {7},
  pages = {812--825},
  issn = {1521-4036},
  doi = {10.1002/bimj.200390051},
  urldate = {2024-02-05},
  abstract = {Point estimation in group sequential and adaptive trials is an important issue in analysing a clinical trial. Most literature in this area is only concerned with estimation after completion of a trial. Since adaptive designs allow reassessment of sample size during the trial, reliable point estimation of the true effect when continuing the trial is additionally needed. We present a bias adjusted estimator which allows a more exact sample size determination based on the conditional power principle than the naive sample mean does.},
  langid = {english},
  keywords = {1 paper,sequential estimation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\RF42MCJD\\Coburger und Wassmer - 2003 - Sample Size Reassessment in Adaptive Clinical Tria.pdf;C\:\\Users\\Admin\\Zotero\\storage\\Z4K8AYNX\\bimj.html}
}

@article{cohen1992,
  title = {A Power Primer},
  shorttitle = {Quantitative Methods in Psychology},
  author = {Cohen, J.},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {112},
  number = {1},
  pages = {1155--1159},
  doi = {10.1037/0033-2909.112.1.155},
  urldate = {2023-04-12},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EXYVZQ4B\\Cohen1992.pdf;C\:\\Users\\Admin\\Zotero\\storage\\6GJMFQS5\\1571417125554624128.html}
}

@book{cohen2009,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {2009},
  edition = {2. ed., reprint},
  publisher = {{Psychology Press}},
  address = {{New York, NY}},
  isbn = {978-0-8058-0283-2},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\F5YNQANA\Cohen - 2009 - Statistical power analysis for the behavioral scie.pdf}
}

@article{cornfield1966,
  title = {Sequential Trials, Sequential Analysis and the Likelihood Principle},
  author = {Cornfield, Jerome},
  year = {1966},
  journal = {The American Statistician},
  volume = {20},
  number = {2},
  pages = {18--23},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\XE8GUFFC\2024 - Sequential Trials, Sequential Analysis and the Lik.pdf}
}

@article{cox1952,
  title = {Sequential Tests for Composite Hypotheses},
  author = {Cox, D R},
  year = {1952},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {48},
  number = {2},
  pages = {290--299},
  doi = {10.1017/S030500410002764X},
  abstract = {A method is given for obtaining sequential tests in the presence of nuisance parameters. It is assumed that a jointly sufficient set of estimators exists for the unknown parameters. A number of special tests are described and their properties discussed.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\IGMXPJIM\Cox - Sequential tests for composite hypotheses.pdf}
}

@article{derosario2020,
  title = {Pwr: {{Basic}} Functions for Power Analysis  {\emph{(}}{{{\emph{Version}}}}{\emph{ 1.3.0) [}}{{{\emph{R Package}}}}{\emph{]}}},
  author = {De Rosario, Helios and Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert},
  year = {2020},
  doi = {https://CRAN.R-project.org/package=pwr},
  keywords = {1 paper,software}
}

@article{emerson1990,
  title = {Parameter Estimation Following Group Sequential Hypothesis Testing},
  author = {EMERSON, SCOTT S. and FLEMING, THOMAS R.},
  year = {1990},
  month = dec,
  journal = {Biometrika},
  volume = {77},
  number = {4},
  pages = {875--892},
  issn = {0006-3444},
  doi = {10.1093/biomet/77.4.875},
  urldate = {2023-02-27},
  abstract = {Parameter estimation techniques which fail to adjust for the interim analyses of group sequential test designs will introduce bias in much the same way that the repeated use of single sample hypothesis testing causes inflation of the type one statistical error rate. Methods based on the duality of hypothesis testing and interval estimation require definition of an ordering for the outcome space for the test statistic. In this paper, estimation following a group sequential hypothesis test for the mean of a normal distribution with known variance is investigated. A proposed ordering of the sample space based on the maximum likelihood estimate of the mean is found to result in estimates which compare favourably with estimates computed from orderings investigated by Tsiatis, Rosner \&amp; Mehta (1984) and Chang \&amp; O'Brien (1986) for a variety of group sequential designs. The proposed ordering is then adapted for use when the sizes of groups accrued between analyses is random.},
  keywords = {1 paper,sequential estimation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\83XHFR9L\\Emerson und Fleming - Parameter estimation following group sequential hy.pdf;C\:\\Users\\Admin\\Zotero\\storage\\AL8PES8Z\\EMERSON und FLEMING - 1990 - Parameter estimation following group sequential hy.pdf;C\:\\Users\\Admin\\Zotero\\storage\\TCLTDPPD\\291010.html}
}

@article{erdfelder1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  year = {1996},
  month = mar,
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {28},
  number = {1},
  pages = {1--11},
  issn = {1532-5970},
  doi = {10.3758/BF03203630},
  urldate = {2023-04-12},
  abstract = {GPOWER is a completely interactive, menu-driven program for IBM-compatible and Apple Macintosh personal computers. It performs high-precision statistical power analyses for the most common statistical tests in behavioral research, that is,t tests,F tests, and{$\chi$}2 tests. GPOWER computes (1) power values for given sample sizes, effect sizes and{$\alpha$} levels (post hoc power analyses); (2) sample sizes for given effect sizes,{$\alpha$} levels, and power values (a priori power analyses); and (3){$\alpha$} and{$\beta$} values for given sample sizes, effect sizes, and{$\beta$}/{$\alpha$} ratios (compromise power analyses). The program may be used to display graphically the relation between any two of the relevant variables, and it offers the opportunity to compute the effect size measures from basic parameters defining the alternative hypothesis. This article delineates reasons for the development of GPOWER and describes the program's capabilities and handling.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\2IN5YR87\Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf}
}

@article{erdfelder2021,
  title = {On the Efficiency of the Independent Segments Procedure: {{A}} Direct Comparison with Sequential Probability Ratio Tests.},
  shorttitle = {On the Efficiency of the Independent Segments Procedure},
  author = {Erdfelder, Edgar and Schnuerch, Martin},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {501--506},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000404},
  urldate = {2023-03-09},
  abstract = {In this comment, we report a simulation study that assesses error rates and average sample sizes required to reach a statistical decision for two sequential procedures, the sequential probability ratio test (SPRT) originally proposed by Wald (1947) and the independent segments procedure (ISP) recently suggested by Miller and Ulrich (2020). Following Miller and Ulrich (2020), we use sequential one-tailed t tests as examples. In line with the optimal efficiency properties of the SPRT already proven by Wald and Wolfowitz (1948), the SPRT outperformed the ISP in terms of efficiency without compromising error probability control. The efficiency gain in terms of sample size reduction achieved with the SPRT t test relative to the ISP may be as high as 25\%. We thus recommend the SPRT as a default sequential testing procedure especially for detecting small or medium hypothesized effect sizes under H1 whenever a priori knowledge of the maximum sample size is not crucial. If a priori control of the maximum sample size is mandatory, however, the ISP is a very useful addition to the sequential testing literature.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\ANAZHWD4\Erdfelder und Schnuerch - 2021 - On the efficiency of the independent segments proc.pdf}
}

@article{etz2018,
  title = {Introduction to the Concept of Likelihood and Its Applications},
  author = {Etz, Alexander},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {60--69},
  doi = {10.1177/2515245917744314},
  abstract = {This Tutorial explains the statistical concept known as likelihood and discusses how it underlies common frequentist and Bayesian statistical methods. The article is suitable for researchers interested in understanding the basis of their statistical tools and is also intended as a resource for teachers to use in their classrooms to introduce the topic to students at a conceptual level.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\5KYDDP4W\Etz - Introduction to the Concept of Likelihood and Its .pdf}
}

@article{fan2004,
  title = {Conditional Bias of Point Estimates Following a Group Sequential Test},
  author = {Fan, Xiaoyin (Frank) and DeMets, David L. and Lan, K. K. Gordon},
  year = {2004},
  month = dec,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {14},
  number = {2},
  pages = {505--530},
  issn = {1054-3406, 1520-5711},
  doi = {10.1081/BIP-120037195},
  urldate = {2024-01-29},
  langid = {english},
  keywords = {1 paper,current reading,sequential estimation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HDNQLBGN\\Fan et al. - 2004 - Conditional Bias of Point Estimates Following a Gr.pdf;C\:\\Users\\Admin\\Zotero\\storage\\JYUPEIGR\\BIP-120037195.html}
}

@article{fritz2012,
  title = {Effect Size Estimates: {{Current}} Use, Calculations, and Interpretation.},
  shorttitle = {Effect Size Estimates},
  author = {Fritz, Catherine O. and Morris, Peter E. and Richler, Jennifer J.},
  year = {2012},
  journal = {Journal of Experimental Psychology: General},
  volume = {141},
  number = {1},
  pages = {2--18},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0024338},
  urldate = {2023-04-13},
  abstract = {The Publication Manual of the American Psychological Association (American Psychological Association, 2001, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial ‚ê©2 was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen's d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\W8LG8DJK\Fritz et al. - 2012 - Effect size estimates Current use, calculations, .pdf}
}

@article{funder2019,
  title = {Evaluating Effect Size in Psychological Research: {{Sense}} and Nonsense},
  shorttitle = {Evaluating Effect Size in Psychological Research},
  author = {Funder, David C. and Ozer, Daniel J.},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245919847202},
  urldate = {2023-03-09},
  abstract = {Effect sizes are underappreciated and often misinterpreted\textemdash the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size ( r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\AKANQJRR\Funder und Ozer - 2019 - Evaluating Effect Size in Psychological Research .pdf}
}

@book{gigerenzer1989,
  title = {The Empire of Chance: {{How}} Probability Changed Science and Everyday Life},
  shorttitle = {The Empire of Chance},
  author = {Gigerenzer, Gerd},
  year = {1989},
  publisher = {{Cambridge University Press}},
  abstract = {This book tells how quantitative ideas of chance have transformed the natural and social sciences as well as everyday life over the past three centuries. A continuous narrative connects the earliest application of probability and statistics in gambling and insurance to the most recent forays into law, medicine, polling, and baseball. Separate chapters explore the theoretical and methodological impact on biology, physics, and psychology. In contrast to the literature on the mathematical development of probability and statistics, this book centers on how these technical innovations recreated our conceptions of nature, mind, and society.},
  googlebooks = {Bw2yKfpvts8C},
  isbn = {978-0-521-39838-1},
  langid = {english},
  keywords = {1 paper}
}

@article{gigerenzer2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {10535357},
  doi = {10.1016/j.socec.2004.09.033},
  urldate = {2021-09-23},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\4M5TCDF2\Gigerenzer - 2004 - Mindless statistics.pdf}
}

@article{gignac2016,
  title = {Effect Size Guidelines for Individual Differences Researchers},
  author = {Gignac, Gilles E. and Szodorai, Eva T.},
  year = {2016},
  month = nov,
  journal = {Personality and Individual Differences},
  volume = {102},
  pages = {74--78},
  issn = {0191-8869},
  doi = {10.1016/j.paid.2016.06.069},
  urldate = {2024-02-02},
  abstract = {Individual differences researchers very commonly report Pearson correlations between their variables of interest. Cohen (1988) provided guidelines for the purposes of interpreting the magnitude of a correlation, as well as estimating power. Specifically, r=0.10, r=0.30, and r=0.50 were recommended to be considered small, medium, and large in magnitude, respectively. However, Cohen's effect size guidelines were based principally upon an essentially qualitative impression, rather than a systematic, quantitative analysis of data. Consequently, the purpose of this investigation was to develop a large sample of previously published meta-analytically derived correlations which would allow for an evaluation of Cohen's guidelines from an empirical perspective. Based on 708 meta-analytically derived correlations, the 25th, 50th, and 75th percentiles corresponded to correlations of 0.11, 0.19, and 0.29, respectively. Based on the results, it is suggested that Cohen's correlation guidelines are too exigent, as {$<$}3\% of correlations in the literature were found to be as large as r=0.50. Consequently, in the absence of any other information, individual differences researchers are recommended to consider correlations of 0.10, 0.20, and 0.30 as relatively small, typical, and relatively large, in the context of a power analysis, as well as the interpretation of statistical results from a normative perspective.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\V6K2UTRN\\Gignac und Szodorai - 2016 - Effect size guidelines for individual differences .pdf;C\:\\Users\\Admin\\Zotero\\storage\\94GMPPMH\\S0191886916308194.html}
}

@article{glass1972,
  title = {Consequences of Failure to Meet Assumptions Underlying the Fixed Effects Analyses of Variance and Covariance},
  author = {Glass, Gene V and Peckham, Percy D and Sanders, James R},
  year = {1972},
  journal = {Review of Educational Research},
  volume = {42},
  number = {3},
  pages = {237--288},
  doi = {10.3102/00346543042003237},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\Y42KJVTW\Glass et al. - Consequences of Failure to Meet Assumptions Underl.pdf}
}

@article{goodman2007,
  title = {Stopping at {{Nothing}}? {{Some Dilemmas}} of {{Data Monitoring}} in {{Clinical Trials}}},
  shorttitle = {Stopping at {{Nothing}}?},
  author = {Goodman, Steven N.},
  year = {2007},
  month = jun,
  journal = {Annals of Internal Medicine},
  volume = {146},
  number = {12},
  pages = {882},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-146-12-200706190-00010},
  urldate = {2023-03-14},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\TIBE8GCZ\Goodman - 2007 - Stopping at Nothing Some Dilemmas of Data Monitor.pdf}
}

@article{hajnal1961,
  title = {A Two-Sample Sequential t-Test},
  author = {Hajnal, J.},
  year = {1961},
  journal = {Biometrika},
  volume = {48},
  number = {1/2},
  eprint = {2333131},
  eprinttype = {jstor},
  pages = {65--75},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2333131},
  urldate = {2023-05-20},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\2MC4MR5T\Hajnal - 1961 - A Two-Sample Sequential t-Test.pdf}
}

@article{lakens2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: {{A}} Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Dani{\"e}l},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  pages = {863},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  urldate = {2022-08-01},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between withinand between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C:\Users\Admin\Zotero\storage\UNR3JYIQ\Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf}
}

@article{lakens2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  month = dec,
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  urldate = {2023-03-21},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\9ZTAXD2W\Lakens - 2014 - Performing high-powered studies efficiently with s.pdf}
}

@article{lakens2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  urldate = {2023-03-09},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\4ZUIFCEK\Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf}
}

@article{lakens2021a,
  title = {Invited Commentary: {{Comparing}} the Independent Segments Procedure with Group Sequential Designs.},
  shorttitle = {Invited Commentary},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {498--500},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000400},
  urldate = {2023-03-09},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\KZXSFMJ2\Lakens - 2021 - Invited commentary Comparing the independent segm.pdf}
}

@techreport{lakens2021b,
  type = {Preprint},
  title = {Group Sequential Designs: {{A}} Tutorial},
  shorttitle = {Group Sequential Designs},
  author = {Lakens, Daniel and Pahlke, Friedrich and Wassmer, Gernot},
  year = {2021},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/x4azm},
  urldate = {2023-03-09},
  abstract = {This tutorial illustrates how to design, analyze, and report group sequential designs. In these designs, groups of observations are collected and repeatedly analyzed, while controlling error rates. Compared to a fixed sample size design, where data is analyzed only once, group sequential designs offer the possibility to stop the study at interim looks at the data either for efficacy or futility. Hence, they provide greater flexibility and are more efficient in the sense that due to early stopping the expected sample size is smaller as compared to the sample size in the design with no interim look. In this tutorial we illustrate how to use the R package 'rpact' and the associated Shiny app to design studies that control the Type I error rate when repeatedly analyzing data, even when neither the number of looks at the data, nor the exact timing of looks at the data, is specified. Specifically for *t*-tests, we illustrate how to perform an a-priori power analysis for group sequential designs, and explain how to stop the data collection for futility by rejecting the presence of an effect of interest based on a beta-spending function. Finally, we discuss how to report adjusted effect size estimates and confidence intervals. The recent availability of accessible software such as 'rpact' makes it possible for psychologists to benefit from the efficiency gains provided by group sequential designs.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\L7C5SQWA\Lakens et al. - 2021 - Group Sequential Designs A Tutorial.pdf}
}

@article{lakens2022,
  title = {Sample Size Justification},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2022-07-04},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  langid = {english},
  keywords = {1 paper}
}

@article{lan1983,
  title = {Discrete Sequential Boundaries for Clinical Trials},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  year = {1983},
  journal = {Biometrika},
  volume = {70},
  number = {3},
  eprint = {2336502},
  eprinttype = {jstor},
  pages = {659--663},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2336502},
  urldate = {2023-04-27},
  abstract = {Pocock (1977), O'Brien \& Fleming (1979) and Slud \& Wei (1982) have proposed different methods to construct discrete sequential boundaries for clinical trials. These methods require that the total number of decision times be specified in advance. In the present paper, we propose a more flexible way to construct discrete sequential boundaries. The method is based on the choice of a function, {$\alpha$}*(t), which characterizes the rate at which the error level {$\alpha$} is spent. The boundary at a decision time is determined by {$\alpha$}*(t), and by past and current decision times, but does not depend on the future decision times or the total number of decision times.},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\VD28IACT\Lan und DeMets - 1983 - Discrete Sequential Boundaries for Clinical Trials.pdf}
}

@article{lang2017,
  title = {Is Intermediately Inspecting Statistical Data Necessarily a Bad Research Practice?},
  author = {Lang, Albert-Georg},
  year = {2017},
  month = may,
  journal = {The Quantitative Methods for Psychology},
  volume = {13},
  number = {2},
  pages = {127--140},
  issn = {2292-1354},
  doi = {10.20982/tqmp.13.2.p127},
  urldate = {2023-04-13},
  abstract = {Intermediately inspecting the statistical data of a running experiment is justifiably referred to as a bad research practice. With only a few intermediate inspections, Type I error rates inflate to a multiple of the previously defined critical alpha. On the other hand, there are research areas where intermediately inspecting data is extremely desirable if not even necessary. For this reason, in medical research, mathematical methods are known as ``group-sequential testing'' which compensate Type I error cumulation by adjusting critical alpha. In the field of psychological research, these methods are widely unknown or at least used very rarely. One reason may be that group-sequential tests focus on test statistics based on the normal distribution, mainly the t-test, while in psychological research often more complex experimental designs are used. The computer program APriot has been developed to enable the user to conduct Monte-Carlo simulations of what happens when intermediately inspecting the data of an ANOVA. The simulations show clearly how bad a research practice intermediately inspecting data (without adjusting alpha) is. Further, it is shown that in many cases adjusted values of alpha can be found by simulations such that the ANOVA can be used together with group-sequential testing similarly as the t-test. A last set of demonstrations shows how the power and the required number of participants of a groupsequential test can be estimated and that group-sequential testing can be favorable from an economic point of view.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\QEGM829U\Lang - 2017 - Is intermediately inspecting statistical data nece.pdf}
}

@article{lee1980,
  title = {A Monte Carlo Study on the Robustness of the Two-Sample Sequential {\emph{t}} Test},
  author = {Lee, Hyunshik and Fung, Karen Yuen},
  year = {1980},
  month = apr,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {10},
  number = {3-4},
  pages = {297--307},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949658008810377},
  urldate = {2022-05-04},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C:\Users\Admin\Zotero\storage\L2M7T7LC\Lee und Fung - 1980 - A monte carlo study on the robustness of the two-s.pdf}
}

@article{lix1996,
  title = {Consequences of Assumption Violations Revisited: {{A}} Quantitative Review of Alternatives to the One-Way Analysis of Variance {{F}} Test},
  author = {Lix, Lisa M and Keselman, Joanne C and Keselman, H J},
  year = {1996},
  journal = {Review of Educational Research},
  volume = {66},
  number = {4},
  pages = {579--619},
  doi = {10.2307/1170654},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C:\Users\Admin\Zotero\storage\KKVX5FW2\Lix et al. - Consequences of Assumption Violations Revisited A.pdf}
}

@article{lovakov2021,
  title = {Empirically Derived Guidelines for Effect Size Interpretation in Social Psychology},
  author = {Lovakov, Andrey and Agadullina, Elena R.},
  year = {2021},
  journal = {European Journal of Social Psychology},
  volume = {51},
  number = {3},
  pages = {485--504},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2752},
  urldate = {2023-04-13},
  abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\UCPK82MY\\Lovakov und Agadullina - 2021 - Empirically derived guidelines for effect size int.pdf;C\:\\Users\\Admin\\Zotero\\storage\\IXCT7XRP\\ejsp.html}
}

@article{lovakov2024,
  title = {Empirically {{Derived Guidelines}} for {{Effect Size Interpretation}} in {{Social Psychology}}},
  author = {Lovakov, Andrey and Agadullina, Elena},
  year = {2024},
  month = feb,
  publisher = {{OSF}},
  doi = {10.31234/osf.io/2epc4},
  urldate = {2024-02-08},
  abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and subdisciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and subdisciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
  langid = {american},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\MGGTBK6E\\Lovakov und Agadullina - 2024 - Empirically Derived Guidelines for Effect Size Int.pdf;C\:\\Users\\Admin\\Zotero\\storage\\4M4VZBZ6\\2epc4.html}
}

@book{maxwell2017,
  title = {Designing Experiments and Analyzing Data: {{A}} Model Comparison Perspective},
  shorttitle = {Designing Experiments and Analyzing Data},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  year = {2017},
  edition = {Third edition},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  isbn = {978-1-138-89228-6},
  langid = {english},
  lccn = {QA279 .M384 2017},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\WXUD2EUZ\Maxwell et al. - 2017 - Designing experiments and analyzing data a model .pdf}
}

@article{miller2021,
  title = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing: {{The}} Independent Segments Procedure},
  shorttitle = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {486--497},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/met0000350},
  urldate = {2023-03-22},
  abstract = {We propose a new sequential hypothesis testing procedure in which data are collected and analyzed in a series of independent segments. As in fixed-sample hypothesis testing and in previous sequential procedures, the overall {$\alpha$} level can be set to any desired value. Like other sequential procedures, the independent segments procedure generally requires smaller samples than fixed-sample procedures\textemdash often approximately 30\% smaller\textemdash to achieve the same {$\alpha$} level and statistical power. Relative to other sequential procedures, the new method has the advantages that it is simpler to use, requires fewer assumptions, and can be used with a wider array of statistical tests. Thus, in some circumstances the independent segments procedure may provide an attractive option for increasing the efficiency of statistical testing. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\XL8RGF75\Miller und Ulrich - 2021 - A simple, general, and efficient method for sequen.pdf}
}

@article{morey2011,
  title = {Bayes Factor Approaches for Testing Interval Null Hypotheses.},
  author = {Morey, Richard D. and Rouder, Jeffrey N.},
  year = {2011},
  month = dec,
  journal = {Psychological Methods},
  volume = {16},
  number = {4},
  pages = {406--419},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0024377},
  urldate = {2023-04-17},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\E7PNQRDK\Morey und Rouder - 2011 - Bayes factor approaches for testing interval null .pdf}
}

@article{neyman1933a,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
  author = {Neyman, J. and Pearson, E. S.},
  year = {1933},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {231},
  pages = {289--337},
  issn = {0264-3952, 2053-9258},
  doi = {10.1098/rsta.1933.0009},
  urldate = {2021-09-23},
  abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a               posteriori               of the possible ``causes" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a ``system'' or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character,               x               , of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and               vice versa               . Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character,               x               , of the observed facts were properly chosen\textemdash were, in fact, a character which he terms ``en quelque sorte remarquable.''},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\HK5B2W3L\1933 - IX. On the problem of the most efficient tests of .pdf}
}

@article{obrien1979,
  title = {A Multiple Testing Procedure for Clinical Trials},
  author = {O'Brien, Peter C. and Fleming, Thomas R.},
  year = {1979},
  journal = {Biometrics},
  volume = {35},
  number = {3},
  eprint = {2530245},
  eprinttype = {jstor},
  pages = {549--556},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2530245},
  urldate = {2023-04-27},
  abstract = {A multiple testing procedure is proposed for comparing two treatments when response to treatment is both dichotomous (i.e., success or failure) and immediate. The proposed test statistic for each test is the usual (Pearson) chi-square statistic based on all data collected to that point. The maximum number (N) of tests and the number (m1 + m2) of observations collected between successive tests is fixed in advance. The overall size of the procedure is shown to be controlled with virtually the same accuracy as the single sample chi-square test based on N(m1 + m2) observations. The power is also found to be virtually the same. However, by affording the opportunity to terminate early when one treatment performs markedly better than the other, the multiple testing procedure may eliminate the ethical dilemmas that often accompany clinical trials.},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\TCLGXCIL\O'Brien und Fleming - 1979 - A Multiple Testing Procedure for Clinical Trials.pdf}
}

@article{perezgonzalez2015,
  title = {Fisher, {{Neyman-Pearson}} or {{NHST}}? {{A}} Tutorial for Teaching Data Testing},
  shorttitle = {Fisher, {{Neyman-Pearson}} or {{NHST}}?},
  author = {Perezgonzalez, Jose D.},
  year = {2015},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00223},
  urldate = {2021-09-23},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\WAD45YAL\Perezgonzalez - 2015 - Fisher, Neyman-Pearson or NHST A tutorial for tea.pdf}
}

@article{pocock1977,
  title = {Group Sequential Methods in the Design and Analysis of Clinical Trials},
  author = {Pocock, Stuart J.},
  year = {1977},
  month = aug,
  journal = {Biometrika},
  volume = {64},
  number = {2},
  pages = {191--199},
  issn = {0006-3444},
  doi = {10.1093/biomet/64.2.191},
  urldate = {2023-04-27},
  abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SDXLUXWW\\POCOCK - 1977 - Group sequential methods in the design and analysi.pdf;C\:\\Users\\Admin\\Zotero\\storage\\XX4732CP\\384776.html}
}

@article{pocock1989,
  title = {Practical Problems in Interim Analyses, with Particular Regard to Estimation},
  author = {Pocock, Stuart J. and Hughes, Michael D.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4, Supplement 1},
  pages = {209--221},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90059-7},
  urldate = {2023-03-14},
  abstract = {This article considers some of the practical problems inherent in interim analyses and stopping rules for randomized clinical trials. Topics covered include group sequential designs, trials with unplanned interim analyses, estimation problems in clinical trials with planned interim analyses, and the balance between individual and collective ethics. Particular attention is paid to the fact that clinical trials that stop early are prone to exaggerate the magnitude of treatment effect. Accordingly, a Bayesian ``shrinkage'' method of analysis is proposed to help quantify the extent to which surprisingly large point and interval estimates of treatment difference in clinical trials that stop early should be moderated.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LD9N8GPJ\\Pocock und Hughes - 1989 - Practical problems in interim analyses, with parti.pdf;C\:\\Users\\Admin\\Zotero\\storage\\FKDGSHGF\\0197245689900597.html}
}

@article{pramanik2021,
  title = {A Modified Sequential Probability Ratio Test},
  author = {Pramanik, Sandipan and Johnson, Valen E. and Bhattacharya, Anirban},
  year = {2021},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {101},
  pages = {102505},
  issn = {00222496},
  doi = {10.1016/j.jmp.2021.102505},
  urldate = {2021-11-04},
  abstract = {We describe a modified sequential probability ratio test that can be used to reduce the average sample size required to perform statistical hypothesis tests at specified levels of significance and power. Examples are provided for z tests, t tests, and tests of binomial success probabilities. A description of a software package to implement the test designs is provided. We compare the sample sizes required in fixed design tests conducted at 5\% significance levels to the average sample sizes required in sequential tests conducted at 0.5\% significance levels, and we find that the two sample sizes are approximately equal.},
  langid = {english},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\96EMIYBI\\Pramanik et al. - 2021 - A modified sequential probability ratio test.pdf;C\:\\Users\\Admin\\Zotero\\storage\\G65W45QX\\S0022249621000109.html}
}

@book{proschan2006,
  title = {Statistical Monitoring of Clinical Trials: {{A}} Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, KK Gordon and Wittes, Janet Turk},
  year = {2006},
  publisher = {{Springer Science \& Business Media}},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\IHIQ5L2B\books.html}
}

@article{richard2003,
  title = {One Hundred Years of Social Psychology Quantitatively Described},
  author = {Richard, F. D. and Bond, Charles F. and {Stokes-Zoota}, Juli J.},
  year = {2003},
  month = dec,
  journal = {Review of General Psychology},
  volume = {7},
  number = {4},
  pages = {331--363},
  issn = {1089-2680, 1939-1552},
  doi = {10.1037/1089-2680.7.4.331},
  urldate = {2023-03-28},
  abstract = {This article compiles results from a century of social psychological research, more than 25,000 studies of 8 million people. A large number of social psychological conclusions are listed alongside meta-analytic information about the magnitude and variability of the corresponding effects. References to 322 meta-analyses of social psychological phenomena are presented, as well as statistical effect-size summaries. Analyses reveal that social psychological effects typically yield a value of r equal to.21 and that, in the typical research literature, effects vary from study to study in ways that produce a standard deviation in r of.15. Uses, limitations, and implications of this large-scale compilation are noted.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\MAKG5KRM\Richard et al. - 2003 - One Hundred Years of Social Psychology Quantitativ.pdf}
}

@article{roberts2007,
  title = {The Power of Personality: {{The}} Comparative Validity of Personality Traits, Socioeconomic Status, and Cognitive Ability for Predicting Important Life Outcomes},
  shorttitle = {The Power of Personality},
  author = {Roberts, Brent W. and Kuncel, Nathan R. and Shiner, Rebecca and Caspi, Avshalom and Goldberg, Lewis R.},
  year = {2007},
  month = dec,
  journal = {Perspectives on Psychological Science},
  volume = {2},
  number = {4},
  pages = {313--345},
  issn = {1745-6916, 1745-6924},
  doi = {10.1111/j.1745-6916.2007.00047.x},
  urldate = {2023-03-28},
  abstract = {The ability of personality traits to predict important life outcomes has traditionally been questioned because of the putative small effects of personality. In this article, we compare the predictive validity of personality traits with that of socioeconomic status (SES) and cognitive ability to test the relative contribution of personality traits to predictions of three critical outcomes: mortality, divorce, and occupational attainment. Only evidence from prospective longitudinal studies was considered. In addition, an attempt was made to limit the review to studies that controlled for important background factors. Results showed that the magnitude of the effects of personality traits on mortality, divorce, and occupational attainment was indistinguishable from the effects of SES and cognitive ability on these outcomes. These results demonstrate the influence of personality traits on important life outcomes, highlight the need to more routinely incorporate measures of personality into quality of life surveys, and encourage further research about the developmental origins of personality traits and the processes by which these traits influence diverse life outcomes.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\8MHZY46E\Roberts et al. - 2007 - The Power of Personality The Comparative Validity.pdf}
}

@article{robertson2023a,
  title = {Point Estimation for Adaptive Trial Designs {{I}}: {{A}} Methodological Review},
  shorttitle = {Point Estimation for Adaptive Trial Designs {{I}}},
  author = {Robertson, David S. and {Choodari-Oskooei}, Babak and Dimairo, Munya and Flight, Laura and Pallmann, Philip and Jaki, Thomas},
  year = {2023},
  journal = {Statistics in Medicine},
  volume = {42},
  number = {2},
  pages = {122--145},
  issn = {1097-0258},
  doi = {10.1002/sim.9605},
  urldate = {2024-01-29},
  abstract = {Recent FDA guidance on adaptive clinical trial designs defines bias as ``a systematic tendency for the estimate of treatment effect to deviate from its true value,'' and states that it is desirable to obtain and report estimates of treatment effects that reduce or remove this bias. The conventional end-of-trial point estimates of the treatment effects are prone to bias in many adaptive designs, because they do not take into account the potential and realized trial adaptations. While much of the methodological developments on adaptive designs have tended to focus on control of type I error rates and power considerations, in contrast the question of biased estimation has received relatively less attention. This article is the first in a two-part series that studies the issue of potential bias in point estimation for adaptive trials. Part I provides a comprehensive review of the methods to remove or reduce the potential bias in point estimation of treatment effects for adaptive designs, while part II illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. The methods reviewed in this article can be broadly classified into unbiased and bias-reduced estimation, and we also provide a classification of estimators by the type of adaptive design. We compare the proposed methods, highlight available software and code, and discuss potential methodological gaps in the literature.},
  langid = {english},
  keywords = {1 paper,current reading,sequential estimation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XUVHKEMR\\Robertson et al. - 2023 - Point estimation for adaptive trial designs I A m.pdf;C\:\\Users\\Admin\\Zotero\\storage\\TVY6UEDI\\sim.html}
}

@article{rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.86.3.638},
  urldate = {2023-05-02},
  keywords = {1 paper},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5FQ6V2ZB\\Rosenthal - The file drawer problem and tolerance for null res.pdf;C\:\\Users\\Admin\\Zotero\\storage\\7SZC7WD3\\1979-27602-001.html}
}

@article{rossi1990,
  title = {Statistical Power of Psychological Research: {{What}} Have We Gained in 20 Years?},
  shorttitle = {Statistical Power of Psychological Research},
  author = {Rossi, Joseph S.},
  year = {1990},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {58},
  number = {5},
  pages = {646--656},
  issn = {1939-2117, 0022-006X},
  doi = {10.1037/0022-006X.58.5.646},
  urldate = {2023-02-15},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\PDKKGZ8K\Rossi - 1990 - Statistical power of psychological research What .pdf}
}

@article{rouder2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  urldate = {2021-09-23},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\RPP8TDBG\Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the n.pdf}
}

@article{rouder2014,
  title = {Optional Stopping: {{No}} Problem for {{Bayesians}}},
  shorttitle = {Optional Stopping},
  author = {Rouder, Jeffrey N.},
  year = {2014},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {2},
  pages = {301--308},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-014-0595-4},
  urldate = {2023-02-22},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\VUPRL5HB\Rouder - 2014 - Optional stopping No problem for Bayesians.pdf}
}

@article{rushton1952,
  title = {On a {{Two-Sided Sequential}} t-{{Test}}},
  author = {Rushton, S.},
  year = {1952},
  month = dec,
  journal = {Biometrika},
  volume = {39},
  number = {3/4},
  eprint = {2334026},
  eprinttype = {jstor},
  pages = {302},
  issn = {00063444},
  doi = {10.2307/2334026},
  urldate = {2021-09-23},
  langid = {english},
  annotation = {Read\_Status: not opened Read\_Status\_Date: 2025-02-17T16:30:39.224Z},
  file = {C\:\\Users\\Meike\\Zotero\\storage\\34ZUGIQC\\Rushton - 1952 - On a Two-Sided Sequential t-Test.pdf}
}


@article{rushton1950,
  title = {On a Sequential T-Test},
  author = {Rushton, S},
  year = {1950},
  journal = {Biometrika},
  volume = {37},
  pages = {326--333},
  doi = {10.2307/2332385},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\BPQJXCQQ\Rushton - 2021 - On a Sequential t-Test.pdf}
}

@article{schmider2010,
  title = {Is It Really Robust?: {{Reinvestigating}} the Robustness of {{ANOVA}} against Violations of the Normal Distribution Assumption},
  shorttitle = {Is It Really Robust?},
  author = {Schmider, Emanuel and Ziegler, Matthias and Danay, Erik and Beyer, Luzi and B{\"u}hner, Markus},
  year = {2010},
  month = jan,
  journal = {Methodology},
  volume = {6},
  number = {4},
  pages = {147--151},
  issn = {1614-1881, 1614-2241},
  doi = {10.1027/1614-2241/a000016},
  urldate = {2022-07-11},
  abstract = {Empirical evidence to the robustness of the analysis of variance (ANOVA) concerning violation of the normality assumption is presented by means of Monte Carlo methods. High-quality samples underlying normally, rectangularly, and exponentially distributed basic populations are created by drawing samples which consist of random numbers from respective generators, checking their goodness of fit, and allowing only the best 10\% to take part in the investigation. A one-way fixed-effect design with three groups of 25 values each is chosen. Effect-sizes are implemented in the samples and varied over a broad range. Comparing the outcomes of the ANOVA calculations for the different types of distributions, gives reason to regard the ANOVA as robust. Both, the empirical type I error {$\alpha$} and the empirical type II error {$\beta$} remain constant under violation. Moreover, regression analysis identifies the factor ``type of distribution'' as not significant in explanation of the ANOVA results.},
  langid = {english},
  keywords = {1 paper,notion},
  file = {C:\Users\Admin\Zotero\storage\LWDGVKID\Schmider et al. - 2010 - Is It Really Robust Reinvestigating the Robustne.pdf}
}

@article{schnuerch2020,
  title = {Controlling Decision Errors with Minimal Costs: {{The}} Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  year = {2020},
  month = apr,
  journal = {Psychological Methods},
  volume = {25},
  number = {2},
  pages = {206--226},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000234},
  urldate = {2021-09-23},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\G42Z22M6\Schnuerch und Erdfelder - 2020 - Controlling decision errors with minimal costs Th.pdf}
}

@article{schnuerch2020a,
  title = {Sequential Hypothesis Tests for Multinomial Processing Tree Models},
  author = {Schnuerch, Martin and Erdfelder, Edgar and Heck, Daniel W.},
  year = {2020},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {95},
  pages = {102326},
  issn = {00222496},
  doi = {10.1016/j.jmp.2020.102326},
  urldate = {2023-04-24},
  abstract = {Stimulated by William H. Batchelder's seminal contributions in the 1980s and 1990s, multinomial processing tree (MPT) modeling has become a powerful and frequently used method in various research fields, most prominently in cognitive psychology and social cognition research. MPT models allow for estimation of, and statistical tests on, parameters that represent psychological processes underlying responses to cognitive tasks. Therefore, their use has also been proposed repeatedly for purposes of psychological assessment, for example, in clinical settings to identify specific cognitive deficits in individuals. However, a considerable drawback of individual MPT analyses emerges from the limited number of data points per individual, resulting in estimation bias, large standard errors, and low power of statistical tests. Classical test procedures such as Neyman\textendash Pearson tests often require very large sample sizes to ensure sufficiently low Type 1 and Type 2 error probabilities. Herein, we propose sequential probability ratio tests (SPRTs) as an efficient alternative. Unlike Neyman\textendash Pearson tests, sequential tests continuously monitor the data and terminate when a predefined criterion is met. As a consequence, SPRTs typically require only about half of the Neyman\textendash Pearson sample size without compromising error probability control. We illustrate the SPRT approach to statistical inference for simple hypotheses in single-parameter MPT models. Moreover, a large-sample approximation, based on ML theory, is presented for typical MPT models with more than one unknown parameter. We evaluate the properties of the proposed test procedures by means of simulations. Finally, we discuss benefits and limitations of sequential MPT analysis.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\I9IVWS4Q\Schnuerch et al. - 2020 - Sequential hypothesis tests for multinomial proces.pdf}
}

@article{schnuerch2022,
  title = {Waldian t Tests: {{Sequential Bayesian}} t Tests with Controlled Error Probabilities.},
  shorttitle = {Waldian t Tests},
  author = {Schnuerch, Martin and Heck, Daniel W. and Erdfelder, Edgar},
  year = {2022},
  month = apr,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000492},
  urldate = {2023-03-09},
  abstract = {Bayesian t tests have become increasingly popular alternatives to null-hypothesis significance testing (NHST) in psychological research. In contrast to NHST, they allow for the quantification of evidence in favor of the null hypothesis and for optional stopping. A major drawback of Bayesian t tests, however, is that error probabilities of statistical decisions remain uncontrolled. Previous approaches in the literature to remedy this problem require time-consuming simulations to calibrate decision thresholds. In this article, we propose a sequential probability ratio test that combines Bayesian t tests with simple decision criteria developed by Abraham Wald in 1947. We discuss this sequential procedure, which we call Waldian t test, in the context of three recently proposed specifications of Bayesian t tests. Waldian t tests preserve the key idea of Bayesian t tests by assuming a distribution for the effect size under the alternative hypothesis. At the same time, they control expected frequentist error probabilities, with the nominal Type I and Type II error probabilities serving as upper bounds to the actual expected error rates under the specified statistical models. Thus, Waldian t tests are fully justified from both a Bayesian and a frequentist point of view. We highlight the relationship between Bayesian and frequentist error probabilities and critically discuss the implications of conventional stopping criteria for sequential Bayesian t tests. Finally, we provide a user-friendly web application that implements the proposed procedure for interested researchers.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\XELAYMT2\Schnuerch et al. - 2022 - Waldian t tests Sequential Bayesian t tests with .pdf}
}

@article{schonbrodt2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences.},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  year = {2017},
  month = jun,
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {322--339},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000061},
  urldate = {2021-09-23},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\NN9HJJMT\Sch√∂nbrodt et al. - 2017 - Sequential hypothesis testing with Bayes factors .pdf}
}

@article{stefan2022c,
  title = {Efficiency in Sequential Testing: {{Comparing}} the Sequential Probability Ratio Test and the Sequential {{Bayes}} Factor Test},
  shorttitle = {Efficiency in Sequential Testing},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D. and Evans, Nathan J. and Wagenmakers, Eric-Jan},
  year = {2022},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {54},
  number = {6},
  pages = {3100--3117},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01754-8},
  urldate = {2023-02-22},
  abstract = {In a sequential hypothesis test, the analyst checks at multiple steps during data collection whether sufficient evidence has accrued to make a decision about the tested hypotheses. As soon as sufficient information has been obtained, data collection is terminated. Here, we compare two sequential hypothesis testing procedures that have recently been proposed for use in psychological research: Sequential Probability Ratio Test (SPRT; Psychological Methods, 25(2), 206\textendash 226, 2020) and the Sequential Bayes Factor Test (SBFT; Psychological Methods, 22(2), 322\textendash 339, 2017). We show that although the two methods have different philosophical roots, they share many similarities and can even be mathematically regarded as two instances of an overarching hypothesis testing framework. We demonstrate that the two methods use the same mechanisms for evidence monitoring and error control, and that differences in efficiency between the methods depend on the exact specification of the statistical models involved, as well as on the population truth. Our simulations indicate that when deciding on a sequential design within a unified sequential testing framework, researchers need to balance the needs of test efficiency, robustness against model misspecification, and appropriate uncertainty quantification. We provide guidance for navigating these design decisions based on individual preferences and simulation-based design analyses.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\QGV2KEE8\Stefan et al. - 2022 - Efficiency in sequential testing Comparing the se.pdf}
}

@article{steiger2004,
  title = {Beyond the {{F}} Test: {{Effect}} Size Confidence Intervals and Tests of Close Fit in the Analysis of Variance and Contrast Analysis.},
  shorttitle = {Beyond the f Test},
  author = {Steiger, James H.},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {164--182},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.164},
  urldate = {2022-08-12},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\F522GWKT\Steiger - 2004 - Beyond the F Test Effect Size Confidence Interval.pdf}
}

@article{steinhilber2023,
  title = {Sprtt: {{Sequential}} Probability Ratio Test Toolbox {\emph{(}}{{{\emph{Version}}}}{\emph{ 0.2.0) [}}{{{\emph{R Package}}}}{\emph{]}}},
  author = {Steinhilber, Meike and Schnuerch, Martin and Schubert, Anna-Lena},
  year = {2023},
  doi = {https://CRAN.R-project.org/package=sprtt},
  keywords = {1 paper,software}
}

@misc{steinhilber2023a,
  title = {Sequential Analysis of Variance: {{Increasing}} Effciency of Hypothesis Testing},
  author = {Steinhilber, Meike and Schnuerch, Martin and Schubert, Anna-Lena},
  year = {2023},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/m64ne},
  keywords = {1 paper}
}

@misc{steinhilber2023c,
  title = {Sequential Analysis of Variance: {{Increasing}} Effciency of Hypothesis Testing},
  shorttitle = {Sequential {{Analysis}} of {{Variance}}},
  author = {Steinhilber, Meike and Schubert, Anna-Lena and Schnuerch, Martin},
  year = {2023},
  month = jun,
  journal = {OSF},
  publisher = {{OSF}},
  urldate = {2024-02-16},
  abstract = {The repository to the simulation study: "Sequential Analysis of Variance: Increasing Efficiency of Hypothesis Testing" Contains all simulation scripts, data and 1\% of the simulated raw data.      Hosted on the Open Science Framework},
  howpublished = {https://osf.io/6yq5f/},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\2S22I6IQ\6yq5f.html}
}

@article{szucs2017,
  title = {When Null Hypothesis Significance Testing Is Unsuitable for Research: {{A}} Reassessment},
  shorttitle = {When Null Hypothesis Significance Testing Is Unsuitable for Research},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  pages = {390},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  urldate = {2023-02-15},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology, and biomedical science in general. We review these shortcomings and suggest that, after sustained negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out.},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\H7FIY3GW\Szucs und Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{szucs2017a,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = mar,
  journal = {PLOS Biology},
  volume = {15},
  number = {3},
  pages = {e2000797},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  urldate = {2023-02-15},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64\textendash 1.46) for nominally statistically significant results and D = 0.24 (0.11\textendash 0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\7V6UCETH\Szucs und Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf}
}

@article{tremmel2010,
  title = {Statistical {{Inference}} after an {{Adaptive Group Sequential Design}}: {{A Case Study}}},
  shorttitle = {Statistical {{Inference}} after an {{Adaptive Group Sequential Design}}},
  author = {Tremmel, Lothar T.},
  year = {2010},
  month = sep,
  journal = {Drug Information Journal},
  volume = {44},
  number = {5},
  pages = {589--598},
  issn = {0092-8615, 2164-9200},
  doi = {10.1177/009286151004400506},
  urldate = {2024-02-05},
  langid = {english},
  keywords = {1 paper,sequential estimation},
  file = {C:\Users\Admin\Zotero\storage\4PPA7XJL\Tremmel - 2010 - Statistical Inference after an Adaptive Group Sequ.pdf}
}

@article{ulrich2021,
  title = {Alternative Sequential Methods in Statistical Testing: {{A}} Reply to {{Lakens}} (2021) and {{Erdfelder}} and {{Schnuerch}} (2021).},
  shorttitle = {Alternative Sequential Methods in Statistical Testing},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2021},
  month = aug,
  journal = {Psychological Methods},
  volume = {26},
  number = {4},
  pages = {507--512},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000420},
  urldate = {2023-03-09},
  abstract = {We recently developed a simple and general sequential sampling method for testing null hypotheses, the independent segments procedure (ISP; Miller \& Ulrich, 2021). In this reply, we discuss the comments of Erdfelder and Schnuerch (2021) and Lakens (2021), who consider alternative methods such as the sequential probability ratio test (SPRT) and the group sequential design (GSD), respectively. We evaluate the pros and cons of these alternatives and conclude that the ISP does have several advantages over these other methods, especially for psychological research. All of these sequential methods can save research resources because smaller sample sizes are required compared to standard nonsequential methods, so it seems appropriate for researchers to choose from a variety of sequential methods based on the practical requirements of their research.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\6AQWZLYL\Ulrich und Miller - 2021 - Alternative sequential methods in statistical test.pdf}
}

@article{wagenmakers2010,
  title = {Bayesian Hypothesis Testing for Psychologists: {{A}} Tutorial on the {{Savage}}\textendash{{Dickey}} Method},
  shorttitle = {Bayesian Hypothesis Testing for Psychologists},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
  year = {2010},
  month = may,
  journal = {Cognitive Psychology},
  volume = {60},
  number = {3},
  pages = {158--189},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2009.12.001},
  urldate = {2021-11-19},
  abstract = {In the field of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage\textendash Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method's validity, generality, and flexibility.},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\K4JTHRH7\Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf}
}

@article{wald1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, A.},
  year = {1945},
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  pages = {117--186},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\4XEITD76\Wald - 1945 - Sequential Tests of Statistical Hypotheses.pdf}
}

@book{wald1947,
  title = {Sequential Analysis},
  author = {Wald, Abraham},
  year = {1947},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-486-61579-0},
  langid = {english},
  lccn = {QA279.7 .W34 1973},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\ICGI6BRX\Wald - 1973 - Sequential analysis.pdf}
}

@article{wald1948,
  title = {Optimum Character of the Sequential Probability Ratio Test},
  author = {Wald, A. and Wolfowitz, J.},
  year = {1948},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {19},
  number = {3},
  pages = {326--339},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177730197},
  urldate = {2021-09-23},
  langid = {english},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\2LCRRDVP\Wald und Wolfowitz - 1948 - Optimum Character of the Sequential Probability Ra.pdf}
}

@book{wetherill1986,
  title = {Sequential Methods in Statistics},
  author = {Wetherill, G. Barrie and Glazebrook, Kevin D.},
  year = {1986},
  series = {Monographs on Statistics and Applied Probability},
  edition = {3rd ed},
  publisher = {{Chapman and Hall}},
  address = {{London ; New York}},
  isbn = {978-0-412-28150-1},
  lccn = {QA279.7 .W47 1986},
  keywords = {1 paper},
  file = {C:\Users\Admin\Zotero\storage\6NJSJDJT\Wetherill_seqANOVA.pdf.pdf}
}

@article{whitehead1986,
  title = {On the Bias of Maximum Likelihood Estimation Following a Sequential Test},
  author = {Whitehead, John},
  year = {1986},
  journal = {Biometrika},
  volume = {73},
  number = {3},
  pages = {573--81},
  abstract = {The bias of maximum likelihood estimates calculated at the end of a sequential procedure is investigated. For the two sequential designs considered in detail, the sequential probability ratio test and the triangular test, this bias is appreciable. A method of calculating an adjusted estimate with reduced bias is described, and an approximation to the standard error of the new estimate is provided. Examples of the implementation of the method are given, and its advantages and disadvantages relative to alternative approaches are discussed.},
  langid = {english},
  keywords = {1 paper,sequential estimation},
  file = {C:\Users\Admin\Zotero\storage\GQ4MT4XM\Whitehead - On the bias of maximum likelihood estimation follo.pdf}
}
